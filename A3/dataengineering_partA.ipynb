{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcb40689-fcac-4014-9f17-8111fd417fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/21 13:11:52 INFO SparkContext: Running Spark version 3.5.0\n",
      "24/02/21 13:11:52 INFO SparkContext: OS info Linux, 5.4.0-167-generic, amd64\n",
      "24/02/21 13:11:52 INFO SparkContext: Java version 17.0.9\n",
      "24/02/21 13:11:52 INFO ResourceUtils: ==============================================================\n",
      "24/02/21 13:11:52 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "24/02/21 13:11:52 INFO ResourceUtils: ==============================================================\n",
      "24/02/21 13:11:52 INFO SparkContext: Submitted application: AlmaLundberg_Sparkapplication\n",
      "24/02/21 13:11:52 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 2, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "24/02/21 13:11:52 INFO ResourceProfile: Limiting resource is cpus at 2 tasks per executor\n",
      "24/02/21 13:11:52 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "24/02/21 13:11:52 INFO SecurityManager: Changing view acls to: ubuntu\n",
      "24/02/21 13:11:52 INFO SecurityManager: Changing modify acls to: ubuntu\n",
      "24/02/21 13:11:52 INFO SecurityManager: Changing view acls groups to: \n",
      "24/02/21 13:11:52 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/02/21 13:11:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ubuntu; groups with view permissions: EMPTY; users with modify permissions: ubuntu; groups with modify permissions: EMPTY\n",
      "24/02/21 13:11:52 INFO Utils: Successfully started service 'sparkDriver' on port 9999.\n",
      "24/02/21 13:11:52 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/02/21 13:11:52 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/02/21 13:11:52 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "24/02/21 13:11:52 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "24/02/21 13:11:52 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/02/21 13:11:52 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-46a6d5d8-3be8-4222-99f0-2e39782a9376\n",
      "24/02/21 13:11:52 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "24/02/21 13:11:52 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "24/02/21 13:11:52 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "24/02/21 13:11:52 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "24/02/21 13:11:52 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://192.168.2.250:7077...\n",
      "24/02/21 13:11:52 INFO TransportClientFactory: Successfully created connection to /192.168.2.250:7077 after 2 ms (0 ms spent in bootstraps)\n",
      "24/02/21 13:11:52 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20240221131152-1494\n",
      "24/02/21 13:11:52 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240221131152-1494/0 on worker-20240203212953-192.168.2.251-33543 (192.168.2.251:33543) with 2 core(s)\n",
      "24/02/21 13:11:52 INFO StandaloneSchedulerBackend: Granted executor ID app-20240221131152-1494/0 on hostPort 192.168.2.251:33543 with 2 core(s), 1024.0 MiB RAM\n",
      "24/02/21 13:11:52 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240221131152-1494/1 on worker-20240203212953-192.168.2.250-32787 (192.168.2.250:32787) with 2 core(s)\n",
      "24/02/21 13:11:52 INFO StandaloneSchedulerBackend: Granted executor ID app-20240221131152-1494/1 on hostPort 192.168.2.250:32787 with 2 core(s), 1024.0 MiB RAM\n",
      "24/02/21 13:11:52 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 10005.\n",
      "24/02/21 13:11:52 INFO NettyBlockTransferService: Server created on host-192-168-2-13-de1:10005\n",
      "24/02/21 13:11:52 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "24/02/21 13:11:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, host-192-168-2-13-de1, 10005, None)\n",
      "24/02/21 13:11:52 INFO BlockManagerMasterEndpoint: Registering block manager host-192-168-2-13-de1:10005 with 434.4 MiB RAM, BlockManagerId(driver, host-192-168-2-13-de1, 10005, None)\n",
      "24/02/21 13:11:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, host-192-168-2-13-de1, 10005, None)\n",
      "24/02/21 13:11:52 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, host-192-168-2-13-de1, 10005, None)\n",
      "24/02/21 13:11:52 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240221131152-1494/0 is now RUNNING\n",
      "24/02/21 13:11:52 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240221131152-1494/1 is now RUNNING\n",
      "24/02/21 13:11:53 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "24/02/21 13:11:54 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.2.251:40148) with ID 0,  ResourceProfileId 0\n",
      "24/02/21 13:11:54 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.251:10006 with 434.4 MiB RAM, BlockManagerId(0, 192.168.2.251, 10006, None)\n",
      "24/02/21 13:11:55 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.2.250:54648) with ID 1,  ResourceProfileId 0\n",
      "24/02/21 13:11:55 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.250:10006 with 434.4 MiB RAM, BlockManagerId(1, 192.168.2.250, 10006, None)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://192.168.2.250:7077\") \\\n",
    "        .appName(\"AlmaLundberg_Sparkapplication\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", False)\\\n",
    "        .config(\"spark.cores.max\", 4)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\",2)\\\n",
    "        .config(\"spark.driver.port\",9999)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# RDD API\n",
    "spark_context = spark_session.sparkContext\n",
    "\n",
    "spark_context.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffafb5a6-41c1-490f-a5ac-6bdcc88d364f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/21 13:11:57 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 221.5 KiB, free 434.2 MiB)\n",
      "24/02/21 13:11:57 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 434.2 MiB)\n",
      "24/02/21 13:11:57 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on host-192-168-2-13-de1:10005 (size: 32.6 KiB, free: 434.4 MiB)\n",
      "24/02/21 13:11:57 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
      "24/02/21 13:11:57 INFO FileInputFormat: Total input files to process : 1\n",
      "24/02/21 13:11:57 INFO NetworkTopology: Adding a new node: /default-rack/192.168.2.250:9866\n",
      "24/02/21 13:11:57 INFO SparkContext: Starting job: count at /tmp/ipykernel_33873/476092916.py:4\n",
      "24/02/21 13:11:57 INFO DAGScheduler: Got job 0 (count at /tmp/ipykernel_33873/476092916.py:4) with 2 output partitions\n",
      "24/02/21 13:11:57 INFO DAGScheduler: Final stage: ResultStage 0 (count at /tmp/ipykernel_33873/476092916.py:4)\n",
      "24/02/21 13:11:57 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/02/21 13:11:57 INFO DAGScheduler: Missing parents: List()\n",
      "24/02/21 13:11:57 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at count at /tmp/ipykernel_33873/476092916.py:4), which has no missing parents\n",
      "24/02/21 13:11:57 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.1 KiB, free 434.1 MiB)\n",
      "24/02/21 13:11:57 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.1 MiB)\n",
      "24/02/21 13:11:57 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on host-192-168-2-13-de1:10005 (size: 5.5 KiB, free: 434.4 MiB)\n",
      "24/02/21 13:11:57 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/21 13:11:57 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[2] at count at /tmp/ipykernel_33873/476092916.py:4) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/02/21 13:11:57 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n",
      "24/02/21 13:11:57 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 0) (192.168.2.250, executor 1, partition 1, NODE_LOCAL, 7690 bytes) \n",
      "24/02/21 13:11:57 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.250:10006 (size: 5.5 KiB, free: 434.4 MiB)\n",
      "24/02/21 13:11:58 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.250:10006 (size: 32.6 KiB, free: 434.4 MiB)\n",
      "24/02/21 13:12:00 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 0) in 2609 ms on 192.168.2.250 (executor 1) (1/2)\n",
      "24/02/21 13:12:00 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 55929\n",
      "24/02/21 13:12:00 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 1) (192.168.2.250, executor 1, partition 0, ANY, 7690 bytes) \n",
      "24/02/21 13:12:01 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 1) in 919 ms on 192.168.2.250 (executor 1) (2/2)\n",
      "24/02/21 13:12:01 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "24/02/21 13:12:01 INFO DAGScheduler: ResultStage 0 (count at /tmp/ipykernel_33873/476092916.py:4) finished in 4.282 s\n",
      "24/02/21 13:12:01 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/21 13:12:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "24/02/21 13:12:01 INFO DAGScheduler: Job 0 finished: count at /tmp/ipykernel_33873/476092916.py:4, took 4.286435 s\n",
      "24/02/21 13:12:01 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 221.5 KiB, free 433.9 MiB)\n",
      "24/02/21 13:12:01 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 433.9 MiB)\n",
      "24/02/21 13:12:01 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on host-192-168-2-13-de1:10005 (size: 32.6 KiB, free: 434.3 MiB)\n",
      "24/02/21 13:12:01 INFO SparkContext: Created broadcast 2 from textFile at NativeMethodAccessorImpl.java:0\n",
      "24/02/21 13:12:01 INFO BlockManagerInfo: Removed broadcast_1_piece0 on host-192-168-2-13-de1:10005 in memory (size: 5.5 KiB, free: 434.3 MiB)\n",
      "24/02/21 13:12:01 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.2.250:10006 in memory (size: 5.5 KiB, free: 434.4 MiB)\n",
      "24/02/21 13:12:01 INFO FileInputFormat: Total input files to process : 1\n",
      "24/02/21 13:12:01 INFO SparkContext: Starting job: count at /tmp/ipykernel_33873/476092916.py:14\n",
      "24/02/21 13:12:01 INFO DAGScheduler: Got job 1 (count at /tmp/ipykernel_33873/476092916.py:14) with 3 output partitions\n",
      "24/02/21 13:12:01 INFO DAGScheduler: Final stage: ResultStage 1 (count at /tmp/ipykernel_33873/476092916.py:14)\n",
      "24/02/21 13:12:01 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/02/21 13:12:01 INFO DAGScheduler: Missing parents: List()\n",
      "24/02/21 13:12:01 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[5] at count at /tmp/ipykernel_33873/476092916.py:14), which has no missing parents\n",
      "24/02/21 13:12:01 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 9.1 KiB, free 433.9 MiB)\n",
      "24/02/21 13:12:01 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 433.9 MiB)\n",
      "24/02/21 13:12:01 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on host-192-168-2-13-de1:10005 (size: 5.5 KiB, free: 434.3 MiB)\n",
      "24/02/21 13:12:01 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/21 13:12:01 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 1 (PythonRDD[5] at count at /tmp/ipykernel_33873/476092916.py:14) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/02/21 13:12:01 INFO TaskSchedulerImpl: Adding task set 1.0 with 3 tasks resource profile 0\n",
      "24/02/21 13:12:01 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2) (192.168.2.250, executor 1, partition 0, ANY, 7690 bytes) \n",
      "24/02/21 13:12:01 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3) (192.168.2.251, executor 0, partition 1, ANY, 7690 bytes) \n",
      "24/02/21 13:12:01 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 4) (192.168.2.250, executor 1, partition 2, ANY, 7690 bytes) \n",
      "24/02/21 13:12:01 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.250:10006 (size: 5.5 KiB, free: 434.4 MiB)\n",
      "24/02/21 13:12:02 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.250:10006 (size: 32.6 KiB, free: 434.3 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines in English file: 1862234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/21 13:12:02 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.251:10006 (size: 5.5 KiB, free: 434.4 MiB)\n",
      "24/02/21 13:12:02 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 4) in 288 ms on 192.168.2.250 (executor 1) (1/3)\n",
      "24/02/21 13:12:02 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.251:10006 (size: 32.6 KiB, free: 434.4 MiB)\n",
      "24/02/21 13:12:03 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 1382 ms on 192.168.2.250 (executor 1) (2/3)\n",
      "[Stage 1:=======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines in Swedish file: 1862234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/21 13:12:04 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 2517 ms on 192.168.2.251 (executor 0) (3/3)\n",
      "24/02/21 13:12:04 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "24/02/21 13:12:04 INFO DAGScheduler: ResultStage 1 (count at /tmp/ipykernel_33873/476092916.py:14) finished in 2.533 s\n",
      "24/02/21 13:12:04 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/21 13:12:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "24/02/21 13:12:04 INFO DAGScheduler: Job 1 finished: count at /tmp/ipykernel_33873/476092916.py:14, took 2.538122 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Using map and reduce from the Spark API, and loading the text file from HDFS.\n",
    "\n",
    "lines_en = spark_context.textFile(\"hdfs://192.168.2.250:9000/europarl/europarl-v7.sv-en.en\")\n",
    "num_lines_en = lines_en.count()\n",
    "print(f\"Total lines in English file: {num_lines_en}\")\n",
    "\n",
    "# print(lines.first())\n",
    "# words = lines.map(lambda line: line.split(' '))\n",
    "# word_counts = words.map(lambda w: len(w))\n",
    "# total_words = word_counts.reduce(add)\n",
    "# print(f'total words= {total_words}')  \n",
    "\n",
    "lines_sv = spark_context.textFile(\"hdfs://192.168.2.250:9000/europarl/europarl-v7.sv-en.sv\")\n",
    "num_lines_sv = lines_sv.count()\n",
    "print(f\"Total lines in Swedish file: {num_lines_sv}\")\n",
    "\n",
    "\n",
    "#lines = spark_context.textFile(\"/home/ubuntu/i_have_a_dream.txt\")\n",
    "# print(lines.first())\n",
    "# words = lines.map(lambda line: line.split(' '))\n",
    "# word_counts = words.map(lambda w: len(w))\n",
    "# total_words = word_counts.reduce(add)\n",
    "# print(f'total words= {total_words}')  \n",
    "# ... the same number of words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cbef32-5bfa-4190-8bc1-1c4b972a00ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total lines in English file: 1862234\n",
    "# Total lines in Swedish file: 1862234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d574749-5a3d-4e1c-b3e9-efef1a078fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions in English file: 2\n",
      "Number of partitions in Swedish file: 3\n"
     ]
    }
   ],
   "source": [
    "# Counting nmr of partitions in the English file\n",
    "num_partitions_en = lines_en.getNumPartitions()\n",
    "print(f\"Number of partitions in English file: {num_partitions_en}\")\n",
    "\n",
    "# Counting nmr of partitions in the Swedish file\n",
    "num_partitions_sv = lines_sv.getNumPartitions()\n",
    "print(f\"Number of partitions in Swedish file: {num_partitions_sv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d864eddf-cc03-4058-82c3-5d3f11ad1b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of partitions in English file: 2\n",
    "# Number of partitions in Swedish file: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fa87b66a-46a1-49eb-9f87-4666a12f237e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(line):\n",
    "    return line.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a2a51f26-9c3c-466c-a409-a5b5bb5abb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_lines_en = lines_en.map(preprocess_text)\n",
    "processed_lines_sv = lines_sv.map(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6ae1159d-545c-4285-a260-a9f770979069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/21 15:22:12 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/02/21 15:22:12 INFO DAGScheduler: Got job 52 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/02/21 15:22:12 INFO DAGScheduler: Final stage: ResultStage 70 (runJob at PythonRDD.scala:181)\n",
      "24/02/21 15:22:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/02/21 15:22:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/02/21 15:22:12 INFO DAGScheduler: Submitting ResultStage 70 (PythonRDD[134] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/02/21 15:22:12 INFO MemoryStore: Block broadcast_70 stored as values in memory (estimated size 8.4 KiB, free 433.9 MiB)\n",
      "24/02/21 15:22:12 INFO MemoryStore: Block broadcast_70_piece0 stored as bytes in memory (estimated size 5.2 KiB, free 433.9 MiB)\n",
      "24/02/21 15:22:12 INFO BlockManagerInfo: Added broadcast_70_piece0 in memory on host-192-168-2-13-de1:10005 (size: 5.2 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:22:12 INFO SparkContext: Created broadcast 70 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/21 15:22:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 70 (PythonRDD[134] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/02/21 15:22:12 INFO TaskSchedulerImpl: Adding task set 70.0 with 1 tasks resource profile 0\n",
      "24/02/21 15:22:12 INFO TaskSetManager: Starting task 0.0 in stage 70.0 (TID 151) (192.168.2.251, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/02/21 15:22:12 INFO BlockManagerInfo: Added broadcast_70_piece0 in memory on 192.168.2.251:10006 (size: 5.2 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:22:12 INFO TaskSetManager: Finished task 0.0 in stage 70.0 (TID 151) in 48 ms on 192.168.2.251 (executor 0) (1/1)\n",
      "24/02/21 15:22:12 INFO TaskSchedulerImpl: Removed TaskSet 70.0, whose tasks have all completed, from pool \n",
      "24/02/21 15:22:12 INFO DAGScheduler: ResultStage 70 (runJob at PythonRDD.scala:181) finished in 0.067 s\n",
      "24/02/21 15:22:12 INFO DAGScheduler: Job 52 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/21 15:22:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 70: Stage finished\n",
      "24/02/21 15:22:12 INFO DAGScheduler: Job 52 finished: runJob at PythonRDD.scala:181, took 0.076579 s\n",
      "24/02/21 15:22:12 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/02/21 15:22:12 INFO DAGScheduler: Got job 53 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/02/21 15:22:12 INFO DAGScheduler: Final stage: ResultStage 71 (runJob at PythonRDD.scala:181)\n",
      "24/02/21 15:22:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/02/21 15:22:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/02/21 15:22:12 INFO DAGScheduler: Submitting ResultStage 71 (PythonRDD[135] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/02/21 15:22:12 INFO MemoryStore: Block broadcast_71 stored as values in memory (estimated size 8.4 KiB, free 433.9 MiB)\n",
      "24/02/21 15:22:12 INFO MemoryStore: Block broadcast_71_piece0 stored as bytes in memory (estimated size 5.2 KiB, free 433.9 MiB)\n",
      "24/02/21 15:22:12 INFO BlockManagerInfo: Added broadcast_71_piece0 in memory on host-192-168-2-13-de1:10005 (size: 5.2 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:22:12 INFO SparkContext: Created broadcast 71 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/21 15:22:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 71 (PythonRDD[135] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/02/21 15:22:12 INFO TaskSchedulerImpl: Adding task set 71.0 with 1 tasks resource profile 0\n",
      "24/02/21 15:22:12 INFO TaskSetManager: Starting task 0.0 in stage 71.0 (TID 152) (192.168.2.250, executor 1, partition 0, ANY, 7690 bytes) \n",
      "24/02/21 15:22:12 INFO BlockManagerInfo: Added broadcast_71_piece0 in memory on 192.168.2.250:10006 (size: 5.2 KiB, free: 434.3 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from English RDD: [['resumption', 'of', 'the', 'session'], ['i', 'declare', 'resumed', 'the', 'session', 'of', 'the', 'european', 'parliament', 'adjourned', 'on', 'friday', '17', 'december', '1999,', 'and', 'i', 'would', 'like', 'once', 'again', 'to', 'wish', 'you', 'a', 'happy', 'new', 'year', 'in', 'the', 'hope', 'that', 'you', 'enjoyed', 'a', 'pleasant', 'festive', 'period.'], ['although,', 'as', 'you', 'will', 'have', 'seen,', 'the', 'dreaded', \"'millennium\", \"bug'\", 'failed', 'to', 'materialise,', 'still', 'the', 'people', 'in', 'a', 'number', 'of', 'countries', 'suffered', 'a', 'series', 'of', 'natural', 'disasters', 'that', 'truly', 'were', 'dreadful.'], ['you', 'have', 'requested', 'a', 'debate', 'on', 'this', 'subject', 'in', 'the', 'course', 'of', 'the', 'next', 'few', 'days,', 'during', 'this', 'part-session.'], ['in', 'the', 'meantime,', 'i', 'should', 'like', 'to', 'observe', 'a', \"minute'\", 's', 'silence,', 'as', 'a', 'number', 'of', 'members', 'have', 'requested,', 'on', 'behalf', 'of', 'all', 'the', 'victims', 'concerned,', 'particularly', 'those', 'of', 'the', 'terrible', 'storms,', 'in', 'the', 'various', 'countries', 'of', 'the', 'european', 'union.'], ['please', 'rise,', 'then,', 'for', 'this', \"minute'\", 's', 'silence.'], ['(the', 'house', 'rose', 'and', 'observed', 'a', \"minute'\", 's', 'silence)'], ['madam', 'president,', 'on', 'a', 'point', 'of', 'order.'], ['you', 'will', 'be', 'aware', 'from', 'the', 'press', 'and', 'television', 'that', 'there', 'have', 'been', 'a', 'number', 'of', 'bomb', 'explosions', 'and', 'killings', 'in', 'sri', 'lanka.'], ['one', 'of', 'the', 'people', 'assassinated', 'very', 'recently', 'in', 'sri', 'lanka', 'was', 'mr', 'kumar', 'ponnambalam,', 'who', 'had', 'visited', 'the', 'european', 'parliament', 'just', 'a', 'few', 'months', 'ago.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 71:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from Swedish RDD: [['återupptagande', 'av', 'sessionen'], ['jag', 'förklarar', 'europaparlamentets', 'session', 'återupptagen', 'efter', 'avbrottet', 'den', '17', 'december.', 'jag', 'vill', 'på', 'nytt', 'önska', 'er', 'ett', 'gott', 'nytt', 'år', 'och', 'jag', 'hoppas', 'att', 'ni', 'haft', 'en', 'trevlig', 'semester.'], ['som', 'ni', 'kunnat', 'konstatera', 'ägde', '\"den', 'stora', 'år', '2000-buggen\"', 'aldrig', 'rum.', 'däremot', 'har', 'invånarna', 'i', 'ett', 'antal', 'av', 'våra', 'medlemsländer', 'drabbats', 'av', 'naturkatastrofer', 'som', 'verkligen', 'varit', 'förskräckliga.'], ['ni', 'har', 'begärt', 'en', 'debatt', 'i', 'ämnet', 'under', 'sammanträdesperiodens', 'kommande', 'dagar.'], ['till', 'dess', 'vill', 'jag', 'att', 'vi,', 'som', 'ett', 'antal', 'kolleger', 'begärt,', 'håller', 'en', 'tyst', 'minut', 'för', 'offren', 'för', 'bl.a.', 'stormarna', 'i', 'de', 'länder', 'i', 'europeiska', 'unionen', 'som', 'drabbats.'], ['jag', 'ber', 'er', 'resa', 'er', 'för', 'en', 'tyst', 'minut.'], ['(parlamentet', 'höll', 'en', 'tyst', 'minut.)'], ['fru', 'talman!', 'det', 'gäller', 'en', 'ordningsfråga.'], ['ni', 'känner', 'till', 'från', 'media', 'att', 'det', 'skett', 'en', 'rad', 'bombexplosioner', 'och', 'mord', 'i', 'sri', 'lanka.'], ['en', 'av', 'de', 'personer', 'som', 'mycket', 'nyligen', 'mördades', 'i', 'sri', 'lanka', 'var', 'kumar', 'ponnambalam,', 'som', 'besökte', 'europaparlamentet', 'för', 'bara', 'några', 'månader', 'sedan.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/21 15:22:13 INFO TaskSetManager: Finished task 0.0 in stage 71.0 (TID 152) in 1375 ms on 192.168.2.250 (executor 1) (1/1)\n",
      "24/02/21 15:22:13 INFO TaskSchedulerImpl: Removed TaskSet 71.0, whose tasks have all completed, from pool \n",
      "24/02/21 15:22:13 INFO DAGScheduler: ResultStage 71 (runJob at PythonRDD.scala:181) finished in 1.384 s\n",
      "24/02/21 15:22:13 INFO DAGScheduler: Job 53 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/21 15:22:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 71: Stage finished\n",
      "24/02/21 15:22:13 INFO DAGScheduler: Job 53 finished: runJob at PythonRDD.scala:181, took 1.394566 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Inspect 10 entries from each of your RDDs to verify the pre-processing\n",
    "\n",
    "print(\"Sample from English RDD:\", processed_lines_en.take(10))\n",
    "print(\"Sample from Swedish RDD:\", processed_lines_sv.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8cee82ee-1bbb-4521-b47e-c3c4982f9b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/21 15:22:19 INFO SparkContext: Starting job: count at /tmp/ipykernel_33873/226898882.py:3\n",
      "24/02/21 15:22:19 INFO DAGScheduler: Got job 54 (count at /tmp/ipykernel_33873/226898882.py:3) with 2 output partitions\n",
      "24/02/21 15:22:19 INFO DAGScheduler: Final stage: ResultStage 72 (count at /tmp/ipykernel_33873/226898882.py:3)\n",
      "24/02/21 15:22:19 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/02/21 15:22:19 INFO DAGScheduler: Missing parents: List()\n",
      "24/02/21 15:22:19 INFO DAGScheduler: Submitting ResultStage 72 (PythonRDD[136] at count at /tmp/ipykernel_33873/226898882.py:3), which has no missing parents\n",
      "24/02/21 15:22:19 INFO MemoryStore: Block broadcast_72 stored as values in memory (estimated size 9.6 KiB, free 433.8 MiB)\n",
      "24/02/21 15:22:19 INFO MemoryStore: Block broadcast_72_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 433.8 MiB)\n",
      "24/02/21 15:22:19 INFO BlockManagerInfo: Added broadcast_72_piece0 in memory on host-192-168-2-13-de1:10005 (size: 5.7 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:22:19 INFO SparkContext: Created broadcast 72 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/21 15:22:19 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 72 (PythonRDD[136] at count at /tmp/ipykernel_33873/226898882.py:3) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/02/21 15:22:19 INFO TaskSchedulerImpl: Adding task set 72.0 with 2 tasks resource profile 0\n",
      "24/02/21 15:22:19 INFO TaskSetManager: Starting task 1.0 in stage 72.0 (TID 153) (192.168.2.250, executor 1, partition 1, NODE_LOCAL, 7690 bytes) \n",
      "24/02/21 15:22:19 INFO BlockManagerInfo: Added broadcast_72_piece0 in memory on 192.168.2.250:10006 (size: 5.7 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:22:21 INFO TaskSetManager: Finished task 1.0 in stage 72.0 (TID 153) in 1914 ms on 192.168.2.250 (executor 1) (1/2)\n",
      "24/02/21 15:22:22 INFO TaskSetManager: Starting task 0.0 in stage 72.0 (TID 154) (192.168.2.251, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/02/21 15:22:22 INFO BlockManagerInfo: Added broadcast_72_piece0 in memory on 192.168.2.251:10006 (size: 5.7 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:22:25 INFO TaskSetManager: Finished task 0.0 in stage 72.0 (TID 154) in 2262 ms on 192.168.2.251 (executor 0) (2/2)\n",
      "24/02/21 15:22:25 INFO TaskSchedulerImpl: Removed TaskSet 72.0, whose tasks have all completed, from pool \n",
      "24/02/21 15:22:25 INFO DAGScheduler: ResultStage 72 (count at /tmp/ipykernel_33873/226898882.py:3) finished in 5.810 s\n",
      "24/02/21 15:22:25 INFO DAGScheduler: Job 54 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/21 15:22:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 72: Stage finished\n",
      "24/02/21 15:22:25 INFO DAGScheduler: Job 54 finished: count at /tmp/ipykernel_33873/226898882.py:3, took 5.814493 s\n",
      "24/02/21 15:22:25 INFO SparkContext: Starting job: count at /tmp/ipykernel_33873/226898882.py:4\n",
      "24/02/21 15:22:25 INFO DAGScheduler: Got job 55 (count at /tmp/ipykernel_33873/226898882.py:4) with 3 output partitions\n",
      "24/02/21 15:22:25 INFO DAGScheduler: Final stage: ResultStage 73 (count at /tmp/ipykernel_33873/226898882.py:4)\n",
      "24/02/21 15:22:25 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/02/21 15:22:25 INFO DAGScheduler: Missing parents: List()\n",
      "24/02/21 15:22:25 INFO DAGScheduler: Submitting ResultStage 73 (PythonRDD[137] at count at /tmp/ipykernel_33873/226898882.py:4), which has no missing parents\n",
      "24/02/21 15:22:25 INFO MemoryStore: Block broadcast_73 stored as values in memory (estimated size 9.6 KiB, free 433.8 MiB)\n",
      "24/02/21 15:22:25 INFO MemoryStore: Block broadcast_73_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 433.8 MiB)\n",
      "24/02/21 15:22:25 INFO BlockManagerInfo: Removed broadcast_72_piece0 on 192.168.2.251:10006 in memory (size: 5.7 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:22:25 INFO BlockManagerInfo: Added broadcast_73_piece0 in memory on host-192-168-2-13-de1:10005 (size: 5.7 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:22:25 INFO BlockManagerInfo: Removed broadcast_72_piece0 on 192.168.2.250:10006 in memory (size: 5.7 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:22:25 INFO SparkContext: Created broadcast 73 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/21 15:22:25 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 73 (PythonRDD[137] at count at /tmp/ipykernel_33873/226898882.py:4) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/02/21 15:22:25 INFO BlockManagerInfo: Removed broadcast_72_piece0 on host-192-168-2-13-de1:10005 in memory (size: 5.7 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:22:25 INFO TaskSchedulerImpl: Adding task set 73.0 with 3 tasks resource profile 0\n",
      "24/02/21 15:22:25 INFO TaskSetManager: Starting task 0.0 in stage 73.0 (TID 155) (192.168.2.250, executor 1, partition 0, ANY, 7690 bytes) \n",
      "24/02/21 15:22:25 INFO TaskSetManager: Starting task 1.0 in stage 73.0 (TID 156) (192.168.2.251, executor 0, partition 1, ANY, 7690 bytes) \n",
      "24/02/21 15:22:25 INFO TaskSetManager: Starting task 2.0 in stage 73.0 (TID 157) (192.168.2.250, executor 1, partition 2, ANY, 7690 bytes) \n",
      "24/02/21 15:22:25 INFO BlockManagerInfo: Removed broadcast_69_piece0 on host-192-168-2-13-de1:10005 in memory (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:22:25 INFO BlockManagerInfo: Added broadcast_73_piece0 in memory on 192.168.2.251:10006 (size: 5.7 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:22:25 INFO BlockManagerInfo: Removed broadcast_69_piece0 on 192.168.2.251:10006 in memory (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:22:25 INFO BlockManagerInfo: Removed broadcast_69_piece0 on 192.168.2.250:10006 in memory (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:22:25 INFO BlockManagerInfo: Added broadcast_73_piece0 in memory on 192.168.2.250:10006 (size: 5.7 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:22:25 INFO BlockManagerInfo: Removed broadcast_71_piece0 on 192.168.2.250:10006 in memory (size: 5.2 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:22:25 INFO BlockManagerInfo: Removed broadcast_71_piece0 on host-192-168-2-13-de1:10005 in memory (size: 5.2 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:22:25 INFO BlockManager: Removing RDD 120\n",
      "24/02/21 15:22:25 INFO BlockManager: Removing RDD 119\n",
      "24/02/21 15:22:25 INFO BlockManagerInfo: Removed broadcast_70_piece0 on 192.168.2.251:10006 in memory (size: 5.2 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:22:25 INFO BlockManagerInfo: Removed broadcast_70_piece0 on host-192-168-2-13-de1:10005 in memory (size: 5.2 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:22:25 INFO TaskSetManager: Finished task 2.0 in stage 73.0 (TID 157) in 450 ms on 192.168.2.250 (executor 1) (1/3)\n",
      "24/02/21 15:22:28 INFO TaskSetManager: Finished task 0.0 in stage 73.0 (TID 155) in 3017 ms on 192.168.2.250 (executor 1) (2/3)\n",
      "[Stage 73:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed line count in English: 1862234\n",
      "Processed line count in Swedish: 1862234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/21 15:22:28 INFO TaskSetManager: Finished task 1.0 in stage 73.0 (TID 156) in 3309 ms on 192.168.2.251 (executor 0) (3/3)\n",
      "24/02/21 15:22:28 INFO TaskSchedulerImpl: Removed TaskSet 73.0, whose tasks have all completed, from pool \n",
      "24/02/21 15:22:28 INFO DAGScheduler: ResultStage 73 (count at /tmp/ipykernel_33873/226898882.py:4) finished in 3.335 s\n",
      "24/02/21 15:22:28 INFO DAGScheduler: Job 55 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/21 15:22:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 73: Stage finished\n",
      "24/02/21 15:22:28 INFO DAGScheduler: Job 55 finished: count at /tmp/ipykernel_33873/226898882.py:4, took 3.342273 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Verify that the line counts still match after the pre-processing:\n",
    "\n",
    "processed_count_en = processed_lines_en.count()\n",
    "processed_count_sv = processed_lines_sv.count()\n",
    "print(f\"Processed line count in English: {processed_count_en}\")\n",
    "print(f\"Processed line count in Swedish: {processed_count_sv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "35e03978-73f7-4c4e-916b-27748e5eab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processed line count in English: 1862234\n",
    "# Processed line count in Swedish: 1862234\n",
    "\n",
    "# The line count still match after pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8790a495-45aa-4bee-9422-a09f1de9a51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/21 15:22:40 INFO SparkContext: Starting job: takeOrdered at /tmp/ipykernel_33873/2132416953.py:9\n",
      "24/02/21 15:22:40 INFO DAGScheduler: Registering RDD 139 (reduceByKey at /tmp/ipykernel_33873/2132416953.py:6) as input to shuffle 18\n",
      "24/02/21 15:22:40 INFO DAGScheduler: Got job 56 (takeOrdered at /tmp/ipykernel_33873/2132416953.py:9) with 2 output partitions\n",
      "24/02/21 15:22:40 INFO DAGScheduler: Final stage: ResultStage 75 (takeOrdered at /tmp/ipykernel_33873/2132416953.py:9)\n",
      "24/02/21 15:22:40 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 74)\n",
      "24/02/21 15:22:40 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 74)\n",
      "24/02/21 15:22:40 INFO DAGScheduler: Submitting ShuffleMapStage 74 (PairwiseRDD[139] at reduceByKey at /tmp/ipykernel_33873/2132416953.py:6), which has no missing parents\n",
      "24/02/21 15:22:40 INFO MemoryStore: Block broadcast_74 stored as values in memory (estimated size 12.9 KiB, free 433.9 MiB)\n",
      "24/02/21 15:22:40 INFO MemoryStore: Block broadcast_74_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 433.9 MiB)\n",
      "24/02/21 15:22:40 INFO BlockManagerInfo: Added broadcast_74_piece0 in memory on host-192-168-2-13-de1:10005 (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:22:40 INFO SparkContext: Created broadcast 74 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/21 15:22:40 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 74 (PairwiseRDD[139] at reduceByKey at /tmp/ipykernel_33873/2132416953.py:6) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/02/21 15:22:40 INFO TaskSchedulerImpl: Adding task set 74.0 with 2 tasks resource profile 0\n",
      "24/02/21 15:22:40 INFO TaskSetManager: Starting task 1.0 in stage 74.0 (TID 158) (192.168.2.250, executor 1, partition 1, NODE_LOCAL, 7679 bytes) \n",
      "24/02/21 15:22:40 INFO BlockManagerInfo: Added broadcast_74_piece0 in memory on 192.168.2.250:10006 (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:22:43 INFO TaskSetManager: Starting task 0.0 in stage 74.0 (TID 159) (192.168.2.250, executor 1, partition 0, ANY, 7679 bytes) \n",
      "24/02/21 15:22:55 INFO TaskSetManager: Finished task 1.0 in stage 74.0 (TID 158) in 14575 ms on 192.168.2.250 (executor 1) (1/2)\n",
      "24/02/21 15:22:58 INFO TaskSetManager: Finished task 0.0 in stage 74.0 (TID 159) in 14116 ms on 192.168.2.250 (executor 1) (2/2)\n",
      "24/02/21 15:22:58 INFO TaskSchedulerImpl: Removed TaskSet 74.0, whose tasks have all completed, from pool \n",
      "24/02/21 15:22:58 INFO DAGScheduler: ShuffleMapStage 74 (reduceByKey at /tmp/ipykernel_33873/2132416953.py:6) finished in 17.562 s\n",
      "24/02/21 15:22:58 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/02/21 15:22:58 INFO DAGScheduler: running: Set()\n",
      "24/02/21 15:22:58 INFO DAGScheduler: waiting: Set(ResultStage 75)\n",
      "24/02/21 15:22:58 INFO DAGScheduler: failed: Set()\n",
      "24/02/21 15:22:58 INFO DAGScheduler: Submitting ResultStage 75 (PythonRDD[146] at takeOrdered at /tmp/ipykernel_33873/2132416953.py:9), which has no missing parents\n",
      "24/02/21 15:22:58 INFO MemoryStore: Block broadcast_75 stored as values in memory (estimated size 11.1 KiB, free 433.9 MiB)\n",
      "24/02/21 15:22:58 INFO MemoryStore: Block broadcast_75_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 433.9 MiB)\n",
      "24/02/21 15:22:58 INFO BlockManagerInfo: Added broadcast_75_piece0 in memory on host-192-168-2-13-de1:10005 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:22:58 INFO SparkContext: Created broadcast 75 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/21 15:22:58 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 75 (PythonRDD[146] at takeOrdered at /tmp/ipykernel_33873/2132416953.py:9) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/02/21 15:22:58 INFO TaskSchedulerImpl: Adding task set 75.0 with 2 tasks resource profile 0\n",
      "24/02/21 15:22:58 INFO TaskSetManager: Starting task 0.0 in stage 75.0 (TID 160) (192.168.2.250, executor 1, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/02/21 15:22:58 INFO TaskSetManager: Starting task 1.0 in stage 75.0 (TID 161) (192.168.2.250, executor 1, partition 1, NODE_LOCAL, 7437 bytes) \n",
      "24/02/21 15:22:58 INFO BlockManagerInfo: Added broadcast_75_piece0 in memory on 192.168.2.250:10006 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:22:58 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 18 to 192.168.2.250:54648\n",
      "24/02/21 15:22:58 INFO TaskSetManager: Finished task 0.0 in stage 75.0 (TID 160) in 168 ms on 192.168.2.250 (executor 1) (1/2)\n",
      "24/02/21 15:22:58 INFO TaskSetManager: Finished task 1.0 in stage 75.0 (TID 161) in 178 ms on 192.168.2.250 (executor 1) (2/2)\n",
      "24/02/21 15:22:58 INFO TaskSchedulerImpl: Removed TaskSet 75.0, whose tasks have all completed, from pool \n",
      "24/02/21 15:22:58 INFO DAGScheduler: ResultStage 75 (takeOrdered at /tmp/ipykernel_33873/2132416953.py:9) finished in 0.189 s\n",
      "24/02/21 15:22:58 INFO DAGScheduler: Job 56 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/21 15:22:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 75: Stage finished\n",
      "24/02/21 15:22:58 INFO DAGScheduler: Job 56 finished: takeOrdered at /tmp/ipykernel_33873/2132416953.py:9, took 17.763645 s\n",
      "24/02/21 15:22:58 INFO SparkContext: Starting job: takeOrdered at /tmp/ipykernel_33873/2132416953.py:10\n",
      "24/02/21 15:22:58 INFO DAGScheduler: Registering RDD 143 (reduceByKey at /tmp/ipykernel_33873/2132416953.py:7) as input to shuffle 19\n",
      "24/02/21 15:22:58 INFO DAGScheduler: Got job 57 (takeOrdered at /tmp/ipykernel_33873/2132416953.py:10) with 3 output partitions\n",
      "24/02/21 15:22:58 INFO DAGScheduler: Final stage: ResultStage 77 (takeOrdered at /tmp/ipykernel_33873/2132416953.py:10)\n",
      "24/02/21 15:22:58 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 76)\n",
      "24/02/21 15:22:58 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 76)\n",
      "24/02/21 15:22:58 INFO DAGScheduler: Submitting ShuffleMapStage 76 (PairwiseRDD[143] at reduceByKey at /tmp/ipykernel_33873/2132416953.py:7), which has no missing parents\n",
      "24/02/21 15:22:58 INFO MemoryStore: Block broadcast_76 stored as values in memory (estimated size 12.9 KiB, free 433.8 MiB)\n",
      "24/02/21 15:22:58 INFO BlockManagerInfo: Removed broadcast_73_piece0 on host-192-168-2-13-de1:10005 in memory (size: 5.7 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:22:58 INFO MemoryStore: Block broadcast_76_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 433.8 MiB)\n",
      "24/02/21 15:22:58 INFO BlockManagerInfo: Added broadcast_76_piece0 in memory on host-192-168-2-13-de1:10005 (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:22:58 INFO SparkContext: Created broadcast 76 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/21 15:22:58 INFO BlockManagerInfo: Removed broadcast_73_piece0 on 192.168.2.250:10006 in memory (size: 5.7 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:22:58 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 76 (PairwiseRDD[143] at reduceByKey at /tmp/ipykernel_33873/2132416953.py:7) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/02/21 15:22:58 INFO BlockManagerInfo: Removed broadcast_73_piece0 on 192.168.2.251:10006 in memory (size: 5.7 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:22:58 INFO TaskSchedulerImpl: Adding task set 76.0 with 3 tasks resource profile 0\n",
      "24/02/21 15:22:58 INFO TaskSetManager: Starting task 0.0 in stage 76.0 (TID 162) (192.168.2.250, executor 1, partition 0, ANY, 7679 bytes) \n",
      "24/02/21 15:22:58 INFO TaskSetManager: Starting task 1.0 in stage 76.0 (TID 163) (192.168.2.251, executor 0, partition 1, ANY, 7679 bytes) \n",
      "24/02/21 15:22:58 INFO TaskSetManager: Starting task 2.0 in stage 76.0 (TID 164) (192.168.2.250, executor 1, partition 2, ANY, 7679 bytes) \n",
      "24/02/21 15:22:58 INFO BlockManagerInfo: Removed broadcast_74_piece0 on host-192-168-2-13-de1:10005 in memory (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:22:58 INFO BlockManagerInfo: Removed broadcast_74_piece0 on 192.168.2.250:10006 in memory (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:22:58 INFO BlockManagerInfo: Added broadcast_76_piece0 in memory on 192.168.2.250:10006 (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:22:58 INFO BlockManagerInfo: Added broadcast_76_piece0 in memory on 192.168.2.251:10006 (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:22:58 INFO BlockManagerInfo: Removed broadcast_75_piece0 on host-192-168-2-13-de1:10005 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:22:58 INFO BlockManagerInfo: Removed broadcast_75_piece0 on 192.168.2.250:10006 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:23:00 INFO TaskSetManager: Finished task 2.0 in stage 76.0 (TID 164) in 1991 ms on 192.168.2.250 (executor 1) (1/3)\n",
      "24/02/21 15:23:12 INFO TaskSetManager: Finished task 0.0 in stage 76.0 (TID 162) in 14122 ms on 192.168.2.250 (executor 1) (2/3)\n",
      "24/02/21 15:23:16 INFO TaskSetManager: Finished task 1.0 in stage 76.0 (TID 163) in 17728 ms on 192.168.2.251 (executor 0) (3/3)\n",
      "24/02/21 15:23:16 INFO TaskSchedulerImpl: Removed TaskSet 76.0, whose tasks have all completed, from pool \n",
      "24/02/21 15:23:16 INFO DAGScheduler: ShuffleMapStage 76 (reduceByKey at /tmp/ipykernel_33873/2132416953.py:7) finished in 17.744 s\n",
      "24/02/21 15:23:16 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/02/21 15:23:16 INFO DAGScheduler: running: Set()\n",
      "24/02/21 15:23:16 INFO DAGScheduler: waiting: Set(ResultStage 77)\n",
      "24/02/21 15:23:16 INFO DAGScheduler: failed: Set()\n",
      "24/02/21 15:23:16 INFO DAGScheduler: Submitting ResultStage 77 (PythonRDD[147] at takeOrdered at /tmp/ipykernel_33873/2132416953.py:10), which has no missing parents\n",
      "24/02/21 15:23:16 INFO MemoryStore: Block broadcast_77 stored as values in memory (estimated size 11.1 KiB, free 433.9 MiB)\n",
      "24/02/21 15:23:16 INFO MemoryStore: Block broadcast_77_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 433.9 MiB)\n",
      "24/02/21 15:23:16 INFO BlockManagerInfo: Added broadcast_77_piece0 in memory on host-192-168-2-13-de1:10005 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:23:16 INFO SparkContext: Created broadcast 77 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/21 15:23:16 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 77 (PythonRDD[147] at takeOrdered at /tmp/ipykernel_33873/2132416953.py:10) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/02/21 15:23:16 INFO TaskSchedulerImpl: Adding task set 77.0 with 3 tasks resource profile 0\n",
      "24/02/21 15:23:16 INFO TaskSetManager: Starting task 0.0 in stage 77.0 (TID 165) (192.168.2.251, executor 0, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/02/21 15:23:16 INFO TaskSetManager: Starting task 1.0 in stage 77.0 (TID 166) (192.168.2.250, executor 1, partition 1, NODE_LOCAL, 7437 bytes) \n",
      "24/02/21 15:23:16 INFO TaskSetManager: Starting task 2.0 in stage 77.0 (TID 167) (192.168.2.251, executor 0, partition 2, NODE_LOCAL, 7437 bytes) \n",
      "24/02/21 15:23:16 INFO BlockManagerInfo: Added broadcast_77_piece0 in memory on 192.168.2.251:10006 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:23:16 INFO BlockManagerInfo: Added broadcast_77_piece0 in memory on 192.168.2.250:10006 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:23:16 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 19 to 192.168.2.251:40148\n",
      "24/02/21 15:23:16 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 19 to 192.168.2.250:54648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 words in English:\n",
      "the: 3498574\n",
      "of: 1659884\n",
      "to: 1539823\n",
      "and: 1288620\n",
      "in: 1086089\n",
      "that: 797576\n",
      "a: 773812\n",
      "is: 758087\n",
      "for: 534270\n",
      "we: 522879\n",
      "\n",
      "Top 10 words in Swedish:\n",
      "att: 1706309\n",
      "och: 1344895\n",
      "i: 1050989\n",
      "det: 924878\n",
      "som: 913302\n",
      "för: 908703\n",
      "av: 738102\n",
      "är: 694389\n",
      "en: 620347\n",
      "vi: 539808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/21 15:23:16 INFO TaskSetManager: Finished task 2.0 in stage 77.0 (TID 167) in 275 ms on 192.168.2.251 (executor 0) (1/3)\n",
      "24/02/21 15:23:16 INFO TaskSetManager: Finished task 1.0 in stage 77.0 (TID 166) in 296 ms on 192.168.2.250 (executor 1) (2/3)\n",
      "24/02/21 15:23:16 INFO TaskSetManager: Finished task 0.0 in stage 77.0 (TID 165) in 298 ms on 192.168.2.251 (executor 0) (3/3)\n",
      "24/02/21 15:23:16 INFO TaskSchedulerImpl: Removed TaskSet 77.0, whose tasks have all completed, from pool \n",
      "24/02/21 15:23:16 INFO DAGScheduler: ResultStage 77 (takeOrdered at /tmp/ipykernel_33873/2132416953.py:10) finished in 0.315 s\n",
      "24/02/21 15:23:16 INFO DAGScheduler: Job 57 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/21 15:23:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 77: Stage finished\n",
      "24/02/21 15:23:16 INFO DAGScheduler: Job 57 finished: takeOrdered at /tmp/ipykernel_33873/2132416953.py:10, took 18.067333 s\n",
      "24/02/21 15:41:53 INFO BlockManagerInfo: Removed broadcast_77_piece0 on 192.168.2.251:10006 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:41:53 INFO BlockManagerInfo: Removed broadcast_77_piece0 on host-192-168-2-13-de1:10005 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:41:53 INFO BlockManagerInfo: Removed broadcast_77_piece0 on 192.168.2.250:10006 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:41:53 INFO BlockManagerInfo: Removed broadcast_76_piece0 on host-192-168-2-13-de1:10005 in memory (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:41:53 INFO BlockManagerInfo: Removed broadcast_76_piece0 on 192.168.2.251:10006 in memory (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/21 15:41:53 INFO BlockManagerInfo: Removed broadcast_76_piece0 on 192.168.2.250:10006 in memory (size: 7.8 KiB, free: 434.3 MiB)\n"
     ]
    }
   ],
   "source": [
    "# Check 10 most frequent words in English and Swedish\n",
    "\n",
    "words_en = processed_lines_en.flatMap(lambda line: line)\n",
    "words_sv = processed_lines_sv.flatMap(lambda line: line)\n",
    "\n",
    "word_counts_en = words_en.map(lambda word: (word, 1)).reduceByKey(add)\n",
    "word_counts_sv = words_sv.map(lambda word: (word, 1)).reduceByKey(add)\n",
    "\n",
    "top_10_words_en = word_counts_en.takeOrdered(10, key=lambda x: -x[1])\n",
    "top_10_words_sv = word_counts_sv.takeOrdered(10, key=lambda x: -x[1])\n",
    "\n",
    "print(\"\\nTop 10 words in English:\")\n",
    "for word, count in top_10_words_en:\n",
    "    print(f'{word}: {count}')\n",
    "    \n",
    "print(\"\\nTop 10 words in Swedish:\")\n",
    "for word, count in top_10_words_sv:\n",
    "    print(f'{word}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9b84eae1-31fb-4c21-9258-1c814012defb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 words in English corpus: [('the', 3498375), ('of', 1659758), ('to', 1539760), ('and', 1288401), ('in', 1085993), \n",
    "#                                  ('that', 797516), ('a', 773522), ('is', 758050), ('for', 534242), ('we', 522849)]\n",
    "# Top 10 words in Swedish corpus: [('att', 1706293), ('och', 1344830), ('i', 1050774), ('det', 924866), ('som', 913276), \n",
    "#                                  ('för', 908680), ('av', 738068), ('är', 694381), ('en', 620310), ('vi', 539797)]\n",
    "\n",
    "# OBS!!! KOLLA DETTA SÅ DET STÄMMER, DETTA ÄR OM DET ÄR SPLIT(' ') MEN SAGAS RESULTAT ÄR OM DET ÄR SPLIT() i preprocessing delen\n",
    "# Top 10 words in English:\n",
    "# the: 3498375\n",
    "# of: 1659758\n",
    "# to: 1539760\n",
    "# and: 1288401\n",
    "# in: 1085993\n",
    "# that: 797516\n",
    "# a: 773522\n",
    "# is: 758050\n",
    "# for: 534242\n",
    "# we: 522849\n",
    "\n",
    "# Top 10 words in Swedish:\n",
    "# att: 1706293\n",
    "# och: 1344830\n",
    "# i: 1050774\n",
    "# det: 924866\n",
    "# som: 913276\n",
    "# för: 908680\n",
    "# av: 738068\n",
    "# är: 694381\n",
    "# en: 620310\n",
    "# vi: 539797\n",
    "\n",
    "# The results are reasonable, as the most frequent words are common words used to build up sentences (konjunctions, prepositions etc)\n",
    "# In the Swedish translation for example, the words also match most of the top 10 most frequent words in the Swedish language like i, och, att, det, som, en, är, av, för "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca90554-9314-4363-a079-901028a989f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65eda738-e27a-469f-a4fd-900ec739f5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/21 13:10:22 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
      "24/02/21 13:10:22 INFO SparkUI: Stopped Spark web UI at http://host-192-168-2-13-de1:4040\n",
      "24/02/21 13:10:22 INFO StandaloneSchedulerBackend: Shutting down all executors\n",
      "24/02/21 13:10:22 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down\n",
      "24/02/21 13:10:22 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "24/02/21 13:10:22 INFO MemoryStore: MemoryStore cleared\n",
      "24/02/21 13:10:22 INFO BlockManager: BlockManager stopped\n",
      "24/02/21 13:10:22 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "24/02/21 13:10:22 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "24/02/21 13:10:22 INFO SparkContext: Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "# release the cores for another application!\n",
    "# spark_context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6167cac4-7534-4a8b-a217-f06a4a2b5df5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
