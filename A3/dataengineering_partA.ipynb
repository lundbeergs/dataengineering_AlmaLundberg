{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fcb40689-fcac-4014-9f17-8111fd417fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/26 20:58:29 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "24/02/26 20:58:29 INFO SharedState: Warehouse path is 'file:/home/ubuntu/dataengineering_AlmaLundberg/A3/spark-warehouse'.\n",
      "24/02/26 20:58:30 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 192.168.2.252:10005 in memory (size: 8.8 KiB, free: 434.3 MiB)\n",
      "24/02/26 20:58:30 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 192.168.2.250:10005 in memory (size: 8.8 KiB, free: 434.3 MiB)\n",
      "24/02/26 20:58:30 INFO BlockManagerInfo: Removed broadcast_18_piece0 on host-192-168-2-136-de1:10005 in memory (size: 8.8 KiB, free: 434.3 MiB)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://192.168.2.250:7077\") \\\n",
    "        .appName(\"AlmaLundberg_Sparkapplication\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", False)\\\n",
    "        .config(\"spark.cores.max\", 4)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\",2)\\\n",
    "        .config(\"spark.driver.port\",9999)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# RDD API\n",
    "spark_context = spark_session.sparkContext\n",
    "\n",
    "spark_context.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ffafb5a6-41c1-490f-a5ac-6bdcc88d364f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/26 20:58:32 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 221.7 KiB, free 433.7 MiB)\n",
      "24/02/26 20:58:32 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 433.7 MiB)\n",
      "24/02/26 20:58:32 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on host-192-168-2-136-de1:10005 (size: 32.7 KiB, free: 434.3 MiB)\n",
      "24/02/26 20:58:32 INFO SparkContext: Created broadcast 19 from textFile at NativeMethodAccessorImpl.java:0\n",
      "24/02/26 20:58:32 INFO FileInputFormat: Total input files to process : 1\n",
      "24/02/26 20:58:32 INFO NetworkTopology: Adding a new node: /default-rack/192.168.2.250:9866\n",
      "24/02/26 20:58:32 INFO SparkContext: Starting job: count at /tmp/ipykernel_31804/3631255496.py:4\n",
      "24/02/26 20:58:32 INFO DAGScheduler: Got job 15 (count at /tmp/ipykernel_31804/3631255496.py:4) with 2 output partitions\n",
      "24/02/26 20:58:32 INFO DAGScheduler: Final stage: ResultStage 21 (count at /tmp/ipykernel_31804/3631255496.py:4)\n",
      "24/02/26 20:58:32 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/02/26 20:58:32 INFO DAGScheduler: Missing parents: List()\n",
      "24/02/26 20:58:32 INFO DAGScheduler: Submitting ResultStage 21 (PythonRDD[40] at count at /tmp/ipykernel_31804/3631255496.py:4), which has no missing parents\n",
      "24/02/26 20:58:32 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 9.1 KiB, free 433.6 MiB)\n",
      "24/02/26 20:58:32 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 433.6 MiB)\n",
      "24/02/26 20:58:32 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on host-192-168-2-136-de1:10005 (size: 5.5 KiB, free: 434.3 MiB)\n",
      "24/02/26 20:58:32 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/26 20:58:32 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 21 (PythonRDD[40] at count at /tmp/ipykernel_31804/3631255496.py:4) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/02/26 20:58:32 INFO TaskSchedulerImpl: Adding task set 21.0 with 2 tasks resource profile 0\n",
      "24/02/26 20:58:32 INFO TaskSetManager: Starting task 1.0 in stage 21.0 (TID 76) (192.168.2.250, executor 1, partition 1, NODE_LOCAL, 7690 bytes) \n",
      "24/02/26 20:58:32 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 192.168.2.250:10005 (size: 5.5 KiB, free: 434.3 MiB)\n",
      "24/02/26 20:58:32 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 192.168.2.250:10005 (size: 32.7 KiB, free: 434.3 MiB)\n",
      "24/02/26 20:58:33 INFO TaskSetManager: Finished task 1.0 in stage 21.0 (TID 76) in 723 ms on 192.168.2.250 (executor 1) (1/2)\n",
      "24/02/26 20:58:36 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 77) (192.168.2.250, executor 1, partition 0, ANY, 7690 bytes) \n",
      "24/02/26 20:58:37 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 77) in 670 ms on 192.168.2.250 (executor 1) (2/2)\n",
      "24/02/26 20:58:37 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool \n",
      "24/02/26 20:58:37 INFO DAGScheduler: ResultStage 21 (count at /tmp/ipykernel_31804/3631255496.py:4) finished in 4.543 s\n",
      "24/02/26 20:58:37 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/26 20:58:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished\n",
      "24/02/26 20:58:37 INFO DAGScheduler: Job 15 finished: count at /tmp/ipykernel_31804/3631255496.py:4, took 4.548855 s\n",
      "24/02/26 20:58:37 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 221.7 KiB, free 433.4 MiB)\n",
      "24/02/26 20:58:37 INFO BlockManagerInfo: Removed broadcast_20_piece0 on host-192-168-2-136-de1:10005 in memory (size: 5.5 KiB, free: 434.3 MiB)\n",
      "24/02/26 20:58:37 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 192.168.2.250:10005 in memory (size: 5.5 KiB, free: 434.3 MiB)\n",
      "24/02/26 20:58:37 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 433.4 MiB)\n",
      "24/02/26 20:58:37 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on host-192-168-2-136-de1:10005 (size: 32.7 KiB, free: 434.3 MiB)\n",
      "24/02/26 20:58:37 INFO SparkContext: Created broadcast 21 from textFile at NativeMethodAccessorImpl.java:0\n",
      "24/02/26 20:58:37 INFO FileInputFormat: Total input files to process : 1\n",
      "24/02/26 20:58:37 INFO SparkContext: Starting job: count at /tmp/ipykernel_31804/3631255496.py:8\n",
      "24/02/26 20:58:37 INFO DAGScheduler: Got job 16 (count at /tmp/ipykernel_31804/3631255496.py:8) with 3 output partitions\n",
      "24/02/26 20:58:37 INFO DAGScheduler: Final stage: ResultStage 22 (count at /tmp/ipykernel_31804/3631255496.py:8)\n",
      "24/02/26 20:58:37 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/02/26 20:58:37 INFO DAGScheduler: Missing parents: List()\n",
      "24/02/26 20:58:37 INFO DAGScheduler: Submitting ResultStage 22 (PythonRDD[43] at count at /tmp/ipykernel_31804/3631255496.py:8), which has no missing parents\n",
      "24/02/26 20:58:37 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 9.1 KiB, free 433.4 MiB)\n",
      "24/02/26 20:58:37 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 433.4 MiB)\n",
      "24/02/26 20:58:37 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on host-192-168-2-136-de1:10005 (size: 5.5 KiB, free: 434.3 MiB)\n",
      "24/02/26 20:58:37 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/26 20:58:37 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 22 (PythonRDD[43] at count at /tmp/ipykernel_31804/3631255496.py:8) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/02/26 20:58:37 INFO TaskSchedulerImpl: Adding task set 22.0 with 3 tasks resource profile 0\n",
      "24/02/26 20:58:37 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 78) (192.168.2.250, executor 1, partition 0, ANY, 7690 bytes) \n",
      "24/02/26 20:58:37 INFO TaskSetManager: Starting task 1.0 in stage 22.0 (TID 79) (192.168.2.252, executor 0, partition 1, ANY, 7690 bytes) \n",
      "24/02/26 20:58:37 INFO TaskSetManager: Starting task 2.0 in stage 22.0 (TID 80) (192.168.2.250, executor 1, partition 2, ANY, 7690 bytes) \n",
      "24/02/26 20:58:37 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 192.168.2.250:10005 (size: 5.5 KiB, free: 434.3 MiB)\n",
      "24/02/26 20:58:37 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 192.168.2.252:10005 (size: 5.5 KiB, free: 434.3 MiB)\n",
      "24/02/26 20:58:37 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 192.168.2.250:10005 (size: 32.7 KiB, free: 434.3 MiB)\n",
      "24/02/26 20:58:37 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 192.168.2.252:10005 (size: 32.7 KiB, free: 434.3 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines in English file: 1862234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/26 20:58:37 INFO TaskSetManager: Finished task 2.0 in stage 22.0 (TID 80) in 236 ms on 192.168.2.250 (executor 1) (1/3)\n",
      "[Stage 22:===================>                                      (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines in Swedish file: 1862234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/26 20:58:38 INFO TaskSetManager: Finished task 1.0 in stage 22.0 (TID 79) in 1098 ms on 192.168.2.252 (executor 0) (2/3)\n",
      "24/02/26 20:58:38 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 78) in 1281 ms on 192.168.2.250 (executor 1) (3/3)\n",
      "24/02/26 20:58:38 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool \n",
      "24/02/26 20:58:38 INFO DAGScheduler: ResultStage 22 (count at /tmp/ipykernel_31804/3631255496.py:8) finished in 1.295 s\n",
      "24/02/26 20:58:38 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/26 20:58:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished\n",
      "24/02/26 20:58:38 INFO DAGScheduler: Job 16 finished: count at /tmp/ipykernel_31804/3631255496.py:8, took 1.298727 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Using map and reduce from the Spark API, and loading the text file from HDFS.\n",
    "\n",
    "lines_en = spark_context.textFile(\"hdfs://192.168.2.250:9000/europarl/europarl-v7.sv-en.en\")\n",
    "num_lines_en = lines_en.count()\n",
    "print(f\"Total lines in English file: {num_lines_en}\") \n",
    "\n",
    "lines_sv = spark_context.textFile(\"hdfs://192.168.2.250:9000/europarl/europarl-v7.sv-en.sv\")\n",
    "num_lines_sv = lines_sv.count()\n",
    "print(f\"Total lines in Swedish file: {num_lines_sv}\")\n",
    "\n",
    "\n",
    "# print(lines.first())\n",
    "# words = lines.map(lambda line: line.split(' '))\n",
    "# word_counts = words.map(lambda w: len(w))\n",
    "# total_words = word_counts.reduce(add)\n",
    "# print(f'total words= {total_words}') \n",
    "\n",
    "#lines = spark_context.textFile(\"/home/ubuntu/i_have_a_dream.txt\")\n",
    "# print(lines.first())\n",
    "# words = lines.map(lambda line: line.split(' '))\n",
    "# word_counts = words.map(lambda w: len(w))\n",
    "# total_words = word_counts.reduce(add)\n",
    "# print(f'total words= {total_words}')  \n",
    "# ... the same number of words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0cbef32-5bfa-4190-8bc1-1c4b972a00ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total lines in English file: 1862234\n",
    "# Total lines in Swedish file: 1862234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d574749-5a3d-4e1c-b3e9-efef1a078fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions in English file: 2\n",
      "Number of partitions in Swedish file: 3\n"
     ]
    }
   ],
   "source": [
    "# Counting nmr of partitions in the English file\n",
    "num_partitions_en = lines_en.getNumPartitions()\n",
    "print(f\"Number of partitions in English file: {num_partitions_en}\")\n",
    "\n",
    "# Counting nmr of partitions in the Swedish file\n",
    "num_partitions_sv = lines_sv.getNumPartitions()\n",
    "print(f\"Number of partitions in Swedish file: {num_partitions_sv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d864eddf-cc03-4058-82c3-5d3f11ad1b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of partitions in English file: 2\n",
    "# Number of partitions in Swedish file: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fa87b66a-46a1-49eb-9f87-4666a12f237e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(line):\n",
    "    return line.lower().split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a2a51f26-9c3c-466c-a409-a5b5bb5abb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_lines_en = lines_en.map(preprocess_text)\n",
    "processed_lines_sv = lines_sv.map(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae1159d-545c-4285-a260-a9f770979069",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8cee82ee-1bbb-4521-b47e-c3c4982f9b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/26 21:42:04 INFO SparkContext: Starting job: count at /tmp/ipykernel_31804/226898882.py:3\n",
      "24/02/26 21:42:04 INFO DAGScheduler: Got job 46 (count at /tmp/ipykernel_31804/226898882.py:3) with 2 output partitions\n",
      "24/02/26 21:42:04 INFO DAGScheduler: Final stage: ResultStage 63 (count at /tmp/ipykernel_31804/226898882.py:3)\n",
      "24/02/26 21:42:04 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/02/26 21:42:04 INFO DAGScheduler: Missing parents: List()\n",
      "24/02/26 21:42:04 INFO DAGScheduler: Submitting ResultStage 63 (PythonRDD[104] at count at /tmp/ipykernel_31804/226898882.py:3), which has no missing parents\n",
      "24/02/26 21:42:04 INFO MemoryStore: Block broadcast_59 stored as values in memory (estimated size 9.6 KiB, free 433.4 MiB)\n",
      "24/02/26 21:42:04 INFO BlockManagerInfo: Removed broadcast_58_piece0 on host-192-168-2-136-de1:10005 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:42:04 INFO MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 433.4 MiB)\n",
      "24/02/26 21:42:04 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on host-192-168-2-136-de1:10005 (size: 5.7 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:42:04 INFO BlockManagerInfo: Removed broadcast_58_piece0 on 192.168.2.250:10005 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:42:04 INFO BlockManagerInfo: Removed broadcast_58_piece0 on 192.168.2.252:10005 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:42:04 INFO SparkContext: Created broadcast 59 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/26 21:42:04 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 63 (PythonRDD[104] at count at /tmp/ipykernel_31804/226898882.py:3) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/02/26 21:42:04 INFO TaskSchedulerImpl: Adding task set 63.0 with 2 tasks resource profile 0\n",
      "24/02/26 21:42:04 INFO TaskSetManager: Starting task 1.0 in stage 63.0 (TID 163) (192.168.2.250, executor 1, partition 1, NODE_LOCAL, 7690 bytes) \n",
      "24/02/26 21:42:04 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on 192.168.2.250:10005 (size: 5.7 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:42:06 INFO TaskSetManager: Finished task 1.0 in stage 63.0 (TID 163) in 1968 ms on 192.168.2.250 (executor 1) (1/2)\n",
      "24/02/26 21:42:07 INFO TaskSetManager: Starting task 0.0 in stage 63.0 (TID 164) (192.168.2.252, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/02/26 21:42:07 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on 192.168.2.252:10005 (size: 5.7 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:42:09 INFO TaskSetManager: Finished task 0.0 in stage 63.0 (TID 164) in 1892 ms on 192.168.2.252 (executor 0) (2/2)\n",
      "24/02/26 21:42:09 INFO TaskSchedulerImpl: Removed TaskSet 63.0, whose tasks have all completed, from pool \n",
      "24/02/26 21:42:09 INFO DAGScheduler: ResultStage 63 (count at /tmp/ipykernel_31804/226898882.py:3) finished in 5.546 s\n",
      "24/02/26 21:42:09 INFO DAGScheduler: Job 46 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/26 21:42:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 63: Stage finished\n",
      "24/02/26 21:42:09 INFO DAGScheduler: Job 46 finished: count at /tmp/ipykernel_31804/226898882.py:3, took 5.550234 s\n",
      "24/02/26 21:42:09 INFO SparkContext: Starting job: count at /tmp/ipykernel_31804/226898882.py:4\n",
      "24/02/26 21:42:09 INFO DAGScheduler: Got job 47 (count at /tmp/ipykernel_31804/226898882.py:4) with 3 output partitions\n",
      "24/02/26 21:42:09 INFO DAGScheduler: Final stage: ResultStage 64 (count at /tmp/ipykernel_31804/226898882.py:4)\n",
      "24/02/26 21:42:09 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/02/26 21:42:09 INFO DAGScheduler: Missing parents: List()\n",
      "24/02/26 21:42:09 INFO DAGScheduler: Submitting ResultStage 64 (PythonRDD[105] at count at /tmp/ipykernel_31804/226898882.py:4), which has no missing parents\n",
      "24/02/26 21:42:09 INFO MemoryStore: Block broadcast_60 stored as values in memory (estimated size 9.6 KiB, free 433.4 MiB)\n",
      "24/02/26 21:42:09 INFO BlockManagerInfo: Removed broadcast_59_piece0 on host-192-168-2-136-de1:10005 in memory (size: 5.7 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:42:09 INFO MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 433.4 MiB)\n",
      "24/02/26 21:42:09 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on host-192-168-2-136-de1:10005 (size: 5.7 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:42:09 INFO SparkContext: Created broadcast 60 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/26 21:42:09 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 64 (PythonRDD[105] at count at /tmp/ipykernel_31804/226898882.py:4) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/02/26 21:42:09 INFO TaskSchedulerImpl: Adding task set 64.0 with 3 tasks resource profile 0\n",
      "24/02/26 21:42:09 INFO BlockManagerInfo: Removed broadcast_59_piece0 on 192.168.2.252:10005 in memory (size: 5.7 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:42:09 INFO TaskSetManager: Starting task 0.0 in stage 64.0 (TID 165) (192.168.2.250, executor 1, partition 0, ANY, 7690 bytes) \n",
      "24/02/26 21:42:09 INFO TaskSetManager: Starting task 1.0 in stage 64.0 (TID 166) (192.168.2.252, executor 0, partition 1, ANY, 7690 bytes) \n",
      "24/02/26 21:42:09 INFO TaskSetManager: Starting task 2.0 in stage 64.0 (TID 167) (192.168.2.250, executor 1, partition 2, ANY, 7690 bytes) \n",
      "24/02/26 21:42:09 INFO BlockManagerInfo: Removed broadcast_59_piece0 on 192.168.2.250:10005 in memory (size: 5.7 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:42:09 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on 192.168.2.252:10005 (size: 5.7 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:42:09 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on 192.168.2.250:10005 (size: 5.7 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:42:10 INFO TaskSetManager: Finished task 2.0 in stage 64.0 (TID 167) in 448 ms on 192.168.2.250 (executor 1) (1/3)\n",
      "[Stage 64:===================>                                      (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed line count in English: 1862234\n",
      "Processed line count in Swedish: 1862234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/26 21:42:12 INFO TaskSetManager: Finished task 1.0 in stage 64.0 (TID 166) in 2837 ms on 192.168.2.252 (executor 0) (2/3)\n",
      "24/02/26 21:42:12 INFO TaskSetManager: Finished task 0.0 in stage 64.0 (TID 165) in 2840 ms on 192.168.2.250 (executor 1) (3/3)\n",
      "24/02/26 21:42:12 INFO TaskSchedulerImpl: Removed TaskSet 64.0, whose tasks have all completed, from pool \n",
      "24/02/26 21:42:12 INFO DAGScheduler: ResultStage 64 (count at /tmp/ipykernel_31804/226898882.py:4) finished in 2.854 s\n",
      "24/02/26 21:42:12 INFO DAGScheduler: Job 47 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/26 21:42:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 64: Stage finished\n",
      "24/02/26 21:42:12 INFO DAGScheduler: Job 47 finished: count at /tmp/ipykernel_31804/226898882.py:4, took 2.857187 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Verify that the line counts still match after the pre-processing:\n",
    "\n",
    "processed_count_en = processed_lines_en.count()\n",
    "processed_count_sv = processed_lines_sv.count()\n",
    "print(f\"Processed line count in English: {processed_count_en}\")\n",
    "print(f\"Processed line count in Swedish: {processed_count_sv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "35e03978-73f7-4c4e-916b-27748e5eab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processed line count in English: 1862234\n",
    "# Processed line count in Swedish: 1862234\n",
    "\n",
    "# The line count still match after pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8790a495-45aa-4bee-9422-a09f1de9a51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/26 21:42:40 INFO SparkContext: Starting job: takeOrdered at /tmp/ipykernel_31804/2132416953.py:9\n",
      "24/02/26 21:42:40 INFO DAGScheduler: Registering RDD 107 (reduceByKey at /tmp/ipykernel_31804/2132416953.py:6) as input to shuffle 11\n",
      "24/02/26 21:42:40 INFO DAGScheduler: Got job 48 (takeOrdered at /tmp/ipykernel_31804/2132416953.py:9) with 2 output partitions\n",
      "24/02/26 21:42:40 INFO DAGScheduler: Final stage: ResultStage 66 (takeOrdered at /tmp/ipykernel_31804/2132416953.py:9)\n",
      "24/02/26 21:42:40 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 65)\n",
      "24/02/26 21:42:40 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 65)\n",
      "24/02/26 21:42:40 INFO DAGScheduler: Submitting ShuffleMapStage 65 (PairwiseRDD[107] at reduceByKey at /tmp/ipykernel_31804/2132416953.py:6), which has no missing parents\n",
      "24/02/26 21:42:40 INFO MemoryStore: Block broadcast_61 stored as values in memory (estimated size 12.9 KiB, free 433.4 MiB)\n",
      "24/02/26 21:42:40 INFO BlockManagerInfo: Removed broadcast_60_piece0 on host-192-168-2-136-de1:10005 in memory (size: 5.7 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:42:40 INFO MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 433.4 MiB)\n",
      "24/02/26 21:42:40 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on host-192-168-2-136-de1:10005 (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:42:40 INFO BlockManagerInfo: Removed broadcast_60_piece0 on 192.168.2.250:10005 in memory (size: 5.7 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:42:40 INFO BlockManagerInfo: Removed broadcast_60_piece0 on 192.168.2.252:10005 in memory (size: 5.7 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:42:40 INFO SparkContext: Created broadcast 61 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/26 21:42:40 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 65 (PairwiseRDD[107] at reduceByKey at /tmp/ipykernel_31804/2132416953.py:6) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/02/26 21:42:40 INFO TaskSchedulerImpl: Adding task set 65.0 with 2 tasks resource profile 0\n",
      "24/02/26 21:42:40 INFO TaskSetManager: Starting task 1.0 in stage 65.0 (TID 168) (192.168.2.250, executor 1, partition 1, NODE_LOCAL, 7679 bytes) \n",
      "24/02/26 21:42:40 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on 192.168.2.250:10005 (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:42:43 INFO TaskSetManager: Starting task 0.0 in stage 65.0 (TID 169) (192.168.2.250, executor 1, partition 0, ANY, 7679 bytes) \n",
      "24/02/26 21:42:54 INFO TaskSetManager: Finished task 1.0 in stage 65.0 (TID 168) in 14533 ms on 192.168.2.250 (executor 1) (1/2)\n",
      "24/02/26 21:42:57 INFO TaskSetManager: Finished task 0.0 in stage 65.0 (TID 169) in 14134 ms on 192.168.2.250 (executor 1) (2/2)\n",
      "24/02/26 21:42:57 INFO TaskSchedulerImpl: Removed TaskSet 65.0, whose tasks have all completed, from pool \n",
      "24/02/26 21:42:57 INFO DAGScheduler: ShuffleMapStage 65 (reduceByKey at /tmp/ipykernel_31804/2132416953.py:6) finished in 17.916 s\n",
      "24/02/26 21:42:57 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/02/26 21:42:57 INFO DAGScheduler: running: Set()\n",
      "24/02/26 21:42:57 INFO DAGScheduler: waiting: Set(ResultStage 66)\n",
      "24/02/26 21:42:57 INFO DAGScheduler: failed: Set()\n",
      "24/02/26 21:42:57 INFO DAGScheduler: Submitting ResultStage 66 (PythonRDD[114] at takeOrdered at /tmp/ipykernel_31804/2132416953.py:9), which has no missing parents\n",
      "24/02/26 21:42:57 INFO MemoryStore: Block broadcast_62 stored as values in memory (estimated size 11.1 KiB, free 433.4 MiB)\n",
      "24/02/26 21:42:57 INFO MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 433.4 MiB)\n",
      "24/02/26 21:42:57 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on host-192-168-2-136-de1:10005 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:42:57 INFO SparkContext: Created broadcast 62 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/26 21:42:57 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 66 (PythonRDD[114] at takeOrdered at /tmp/ipykernel_31804/2132416953.py:9) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/02/26 21:42:57 INFO TaskSchedulerImpl: Adding task set 66.0 with 2 tasks resource profile 0\n",
      "24/02/26 21:42:57 INFO TaskSetManager: Starting task 0.0 in stage 66.0 (TID 170) (192.168.2.250, executor 1, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/02/26 21:42:57 INFO TaskSetManager: Starting task 1.0 in stage 66.0 (TID 171) (192.168.2.250, executor 1, partition 1, NODE_LOCAL, 7437 bytes) \n",
      "24/02/26 21:42:57 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on 192.168.2.250:10005 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:42:57 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 11 to 192.168.2.250:57716\n",
      "24/02/26 21:42:57 INFO BlockManagerInfo: Removed broadcast_61_piece0 on host-192-168-2-136-de1:10005 in memory (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:42:57 INFO BlockManagerInfo: Removed broadcast_61_piece0 on 192.168.2.250:10005 in memory (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:42:58 INFO TaskSetManager: Finished task 1.0 in stage 66.0 (TID 171) in 160 ms on 192.168.2.250 (executor 1) (1/2)\n",
      "24/02/26 21:42:58 INFO TaskSetManager: Finished task 0.0 in stage 66.0 (TID 170) in 179 ms on 192.168.2.250 (executor 1) (2/2)\n",
      "24/02/26 21:42:58 INFO TaskSchedulerImpl: Removed TaskSet 66.0, whose tasks have all completed, from pool \n",
      "24/02/26 21:42:58 INFO DAGScheduler: ResultStage 66 (takeOrdered at /tmp/ipykernel_31804/2132416953.py:9) finished in 0.191 s\n",
      "24/02/26 21:42:58 INFO DAGScheduler: Job 48 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/26 21:42:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 66: Stage finished\n",
      "24/02/26 21:42:58 INFO DAGScheduler: Job 48 finished: takeOrdered at /tmp/ipykernel_31804/2132416953.py:9, took 18.112606 s\n",
      "24/02/26 21:42:58 INFO SparkContext: Starting job: takeOrdered at /tmp/ipykernel_31804/2132416953.py:10\n",
      "24/02/26 21:42:58 INFO DAGScheduler: Registering RDD 111 (reduceByKey at /tmp/ipykernel_31804/2132416953.py:7) as input to shuffle 12\n",
      "24/02/26 21:42:58 INFO DAGScheduler: Got job 49 (takeOrdered at /tmp/ipykernel_31804/2132416953.py:10) with 3 output partitions\n",
      "24/02/26 21:42:58 INFO DAGScheduler: Final stage: ResultStage 68 (takeOrdered at /tmp/ipykernel_31804/2132416953.py:10)\n",
      "24/02/26 21:42:58 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 67)\n",
      "24/02/26 21:42:58 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 67)\n",
      "24/02/26 21:42:58 INFO DAGScheduler: Submitting ShuffleMapStage 67 (PairwiseRDD[111] at reduceByKey at /tmp/ipykernel_31804/2132416953.py:7), which has no missing parents\n",
      "24/02/26 21:42:58 INFO MemoryStore: Block broadcast_63 stored as values in memory (estimated size 12.9 KiB, free 433.4 MiB)\n",
      "24/02/26 21:42:58 INFO MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 433.4 MiB)\n",
      "24/02/26 21:42:58 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on host-192-168-2-136-de1:10005 (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:42:58 INFO SparkContext: Created broadcast 63 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/26 21:42:58 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 67 (PairwiseRDD[111] at reduceByKey at /tmp/ipykernel_31804/2132416953.py:7) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/02/26 21:42:58 INFO TaskSchedulerImpl: Adding task set 67.0 with 3 tasks resource profile 0\n",
      "24/02/26 21:42:58 INFO BlockManagerInfo: Removed broadcast_62_piece0 on host-192-168-2-136-de1:10005 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:42:58 INFO TaskSetManager: Starting task 0.0 in stage 67.0 (TID 172) (192.168.2.250, executor 1, partition 0, ANY, 7679 bytes) \n",
      "24/02/26 21:42:58 INFO TaskSetManager: Starting task 1.0 in stage 67.0 (TID 173) (192.168.2.252, executor 0, partition 1, ANY, 7679 bytes) \n",
      "24/02/26 21:42:58 INFO TaskSetManager: Starting task 2.0 in stage 67.0 (TID 174) (192.168.2.250, executor 1, partition 2, ANY, 7679 bytes) \n",
      "24/02/26 21:42:58 INFO BlockManagerInfo: Removed broadcast_62_piece0 on 192.168.2.250:10005 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:42:58 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on 192.168.2.250:10005 (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:42:58 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on 192.168.2.252:10005 (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:43:00 INFO TaskSetManager: Finished task 2.0 in stage 67.0 (TID 174) in 2068 ms on 192.168.2.250 (executor 1) (1/3)\n",
      "24/02/26 21:43:12 INFO TaskSetManager: Finished task 0.0 in stage 67.0 (TID 172) in 13901 ms on 192.168.2.250 (executor 1) (2/3)\n",
      "24/02/26 21:43:12 INFO TaskSetManager: Finished task 1.0 in stage 67.0 (TID 173) in 13958 ms on 192.168.2.252 (executor 0) (3/3)\n",
      "24/02/26 21:43:12 INFO TaskSchedulerImpl: Removed TaskSet 67.0, whose tasks have all completed, from pool \n",
      "24/02/26 21:43:12 INFO DAGScheduler: ShuffleMapStage 67 (reduceByKey at /tmp/ipykernel_31804/2132416953.py:7) finished in 13.967 s\n",
      "24/02/26 21:43:12 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/02/26 21:43:12 INFO DAGScheduler: running: Set()\n",
      "24/02/26 21:43:12 INFO DAGScheduler: waiting: Set(ResultStage 68)\n",
      "24/02/26 21:43:12 INFO DAGScheduler: failed: Set()\n",
      "24/02/26 21:43:12 INFO DAGScheduler: Submitting ResultStage 68 (PythonRDD[115] at takeOrdered at /tmp/ipykernel_31804/2132416953.py:10), which has no missing parents\n",
      "24/02/26 21:43:12 INFO MemoryStore: Block broadcast_64 stored as values in memory (estimated size 11.1 KiB, free 433.4 MiB)\n",
      "24/02/26 21:43:12 INFO MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 433.4 MiB)\n",
      "24/02/26 21:43:12 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on host-192-168-2-136-de1:10005 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:43:12 INFO SparkContext: Created broadcast 64 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/26 21:43:12 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 68 (PythonRDD[115] at takeOrdered at /tmp/ipykernel_31804/2132416953.py:10) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/02/26 21:43:12 INFO TaskSchedulerImpl: Adding task set 68.0 with 3 tasks resource profile 0\n",
      "24/02/26 21:43:12 INFO TaskSetManager: Starting task 0.0 in stage 68.0 (TID 175) (192.168.2.250, executor 1, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/02/26 21:43:12 INFO TaskSetManager: Starting task 1.0 in stage 68.0 (TID 176) (192.168.2.252, executor 0, partition 1, NODE_LOCAL, 7437 bytes) \n",
      "24/02/26 21:43:12 INFO TaskSetManager: Starting task 2.0 in stage 68.0 (TID 177) (192.168.2.250, executor 1, partition 2, NODE_LOCAL, 7437 bytes) \n",
      "24/02/26 21:43:12 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on 192.168.2.250:10005 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:43:12 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on 192.168.2.252:10005 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:43:12 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 12 to 192.168.2.250:57716\n",
      "24/02/26 21:43:12 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 12 to 192.168.2.252:48736\n",
      "24/02/26 21:43:12 INFO BlockManagerInfo: Removed broadcast_63_piece0 on host-192-168-2-136-de1:10005 in memory (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:43:12 INFO BlockManagerInfo: Removed broadcast_63_piece0 on 192.168.2.252:10005 in memory (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:43:12 INFO BlockManagerInfo: Removed broadcast_63_piece0 on 192.168.2.250:10005 in memory (size: 7.8 KiB, free: 434.3 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 words in English:\n",
      "the: 3498375\n",
      "of: 1659758\n",
      "to: 1539760\n",
      "and: 1288401\n",
      "in: 1085993\n",
      "that: 797516\n",
      "a: 773522\n",
      "is: 758050\n",
      "for: 534242\n",
      "we: 522849\n",
      "\n",
      "Top 10 words in Swedish:\n",
      "att: 1706293\n",
      "och: 1344830\n",
      "i: 1050774\n",
      "det: 924866\n",
      "som: 913276\n",
      "för: 908680\n",
      "av: 738068\n",
      "är: 694381\n",
      "en: 620310\n",
      "vi: 539797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/26 21:43:12 INFO TaskSetManager: Finished task 1.0 in stage 68.0 (TID 176) in 236 ms on 192.168.2.252 (executor 0) (1/3)\n",
      "24/02/26 21:43:12 INFO TaskSetManager: Finished task 2.0 in stage 68.0 (TID 177) in 260 ms on 192.168.2.250 (executor 1) (2/3)\n",
      "24/02/26 21:43:12 INFO TaskSetManager: Finished task 0.0 in stage 68.0 (TID 175) in 299 ms on 192.168.2.250 (executor 1) (3/3)\n",
      "24/02/26 21:43:12 INFO TaskSchedulerImpl: Removed TaskSet 68.0, whose tasks have all completed, from pool \n",
      "24/02/26 21:43:12 INFO DAGScheduler: ResultStage 68 (takeOrdered at /tmp/ipykernel_31804/2132416953.py:10) finished in 0.315 s\n",
      "24/02/26 21:43:12 INFO DAGScheduler: Job 49 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/26 21:43:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 68: Stage finished\n",
      "24/02/26 21:43:12 INFO DAGScheduler: Job 49 finished: takeOrdered at /tmp/ipykernel_31804/2132416953.py:10, took 14.286866 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check 10 most frequent words in English and Swedish\n",
    "\n",
    "words_en = processed_lines_en.flatMap(lambda line: line)\n",
    "words_sv = processed_lines_sv.flatMap(lambda line: line)\n",
    "\n",
    "word_counts_en = words_en.map(lambda word: (word, 1)).reduceByKey(add)\n",
    "word_counts_sv = words_sv.map(lambda word: (word, 1)).reduceByKey(add)\n",
    "\n",
    "top_10_words_en = word_counts_en.takeOrdered(10, key=lambda x: -x[1])\n",
    "top_10_words_sv = word_counts_sv.takeOrdered(10, key=lambda x: -x[1])\n",
    "\n",
    "print(\"\\nTop 10 words in English:\")\n",
    "for word, count in top_10_words_en:\n",
    "    print(f'{word}: {count}')\n",
    "    \n",
    "print(\"\\nTop 10 words in Swedish:\")\n",
    "for word, count in top_10_words_sv:\n",
    "    print(f'{word}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9b84eae1-31fb-4c21-9258-1c814012defb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 words in English corpus: [('the', 3498375), ('of', 1659758), ('to', 1539760), ('and', 1288401), ('in', 1085993), \n",
    "#                                  ('that', 797516), ('a', 773522), ('is', 758050), ('for', 534242), ('we', 522849)]\n",
    "# Top 10 words in Swedish corpus: [('att', 1706293), ('och', 1344830), ('i', 1050774), ('det', 924866), ('som', 913276), \n",
    "#                                  ('för', 908680), ('av', 738068), ('är', 694381), ('en', 620310), ('vi', 539797)]\n",
    "\n",
    "# OBS!!! KOLLA DETTA SÅ DET STÄMMER, DETTA ÄR OM DET ÄR SPLIT(' ') MEN SAGAS RESULTAT ÄR OM DET ÄR SPLIT() i preprocessing delen\n",
    "# Top 10 words in English:\n",
    "# the: 3498375\n",
    "# of: 1659758\n",
    "# to: 1539760\n",
    "# and: 1288401\n",
    "# in: 1085993\n",
    "# that: 797516\n",
    "# a: 773522\n",
    "# is: 758050\n",
    "# for: 534242\n",
    "# we: 522849\n",
    "\n",
    "# Top 10 words in Swedish:\n",
    "# att: 1706293\n",
    "# och: 1344830\n",
    "# i: 1050774\n",
    "# det: 924866\n",
    "# som: 913276\n",
    "# för: 908680\n",
    "# av: 738068\n",
    "# är: 694381\n",
    "# en: 620310\n",
    "# vi: 539797\n",
    "\n",
    "# The results are reasonable, as the most frequent words are common words used to build up sentences (konjunctions, prepositions etc)\n",
    "# In the Swedish translation for example, the words also match most of the top 10 most frequent words in the Swedish language like i, och, att, det, som, en, är, av, för "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "435ab700-0ceb-4bc7-accf-e58f56667379",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/26 21:43:54 INFO SparkContext: Starting job: zipWithIndex at /tmp/ipykernel_31804/4241438500.py:2\n",
      "24/02/26 21:43:54 INFO DAGScheduler: Got job 50 (zipWithIndex at /tmp/ipykernel_31804/4241438500.py:2) with 3 output partitions\n",
      "24/02/26 21:43:54 INFO DAGScheduler: Final stage: ResultStage 69 (zipWithIndex at /tmp/ipykernel_31804/4241438500.py:2)\n",
      "24/02/26 21:43:54 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/02/26 21:43:54 INFO DAGScheduler: Missing parents: List()\n",
      "24/02/26 21:43:54 INFO DAGScheduler: Submitting ResultStage 69 (PythonRDD[116] at zipWithIndex at /tmp/ipykernel_31804/4241438500.py:2), which has no missing parents\n",
      "24/02/26 21:43:54 INFO MemoryStore: Block broadcast_65 stored as values in memory (estimated size 8.3 KiB, free 433.4 MiB)\n",
      "24/02/26 21:43:54 INFO MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 433.4 MiB)\n",
      "24/02/26 21:43:54 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on host-192-168-2-136-de1:10005 (size: 5.1 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:43:54 INFO SparkContext: Created broadcast 65 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/26 21:43:54 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 69 (PythonRDD[116] at zipWithIndex at /tmp/ipykernel_31804/4241438500.py:2) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/02/26 21:43:54 INFO TaskSchedulerImpl: Adding task set 69.0 with 3 tasks resource profile 0\n",
      "24/02/26 21:43:54 INFO TaskSetManager: Starting task 0.0 in stage 69.0 (TID 178) (192.168.2.252, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/02/26 21:43:54 INFO BlockManagerInfo: Removed broadcast_64_piece0 on 192.168.2.252:10005 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:43:54 INFO TaskSetManager: Starting task 1.0 in stage 69.0 (TID 179) (192.168.2.250, executor 1, partition 1, ANY, 7690 bytes) \n",
      "24/02/26 21:43:54 INFO TaskSetManager: Starting task 2.0 in stage 69.0 (TID 180) (192.168.2.252, executor 0, partition 2, ANY, 7690 bytes) \n",
      "24/02/26 21:43:54 INFO BlockManagerInfo: Removed broadcast_64_piece0 on 192.168.2.250:10005 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:43:54 INFO BlockManagerInfo: Removed broadcast_64_piece0 on host-192-168-2-136-de1:10005 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:43:54 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on 192.168.2.252:10005 (size: 5.1 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:43:54 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on 192.168.2.250:10005 (size: 5.1 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:43:55 INFO TaskSetManager: Finished task 2.0 in stage 69.0 (TID 180) in 424 ms on 192.168.2.252 (executor 0) (1/3)\n",
      "24/02/26 21:43:57 INFO TaskSetManager: Finished task 1.0 in stage 69.0 (TID 179) in 2850 ms on 192.168.2.250 (executor 1) (2/3)\n",
      "24/02/26 21:43:57 INFO TaskSetManager: Finished task 0.0 in stage 69.0 (TID 178) in 3107 ms on 192.168.2.252 (executor 0) (3/3)\n",
      "24/02/26 21:43:57 INFO TaskSchedulerImpl: Removed TaskSet 69.0, whose tasks have all completed, from pool \n",
      "24/02/26 21:43:57 INFO DAGScheduler: ResultStage 69 (zipWithIndex at /tmp/ipykernel_31804/4241438500.py:2) finished in 3.116 s\n",
      "24/02/26 21:43:57 INFO DAGScheduler: Job 50 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/26 21:43:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 69: Stage finished\n",
      "24/02/26 21:43:57 INFO DAGScheduler: Job 50 finished: zipWithIndex at /tmp/ipykernel_31804/4241438500.py:2, took 3.118618 s\n",
      "24/02/26 21:43:57 INFO SparkContext: Starting job: zipWithIndex at /tmp/ipykernel_31804/4241438500.py:3\n",
      "24/02/26 21:43:57 INFO DAGScheduler: Got job 51 (zipWithIndex at /tmp/ipykernel_31804/4241438500.py:3) with 2 output partitions\n",
      "24/02/26 21:43:57 INFO DAGScheduler: Final stage: ResultStage 70 (zipWithIndex at /tmp/ipykernel_31804/4241438500.py:3)\n",
      "24/02/26 21:43:57 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/02/26 21:43:57 INFO DAGScheduler: Missing parents: List()\n",
      "24/02/26 21:43:57 INFO DAGScheduler: Submitting ResultStage 70 (PythonRDD[117] at zipWithIndex at /tmp/ipykernel_31804/4241438500.py:3), which has no missing parents\n",
      "24/02/26 21:43:57 INFO MemoryStore: Block broadcast_66 stored as values in memory (estimated size 8.3 KiB, free 433.4 MiB)\n",
      "24/02/26 21:43:57 INFO MemoryStore: Block broadcast_66_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 433.4 MiB)\n",
      "24/02/26 21:43:57 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on host-192-168-2-136-de1:10005 (size: 5.1 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:43:57 INFO BlockManagerInfo: Removed broadcast_65_piece0 on 192.168.2.252:10005 in memory (size: 5.1 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:43:57 INFO SparkContext: Created broadcast 66 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/26 21:43:57 INFO BlockManagerInfo: Removed broadcast_65_piece0 on 192.168.2.250:10005 in memory (size: 5.1 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:43:57 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 70 (PythonRDD[117] at zipWithIndex at /tmp/ipykernel_31804/4241438500.py:3) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/02/26 21:43:57 INFO TaskSchedulerImpl: Adding task set 70.0 with 2 tasks resource profile 0\n",
      "24/02/26 21:43:57 INFO TaskSetManager: Starting task 1.0 in stage 70.0 (TID 181) (192.168.2.250, executor 1, partition 1, NODE_LOCAL, 7690 bytes) \n",
      "24/02/26 21:43:57 INFO BlockManagerInfo: Removed broadcast_65_piece0 on host-192-168-2-136-de1:10005 in memory (size: 5.1 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:43:57 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on 192.168.2.250:10005 (size: 5.1 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:43:59 INFO TaskSetManager: Finished task 1.0 in stage 70.0 (TID 181) in 1952 ms on 192.168.2.250 (executor 1) (1/2)\n",
      "24/02/26 21:44:01 INFO TaskSetManager: Starting task 0.0 in stage 70.0 (TID 182) (192.168.2.252, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/02/26 21:44:01 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on 192.168.2.252:10005 (size: 5.1 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:44:03 INFO TaskSetManager: Finished task 0.0 in stage 70.0 (TID 182) in 2123 ms on 192.168.2.252 (executor 0) (2/2)\n",
      "24/02/26 21:44:03 INFO TaskSchedulerImpl: Removed TaskSet 70.0, whose tasks have all completed, from pool \n",
      "24/02/26 21:44:03 INFO DAGScheduler: ResultStage 70 (zipWithIndex at /tmp/ipykernel_31804/4241438500.py:3) finished in 6.088 s\n",
      "24/02/26 21:44:03 INFO DAGScheduler: Job 51 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/26 21:44:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 70: Stage finished\n",
      "24/02/26 21:44:03 INFO DAGScheduler: Job 51 finished: zipWithIndex at /tmp/ipykernel_31804/4241438500.py:3, took 6.089614 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Key the lines by their line number and swap\n",
    "sv_swapped = processed_lines_sv.zipWithIndex().map(lambda x: (x[1], x[0]))\n",
    "en_swapped = processed_lines_en.zipWithIndex().map(lambda x: (x[1], x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "31525e83-91a2-46d5-811b-c42d3de19921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Key the lines by their line number\n",
    "#sv_keyed = processed_lines_sv.zipWithIndex().map(lambda x: (x[1], x[0]))\n",
    "#en_keyed = processed_lines_en.zipWithIndex().map(lambda x: (x[1], x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3bcfa5f3-a091-4097-a1d9-c3ccae4b22ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swap the key and value\n",
    "#sv_swapped = sv_with_index.map(lambda x: (x[1], x[0]))\n",
    "#en_swapped = en_with_index.map(lambda x: (x[1], x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b4a3fa84-d2e9-4e52-8013-08ddf55670ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/26 21:44:31 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/02/26 21:44:31 INFO DAGScheduler: Got job 52 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/02/26 21:44:31 INFO DAGScheduler: Final stage: ResultStage 71 (runJob at PythonRDD.scala:181)\n",
      "24/02/26 21:44:31 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/02/26 21:44:31 INFO DAGScheduler: Missing parents: List()\n",
      "24/02/26 21:44:31 INFO DAGScheduler: Submitting ResultStage 71 (PythonRDD[118] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/02/26 21:44:31 INFO MemoryStore: Block broadcast_67 stored as values in memory (estimated size 9.2 KiB, free 433.4 MiB)\n",
      "24/02/26 21:44:31 INFO BlockManagerInfo: Removed broadcast_66_piece0 on host-192-168-2-136-de1:10005 in memory (size: 5.1 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:44:31 INFO BlockManagerInfo: Removed broadcast_66_piece0 on 192.168.2.252:10005 in memory (size: 5.1 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:44:31 INFO BlockManagerInfo: Removed broadcast_66_piece0 on 192.168.2.250:10005 in memory (size: 5.1 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:44:31 INFO MemoryStore: Block broadcast_67_piece0 stored as bytes in memory (estimated size 5.6 KiB, free 433.4 MiB)\n",
      "24/02/26 21:44:31 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on host-192-168-2-136-de1:10005 (size: 5.6 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:44:31 INFO SparkContext: Created broadcast 67 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/26 21:44:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 71 (PythonRDD[118] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/02/26 21:44:31 INFO TaskSchedulerImpl: Adding task set 71.0 with 1 tasks resource profile 0\n",
      "24/02/26 21:44:31 INFO TaskSetManager: Starting task 0.0 in stage 71.0 (TID 183) (192.168.2.250, executor 1, partition 0, ANY, 7690 bytes) \n",
      "24/02/26 21:44:31 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on 192.168.2.250:10005 (size: 5.6 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:44:32 INFO TaskSetManager: Finished task 0.0 in stage 71.0 (TID 183) in 1207 ms on 192.168.2.250 (executor 1) (1/1)\n",
      "24/02/26 21:44:32 INFO TaskSchedulerImpl: Removed TaskSet 71.0, whose tasks have all completed, from pool \n",
      "24/02/26 21:44:32 INFO DAGScheduler: ResultStage 71 (runJob at PythonRDD.scala:181) finished in 1.223 s\n",
      "24/02/26 21:44:32 INFO DAGScheduler: Job 52 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/26 21:44:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 71: Stage finished\n",
      "24/02/26 21:44:32 INFO DAGScheduler: Job 52 finished: runJob at PythonRDD.scala:181, took 1.226026 s\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, ['återupptagande', 'av', 'sessionen']),\n",
       " (1,\n",
       "  ['jag',\n",
       "   'förklarar',\n",
       "   'europaparlamentets',\n",
       "   'session',\n",
       "   'återupptagen',\n",
       "   'efter',\n",
       "   'avbrottet',\n",
       "   'den',\n",
       "   '17',\n",
       "   'december.',\n",
       "   'jag',\n",
       "   'vill',\n",
       "   'på',\n",
       "   'nytt',\n",
       "   'önska',\n",
       "   'er',\n",
       "   'ett',\n",
       "   'gott',\n",
       "   'nytt',\n",
       "   'år',\n",
       "   'och',\n",
       "   'jag',\n",
       "   'hoppas',\n",
       "   'att',\n",
       "   'ni',\n",
       "   'haft',\n",
       "   'en',\n",
       "   'trevlig',\n",
       "   'semester.']),\n",
       " (2,\n",
       "  ['som',\n",
       "   'ni',\n",
       "   'kunnat',\n",
       "   'konstatera',\n",
       "   'ägde',\n",
       "   '\"den',\n",
       "   'stora',\n",
       "   'år',\n",
       "   '2000-buggen\"',\n",
       "   'aldrig',\n",
       "   'rum.',\n",
       "   'däremot',\n",
       "   'har',\n",
       "   'invånarna',\n",
       "   'i',\n",
       "   'ett',\n",
       "   'antal',\n",
       "   'av',\n",
       "   'våra',\n",
       "   'medlemsländer',\n",
       "   'drabbats',\n",
       "   'av',\n",
       "   'naturkatastrofer',\n",
       "   'som',\n",
       "   'verkligen',\n",
       "   'varit',\n",
       "   'förskräckliga.']),\n",
       " (3,\n",
       "  ['ni',\n",
       "   'har',\n",
       "   'begärt',\n",
       "   'en',\n",
       "   'debatt',\n",
       "   'i',\n",
       "   'ämnet',\n",
       "   'under',\n",
       "   'sammanträdesperiodens',\n",
       "   'kommande',\n",
       "   'dagar.']),\n",
       " (4,\n",
       "  ['till',\n",
       "   'dess',\n",
       "   'vill',\n",
       "   'jag',\n",
       "   'att',\n",
       "   'vi,',\n",
       "   'som',\n",
       "   'ett',\n",
       "   'antal',\n",
       "   'kolleger',\n",
       "   'begärt,',\n",
       "   'håller',\n",
       "   'en',\n",
       "   'tyst',\n",
       "   'minut',\n",
       "   'för',\n",
       "   'offren',\n",
       "   'för',\n",
       "   'bl.a.',\n",
       "   'stormarna',\n",
       "   'i',\n",
       "   'de',\n",
       "   'länder',\n",
       "   'i',\n",
       "   'europeiska',\n",
       "   'unionen',\n",
       "   'som',\n",
       "   'drabbats.']),\n",
       " (5, ['jag', 'ber', 'er', 'resa', 'er', 'för', 'en', 'tyst', 'minut.']),\n",
       " (6, ['(parlamentet', 'höll', 'en', 'tyst', 'minut.)']),\n",
       " (7, ['fru', 'talman!', 'det', 'gäller', 'en', 'ordningsfråga.']),\n",
       " (8,\n",
       "  ['ni',\n",
       "   'känner',\n",
       "   'till',\n",
       "   'från',\n",
       "   'media',\n",
       "   'att',\n",
       "   'det',\n",
       "   'skett',\n",
       "   'en',\n",
       "   'rad',\n",
       "   'bombexplosioner',\n",
       "   'och',\n",
       "   'mord',\n",
       "   'i',\n",
       "   'sri',\n",
       "   'lanka.']),\n",
       " (9,\n",
       "  ['en',\n",
       "   'av',\n",
       "   'de',\n",
       "   'personer',\n",
       "   'som',\n",
       "   'mycket',\n",
       "   'nyligen',\n",
       "   'mördades',\n",
       "   'i',\n",
       "   'sri',\n",
       "   'lanka',\n",
       "   'var',\n",
       "   'kumar',\n",
       "   'ponnambalam,',\n",
       "   'som',\n",
       "   'besökte',\n",
       "   'europaparlamentet',\n",
       "   'för',\n",
       "   'bara',\n",
       "   'några',\n",
       "   'månader',\n",
       "   'sedan.'])]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv_swapped.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2df64d79-ba28-46ec-9d10-8234fb07b93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/26 21:44:59 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/02/26 21:44:59 INFO DAGScheduler: Got job 53 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/02/26 21:44:59 INFO DAGScheduler: Final stage: ResultStage 72 (runJob at PythonRDD.scala:181)\n",
      "24/02/26 21:44:59 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/02/26 21:44:59 INFO DAGScheduler: Missing parents: List()\n",
      "24/02/26 21:44:59 INFO DAGScheduler: Submitting ResultStage 72 (PythonRDD[119] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/02/26 21:44:59 INFO MemoryStore: Block broadcast_68 stored as values in memory (estimated size 9.2 KiB, free 433.4 MiB)\n",
      "24/02/26 21:44:59 INFO MemoryStore: Block broadcast_68_piece0 stored as bytes in memory (estimated size 5.6 KiB, free 433.4 MiB)\n",
      "24/02/26 21:44:59 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on host-192-168-2-136-de1:10005 (size: 5.6 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:44:59 INFO SparkContext: Created broadcast 68 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/26 21:44:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 72 (PythonRDD[119] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/02/26 21:44:59 INFO TaskSchedulerImpl: Adding task set 72.0 with 1 tasks resource profile 0\n",
      "24/02/26 21:44:59 INFO TaskSetManager: Starting task 0.0 in stage 72.0 (TID 184) (192.168.2.250, executor 1, partition 0, ANY, 7690 bytes) \n",
      "24/02/26 21:44:59 INFO BlockManagerInfo: Removed broadcast_67_piece0 on 192.168.2.250:10005 in memory (size: 5.6 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:44:59 INFO BlockManagerInfo: Removed broadcast_67_piece0 on host-192-168-2-136-de1:10005 in memory (size: 5.6 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:44:59 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on 192.168.2.250:10005 (size: 5.6 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:44:59 INFO TaskSetManager: Finished task 0.0 in stage 72.0 (TID 184) in 673 ms on 192.168.2.250 (executor 1) (1/1)\n",
      "24/02/26 21:44:59 INFO TaskSchedulerImpl: Removed TaskSet 72.0, whose tasks have all completed, from pool \n",
      "24/02/26 21:44:59 INFO DAGScheduler: ResultStage 72 (runJob at PythonRDD.scala:181) finished in 0.687 s\n",
      "24/02/26 21:44:59 INFO DAGScheduler: Job 53 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/26 21:44:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 72: Stage finished\n",
      "24/02/26 21:44:59 INFO DAGScheduler: Job 53 finished: runJob at PythonRDD.scala:181, took 0.689886 s\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, ['resumption', 'of', 'the', 'session']),\n",
       " (1,\n",
       "  ['i',\n",
       "   'declare',\n",
       "   'resumed',\n",
       "   'the',\n",
       "   'session',\n",
       "   'of',\n",
       "   'the',\n",
       "   'european',\n",
       "   'parliament',\n",
       "   'adjourned',\n",
       "   'on',\n",
       "   'friday',\n",
       "   '17',\n",
       "   'december',\n",
       "   '1999,',\n",
       "   'and',\n",
       "   'i',\n",
       "   'would',\n",
       "   'like',\n",
       "   'once',\n",
       "   'again',\n",
       "   'to',\n",
       "   'wish',\n",
       "   'you',\n",
       "   'a',\n",
       "   'happy',\n",
       "   'new',\n",
       "   'year',\n",
       "   'in',\n",
       "   'the',\n",
       "   'hope',\n",
       "   'that',\n",
       "   'you',\n",
       "   'enjoyed',\n",
       "   'a',\n",
       "   'pleasant',\n",
       "   'festive',\n",
       "   'period.']),\n",
       " (2,\n",
       "  ['although,',\n",
       "   'as',\n",
       "   'you',\n",
       "   'will',\n",
       "   'have',\n",
       "   'seen,',\n",
       "   'the',\n",
       "   'dreaded',\n",
       "   \"'millennium\",\n",
       "   \"bug'\",\n",
       "   'failed',\n",
       "   'to',\n",
       "   'materialise,',\n",
       "   'still',\n",
       "   'the',\n",
       "   'people',\n",
       "   'in',\n",
       "   'a',\n",
       "   'number',\n",
       "   'of',\n",
       "   'countries',\n",
       "   'suffered',\n",
       "   'a',\n",
       "   'series',\n",
       "   'of',\n",
       "   'natural',\n",
       "   'disasters',\n",
       "   'that',\n",
       "   'truly',\n",
       "   'were',\n",
       "   'dreadful.']),\n",
       " (3,\n",
       "  ['you',\n",
       "   'have',\n",
       "   'requested',\n",
       "   'a',\n",
       "   'debate',\n",
       "   'on',\n",
       "   'this',\n",
       "   'subject',\n",
       "   'in',\n",
       "   'the',\n",
       "   'course',\n",
       "   'of',\n",
       "   'the',\n",
       "   'next',\n",
       "   'few',\n",
       "   'days,',\n",
       "   'during',\n",
       "   'this',\n",
       "   'part-session.']),\n",
       " (4,\n",
       "  ['in',\n",
       "   'the',\n",
       "   'meantime,',\n",
       "   'i',\n",
       "   'should',\n",
       "   'like',\n",
       "   'to',\n",
       "   'observe',\n",
       "   'a',\n",
       "   \"minute'\",\n",
       "   's',\n",
       "   'silence,',\n",
       "   'as',\n",
       "   'a',\n",
       "   'number',\n",
       "   'of',\n",
       "   'members',\n",
       "   'have',\n",
       "   'requested,',\n",
       "   'on',\n",
       "   'behalf',\n",
       "   'of',\n",
       "   'all',\n",
       "   'the',\n",
       "   'victims',\n",
       "   'concerned,',\n",
       "   'particularly',\n",
       "   'those',\n",
       "   'of',\n",
       "   'the',\n",
       "   'terrible',\n",
       "   'storms,',\n",
       "   'in',\n",
       "   'the',\n",
       "   'various',\n",
       "   'countries',\n",
       "   'of',\n",
       "   'the',\n",
       "   'european',\n",
       "   'union.']),\n",
       " (5, ['please', 'rise,', 'then,', 'for', 'this', \"minute'\", 's', 'silence.']),\n",
       " (6,\n",
       "  ['(the',\n",
       "   'house',\n",
       "   'rose',\n",
       "   'and',\n",
       "   'observed',\n",
       "   'a',\n",
       "   \"minute'\",\n",
       "   's',\n",
       "   'silence)']),\n",
       " (7, ['madam', 'president,', 'on', 'a', 'point', 'of', 'order.']),\n",
       " (8,\n",
       "  ['you',\n",
       "   'will',\n",
       "   'be',\n",
       "   'aware',\n",
       "   'from',\n",
       "   'the',\n",
       "   'press',\n",
       "   'and',\n",
       "   'television',\n",
       "   'that',\n",
       "   'there',\n",
       "   'have',\n",
       "   'been',\n",
       "   'a',\n",
       "   'number',\n",
       "   'of',\n",
       "   'bomb',\n",
       "   'explosions',\n",
       "   'and',\n",
       "   'killings',\n",
       "   'in',\n",
       "   'sri',\n",
       "   'lanka.']),\n",
       " (9,\n",
       "  ['one',\n",
       "   'of',\n",
       "   'the',\n",
       "   'people',\n",
       "   'assassinated',\n",
       "   'very',\n",
       "   'recently',\n",
       "   'in',\n",
       "   'sri',\n",
       "   'lanka',\n",
       "   'was',\n",
       "   'mr',\n",
       "   'kumar',\n",
       "   'ponnambalam,',\n",
       "   'who',\n",
       "   'had',\n",
       "   'visited',\n",
       "   'the',\n",
       "   'european',\n",
       "   'parliament',\n",
       "   'just',\n",
       "   'a',\n",
       "   'few',\n",
       "   'months',\n",
       "   'ago.'])]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_swapped.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "eba067bb-52db-4d8a-995f-008db52af32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the two RDDs together\n",
    "sv_en_joined_rdd = sv_swapped.join(en_swapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "15c67527-577e-436b-9c6d-c1d83cd0469f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/26 21:45:06 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/02/26 21:45:06 INFO DAGScheduler: Registering RDD 124 (join at /tmp/ipykernel_31804/242331830.py:2) as input to shuffle 13\n",
      "24/02/26 21:45:06 INFO DAGScheduler: Got job 54 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/02/26 21:45:06 INFO DAGScheduler: Final stage: ResultStage 74 (runJob at PythonRDD.scala:181)\n",
      "24/02/26 21:45:06 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 73)\n",
      "24/02/26 21:45:06 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 73)\n",
      "24/02/26 21:45:06 INFO DAGScheduler: Submitting ShuffleMapStage 73 (PairwiseRDD[124] at join at /tmp/ipykernel_31804/242331830.py:2), which has no missing parents\n",
      "24/02/26 21:45:06 INFO MemoryStore: Block broadcast_69 stored as values in memory (estimated size 18.5 KiB, free 433.4 MiB)\n",
      "24/02/26 21:45:06 INFO MemoryStore: Block broadcast_69_piece0 stored as bytes in memory (estimated size 8.7 KiB, free 433.4 MiB)\n",
      "24/02/26 21:45:06 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on host-192-168-2-136-de1:10005 (size: 8.7 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:45:06 INFO SparkContext: Created broadcast 69 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/26 21:45:06 INFO DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 73 (PairwiseRDD[124] at join at /tmp/ipykernel_31804/242331830.py:2) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\n",
      "24/02/26 21:45:06 INFO TaskSchedulerImpl: Adding task set 73.0 with 5 tasks resource profile 0\n",
      "24/02/26 21:45:06 INFO TaskSetManager: Starting task 4.0 in stage 73.0 (TID 185) (192.168.2.250, executor 1, partition 4, NODE_LOCAL, 7788 bytes) \n",
      "24/02/26 21:45:06 INFO BlockManagerInfo: Removed broadcast_68_piece0 on 192.168.2.250:10005 in memory (size: 5.6 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:45:06 INFO BlockManagerInfo: Removed broadcast_68_piece0 on host-192-168-2-136-de1:10005 in memory (size: 5.6 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:45:06 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on 192.168.2.250:10005 (size: 8.7 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:45:10 INFO TaskSetManager: Starting task 0.0 in stage 73.0 (TID 186) (192.168.2.250, executor 1, partition 0, ANY, 7788 bytes) \n",
      "24/02/26 21:45:10 INFO TaskSetManager: Starting task 1.0 in stage 73.0 (TID 187) (192.168.2.252, executor 0, partition 1, ANY, 7788 bytes) \n",
      "24/02/26 21:45:10 INFO TaskSetManager: Starting task 2.0 in stage 73.0 (TID 188) (192.168.2.252, executor 0, partition 2, ANY, 7788 bytes) \n",
      "24/02/26 21:45:10 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on 192.168.2.252:10005 (size: 8.7 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:45:12 INFO TaskSetManager: Starting task 3.0 in stage 73.0 (TID 189) (192.168.2.252, executor 0, partition 3, ANY, 7788 bytes) \n",
      "24/02/26 21:45:12 INFO TaskSetManager: Finished task 2.0 in stage 73.0 (TID 188) in 1847 ms on 192.168.2.252 (executor 0) (1/5)\n",
      "24/02/26 21:45:43 INFO TaskSetManager: Finished task 4.0 in stage 73.0 (TID 185) in 36824 ms on 192.168.2.250 (executor 1) (2/5)\n",
      "24/02/26 21:45:44 INFO TaskSetManager: Finished task 1.0 in stage 73.0 (TID 187) in 33393 ms on 192.168.2.252 (executor 0) (3/5)\n",
      "24/02/26 21:45:44 INFO TaskSetManager: Finished task 0.0 in stage 73.0 (TID 186) in 34085 ms on 192.168.2.250 (executor 1) (4/5)\n",
      "24/02/26 21:45:46 INFO TaskSetManager: Finished task 3.0 in stage 73.0 (TID 189) in 33580 ms on 192.168.2.252 (executor 0) (5/5)\n",
      "24/02/26 21:45:46 INFO TaskSchedulerImpl: Removed TaskSet 73.0, whose tasks have all completed, from pool \n",
      "24/02/26 21:45:46 INFO DAGScheduler: ShuffleMapStage 73 (join at /tmp/ipykernel_31804/242331830.py:2) finished in 39.385 s\n",
      "24/02/26 21:45:46 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/02/26 21:45:46 INFO DAGScheduler: running: Set()\n",
      "24/02/26 21:45:46 INFO DAGScheduler: waiting: Set(ResultStage 74)\n",
      "24/02/26 21:45:46 INFO DAGScheduler: failed: Set()\n",
      "24/02/26 21:45:46 INFO DAGScheduler: Submitting ResultStage 74 (PythonRDD[127] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/02/26 21:45:46 INFO MemoryStore: Block broadcast_70 stored as values in memory (estimated size 12.6 KiB, free 433.4 MiB)\n",
      "24/02/26 21:45:46 INFO MemoryStore: Block broadcast_70_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 433.4 MiB)\n",
      "24/02/26 21:45:46 INFO BlockManagerInfo: Added broadcast_70_piece0 in memory on host-192-168-2-136-de1:10005 (size: 7.2 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:45:46 INFO SparkContext: Created broadcast 70 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/26 21:45:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 74 (PythonRDD[127] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/02/26 21:45:46 INFO TaskSchedulerImpl: Adding task set 74.0 with 1 tasks resource profile 0\n",
      "24/02/26 21:45:46 INFO TaskSetManager: Starting task 0.0 in stage 74.0 (TID 190) (192.168.2.252, executor 0, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/02/26 21:45:46 INFO BlockManagerInfo: Added broadcast_70_piece0 in memory on 192.168.2.252:10005 (size: 7.2 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:45:46 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 13 to 192.168.2.252:48736\n",
      "24/02/26 21:45:46 INFO BlockManagerInfo: Removed broadcast_69_piece0 on host-192-168-2-136-de1:10005 in memory (size: 8.7 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:45:46 INFO BlockManagerInfo: Removed broadcast_69_piece0 on 192.168.2.252:10005 in memory (size: 8.7 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:45:46 INFO BlockManagerInfo: Removed broadcast_69_piece0 on 192.168.2.250:10005 in memory (size: 8.7 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:46:04 INFO TaskSetManager: Finished task 0.0 in stage 74.0 (TID 190) in 17938 ms on 192.168.2.252 (executor 0) (1/1)\n",
      "24/02/26 21:46:04 INFO TaskSchedulerImpl: Removed TaskSet 74.0, whose tasks have all completed, from pool \n",
      "24/02/26 21:46:04 INFO DAGScheduler: ResultStage 74 (runJob at PythonRDD.scala:181) finished in 17.955 s\n",
      "24/02/26 21:46:04 INFO DAGScheduler: Job 54 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/26 21:46:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 74: Stage finished\n",
      "24/02/26 21:46:04 INFO DAGScheduler: Job 54 finished: runJob at PythonRDD.scala:181, took 57.350529 s\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(868820,\n",
       "  (['så',\n",
       "    'sent',\n",
       "    'som',\n",
       "    'januari',\n",
       "    '2006',\n",
       "    'började',\n",
       "    'eu-länderna',\n",
       "    'inse',\n",
       "    'att',\n",
       "    'energi',\n",
       "    'har',\n",
       "    'utnyttjats',\n",
       "    'och',\n",
       "    'kommer',\n",
       "    'att',\n",
       "    'utnyttjas',\n",
       "    'av',\n",
       "    'ryssland',\n",
       "    'som',\n",
       "    'ett',\n",
       "    'centralt',\n",
       "    'utrikespolitiskt',\n",
       "    'instrument.'],\n",
       "   ['as',\n",
       "    'late',\n",
       "    'as',\n",
       "    'january',\n",
       "    '2006,',\n",
       "    'the',\n",
       "    'eu',\n",
       "    'countries',\n",
       "    'started',\n",
       "    'to',\n",
       "    'realise',\n",
       "    'that',\n",
       "    'energy',\n",
       "    'has',\n",
       "    'been',\n",
       "    'used',\n",
       "    'and',\n",
       "    'will',\n",
       "    'be',\n",
       "    'used',\n",
       "    'by',\n",
       "    'russia',\n",
       "    'as',\n",
       "    'a',\n",
       "    'central',\n",
       "    'instrument',\n",
       "    'of',\n",
       "    'its',\n",
       "    'foreign',\n",
       "    'policies.'])),\n",
       " (885305,\n",
       "  (['på',\n",
       "    'senare',\n",
       "    'tid',\n",
       "    'har',\n",
       "    'vi',\n",
       "    'bevittnat',\n",
       "    'våldsangrepp',\n",
       "    'på',\n",
       "    'kristna',\n",
       "    'präster,',\n",
       "    'missionärer,',\n",
       "    'utgivare',\n",
       "    'och',\n",
       "    'konvertiter.'],\n",
       "   ['in',\n",
       "    'recent',\n",
       "    'times',\n",
       "    'we',\n",
       "    'have',\n",
       "    'witnessed',\n",
       "    'violent',\n",
       "    'attacks',\n",
       "    'on',\n",
       "    'christian',\n",
       "    'priests,',\n",
       "    'missionaries,',\n",
       "    'publishers',\n",
       "    'or',\n",
       "    'converts.'])),\n",
       " (908215,\n",
       "  (['det',\n",
       "    'finns',\n",
       "    'mycket',\n",
       "    'för',\n",
       "    'europeiska',\n",
       "    'unionen',\n",
       "    'att',\n",
       "    'göra,',\n",
       "    'särskilt',\n",
       "    'för',\n",
       "    'de',\n",
       "    'mänskliga',\n",
       "    'rättigheterna.'],\n",
       "   ['there',\n",
       "    'is',\n",
       "    'a',\n",
       "    'lot',\n",
       "    'for',\n",
       "    'the',\n",
       "    'european',\n",
       "    'union',\n",
       "    'to',\n",
       "    'do',\n",
       "    'there,',\n",
       "    'especially',\n",
       "    'regarding',\n",
       "    'human',\n",
       "    'rights.'])),\n",
       " (932485,\n",
       "  (['alla',\n",
       "    'som',\n",
       "    'följer',\n",
       "    'mig',\n",
       "    'i',\n",
       "    'mitt',\n",
       "    'politiska',\n",
       "    'arbete',\n",
       "    'och',\n",
       "    'mina',\n",
       "    'uttalanden',\n",
       "    'känner',\n",
       "    'till',\n",
       "    'att',\n",
       "    'jag',\n",
       "    'är',\n",
       "    'en',\n",
       "    'kritisk',\n",
       "    'politiker',\n",
       "    'och',\n",
       "    'att',\n",
       "    'jag',\n",
       "    'också',\n",
       "    'är',\n",
       "    'mycket',\n",
       "    'kritisk',\n",
       "    'till',\n",
       "    'eu',\n",
       "    'men',\n",
       "    'att',\n",
       "    'jag',\n",
       "    'fortfarande',\n",
       "    'är',\n",
       "    'en',\n",
       "    'stor',\n",
       "    'förespråkare',\n",
       "    'för',\n",
       "    'europa.'],\n",
       "   ['anyone',\n",
       "    'who',\n",
       "    'follows',\n",
       "    'me',\n",
       "    'in',\n",
       "    'my',\n",
       "    'political',\n",
       "    'office',\n",
       "    'and',\n",
       "    'my',\n",
       "    'statements',\n",
       "    'knows',\n",
       "    'that',\n",
       "    'i',\n",
       "    'am',\n",
       "    'a',\n",
       "    'critical',\n",
       "    'politician',\n",
       "    'and',\n",
       "    'that',\n",
       "    'i',\n",
       "    'am',\n",
       "    'also',\n",
       "    'highly',\n",
       "    'critical',\n",
       "    'of',\n",
       "    'the',\n",
       "    'european',\n",
       "    'union',\n",
       "    'but',\n",
       "    'that',\n",
       "    'i',\n",
       "    'am',\n",
       "    'still',\n",
       "    'very',\n",
       "    'strongly',\n",
       "    'pro-european.'])),\n",
       " (955525,\n",
       "  (['kinas',\n",
       "    'regering',\n",
       "    'ger',\n",
       "    'massivt',\n",
       "    'stöd',\n",
       "    'åt',\n",
       "    'de',\n",
       "    'kinesiska',\n",
       "    'företagen.'],\n",
       "   ['the',\n",
       "    'chinese',\n",
       "    'government',\n",
       "    'gives',\n",
       "    'massive',\n",
       "    'support',\n",
       "    'to',\n",
       "    'chinese',\n",
       "    'firms.'])),\n",
       " (978435,\n",
       "  (['en',\n",
       "    'skyddande',\n",
       "    'lag',\n",
       "    'är',\n",
       "    'också',\n",
       "    'meningslös',\n",
       "    'om',\n",
       "    'den',\n",
       "    'inte',\n",
       "    'kan',\n",
       "    'användas',\n",
       "    'av',\n",
       "    'dem',\n",
       "    'som',\n",
       "    'den',\n",
       "    'är',\n",
       "    'avsedd',\n",
       "    'att',\n",
       "    'skydda.'],\n",
       "   ['a',\n",
       "    'protective',\n",
       "    'law',\n",
       "    'is',\n",
       "    'also',\n",
       "    'worthless',\n",
       "    'unless',\n",
       "    'it',\n",
       "    'can',\n",
       "    'actually',\n",
       "    'be',\n",
       "    'used',\n",
       "    'by',\n",
       "    'those',\n",
       "    'it',\n",
       "    'is',\n",
       "    'designed',\n",
       "    'to',\n",
       "    'protect.'])),\n",
       " (1110705,\n",
       "  (['jag',\n",
       "    'har',\n",
       "    'en',\n",
       "    'fråga',\n",
       "    'till',\n",
       "    'kommissionsledamot',\n",
       "    'dimas:',\n",
       "    'kan',\n",
       "    'han,',\n",
       "    'efter',\n",
       "    'vår',\n",
       "    'överenskommelse',\n",
       "    'vid',\n",
       "    'första',\n",
       "    'behandlingen',\n",
       "    'om',\n",
       "    'mitt',\n",
       "    'betänkande',\n",
       "    'om',\n",
       "    'översynen',\n",
       "    'av',\n",
       "    'gemenskapens',\n",
       "    'system',\n",
       "    'för',\n",
       "    'utsläppshandel',\n",
       "    'för',\n",
       "    'sex',\n",
       "    'veckor',\n",
       "    'sedan,',\n",
       "    'i',\n",
       "    'dag',\n",
       "    'tala',\n",
       "    'om',\n",
       "    'exakt',\n",
       "    'vilket',\n",
       "    'arbetsprogram',\n",
       "    'som',\n",
       "    'har',\n",
       "    'inletts',\n",
       "    'för',\n",
       "    'att',\n",
       "    'förbereda',\n",
       "    'kommittébesluten,',\n",
       "    'i',\n",
       "    'synnerhet',\n",
       "    'vad',\n",
       "    'gäller',\n",
       "    'tidsplanen',\n",
       "    'och',\n",
       "    'delaktigheten',\n",
       "    'för',\n",
       "    'parlamentet',\n",
       "    'och',\n",
       "    'aktörerna?'],\n",
       "   ['i',\n",
       "    'have',\n",
       "    'one',\n",
       "    'question',\n",
       "    'for',\n",
       "    'commissioner',\n",
       "    'dimas:',\n",
       "    'following',\n",
       "    'our',\n",
       "    'first-reading',\n",
       "    'agreement',\n",
       "    'on',\n",
       "    'my',\n",
       "    'report',\n",
       "    'on',\n",
       "    'the',\n",
       "    'revised',\n",
       "    'eu-ets',\n",
       "    'six',\n",
       "    'weeks',\n",
       "    'ago,',\n",
       "    'could',\n",
       "    'the',\n",
       "    'commissioner',\n",
       "    'put',\n",
       "    'on',\n",
       "    'record',\n",
       "    'today',\n",
       "    'exactly',\n",
       "    'what',\n",
       "    'work',\n",
       "    'programme',\n",
       "    'is',\n",
       "    'in',\n",
       "    'train',\n",
       "    'for',\n",
       "    'preparing',\n",
       "    'the',\n",
       "    'comitology',\n",
       "    'decisions,',\n",
       "    'particularly',\n",
       "    'the',\n",
       "    'timing',\n",
       "    'and',\n",
       "    'involvement',\n",
       "    'of',\n",
       "    'this',\n",
       "    'parliament',\n",
       "    'and',\n",
       "    'the',\n",
       "    'stakeholders?'])),\n",
       " (1117260,\n",
       "  (['fram',\n",
       "    'till',\n",
       "    'det',\n",
       "    'sjunde',\n",
       "    'ramprogrammet',\n",
       "    'täcktes',\n",
       "    'båda',\n",
       "    'dessa',\n",
       "    'områden',\n",
       "    'med',\n",
       "    'samma',\n",
       "    'medel,',\n",
       "    'och',\n",
       "    'de',\n",
       "    'rapporterade',\n",
       "    'till',\n",
       "    'generaldirektoratet',\n",
       "    'för',\n",
       "    'fiskerifrågor,',\n",
       "    'vilket',\n",
       "    'gjorde',\n",
       "    'att',\n",
       "    'de',\n",
       "    'kunde',\n",
       "    'komplettera',\n",
       "    'varandra.'],\n",
       "   ['up',\n",
       "    'until',\n",
       "    'the',\n",
       "    'seventh',\n",
       "    'framework',\n",
       "    'programme,',\n",
       "    'both',\n",
       "    'of',\n",
       "    'these',\n",
       "    'fields',\n",
       "    'were',\n",
       "    'covered',\n",
       "    'by',\n",
       "    'the',\n",
       "    'same',\n",
       "    'funds,',\n",
       "    'and',\n",
       "    'reported',\n",
       "    'to',\n",
       "    'the',\n",
       "    'fisheries',\n",
       "    'directorate-general,',\n",
       "    'which',\n",
       "    'enabled',\n",
       "    'them',\n",
       "    'to',\n",
       "    'complement',\n",
       "    'each',\n",
       "    'other.'])),\n",
       " (1133745,\n",
       "  (['vi',\n",
       "    'är',\n",
       "    'alla',\n",
       "    'medvetna',\n",
       "    'om',\n",
       "    'kritiken',\n",
       "    'i',\n",
       "    'våra',\n",
       "    'egna',\n",
       "    'länder,',\n",
       "    'där',\n",
       "    'människor',\n",
       "    'undrar',\n",
       "    'om',\n",
       "    'det',\n",
       "    'verkligen',\n",
       "    'är',\n",
       "    'meningsfullt',\n",
       "    'att',\n",
       "    'fortsätta',\n",
       "    'att',\n",
       "    'investera',\n",
       "    'pengar',\n",
       "    'i',\n",
       "    'afrika,',\n",
       "    'framför',\n",
       "    'allt',\n",
       "    'i',\n",
       "    'dessa',\n",
       "    'tider',\n",
       "    'av',\n",
       "    'ekonomisk',\n",
       "    'kris.'],\n",
       "   ['we',\n",
       "    'are',\n",
       "    'all',\n",
       "    'aware',\n",
       "    'of',\n",
       "    'the',\n",
       "    'criticism',\n",
       "    'in',\n",
       "    'our',\n",
       "    'own',\n",
       "    'countries,',\n",
       "    'where',\n",
       "    'people',\n",
       "    'wonder',\n",
       "    'whether',\n",
       "    'it',\n",
       "    'really',\n",
       "    'makes',\n",
       "    'sense',\n",
       "    'to',\n",
       "    'continue',\n",
       "    'investing',\n",
       "    'money',\n",
       "    'in',\n",
       "    'africa,',\n",
       "    'especially',\n",
       "    'in',\n",
       "    'these',\n",
       "    'times',\n",
       "    'of',\n",
       "    'economic',\n",
       "    'crisis.'])),\n",
       " (1180925,\n",
       "  (['det',\n",
       "    'aktuella',\n",
       "    'läget',\n",
       "    'är',\n",
       "    'inte',\n",
       "    'tillfredsställande,',\n",
       "    'utan',\n",
       "    'snarare',\n",
       "    'ganska',\n",
       "    'grått,',\n",
       "    'för',\n",
       "    'trots',\n",
       "    'alla',\n",
       "    'förklaringar',\n",
       "    'och',\n",
       "    'all',\n",
       "    'uppenbar',\n",
       "    'goodwill',\n",
       "    'är',\n",
       "    'de',\n",
       "    'framsteg',\n",
       "    'som',\n",
       "    'gjorts',\n",
       "    'när',\n",
       "    'det',\n",
       "    'gäller',\n",
       "    'energiinfrastruktur',\n",
       "    'och',\n",
       "    'krismekanismer',\n",
       "    'långt',\n",
       "    'ifrån',\n",
       "    'tillräckliga.'],\n",
       "   ['the',\n",
       "    'present',\n",
       "    'situation',\n",
       "    'is',\n",
       "    'not',\n",
       "    'satisfactory,',\n",
       "    'but',\n",
       "    'rather',\n",
       "    'grey,',\n",
       "    'because,',\n",
       "    'in',\n",
       "    'spite',\n",
       "    'of',\n",
       "    'all',\n",
       "    'the',\n",
       "    'declarations',\n",
       "    'and',\n",
       "    'apparent',\n",
       "    'goodwill,',\n",
       "    'the',\n",
       "    'progress',\n",
       "    'made',\n",
       "    'in',\n",
       "    'terms',\n",
       "    'of',\n",
       "    'energy',\n",
       "    'infrastructure',\n",
       "    'and',\n",
       "    'crisis',\n",
       "    'mechanisms',\n",
       "    'is',\n",
       "    'far',\n",
       "    'from',\n",
       "    'sufficient.']))]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv_en_joined_rdd.take(10) #check how it looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "eca87084-218f-4a1c-b235-6b06ab2f8b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(lines):\n",
    "    #Filter to exclude empty/missing lines\n",
    "    filtered_rdd = lines.filter(lambda x: len(x[1][0]) > 1 and len(x[1][1]) > 1)\n",
    "\n",
    "    #Filter sentences with a small number of words (optional)\n",
    "    threshold = 5\n",
    "    filtered_rdd = filtered_rdd.filter(lambda x: len(x[1][0]) < threshold and len(x[1][1]) < threshold)\n",
    "\n",
    "    # Filter pairs with the same number of words\n",
    "    filtered_rdd = filtered_rdd.filter(lambda x: len(x[1][0]) == len(x[1][1]))\n",
    "    return filtered_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e8bcf11d-7bf9-4ae3-947a-7e30a5c2bffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_rdd = filter(sv_en_joined_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "953d4d5a-0b86-4534-bf0c-8d272fe09315",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/26 21:50:30 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/02/26 21:50:30 INFO DAGScheduler: Got job 58 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/02/26 21:50:30 INFO DAGScheduler: Final stage: ResultStage 82 (runJob at PythonRDD.scala:181)\n",
      "24/02/26 21:50:30 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 81)\n",
      "24/02/26 21:50:30 INFO DAGScheduler: Missing parents: List()\n",
      "24/02/26 21:50:30 INFO DAGScheduler: Submitting ResultStage 82 (PythonRDD[131] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/02/26 21:50:30 INFO MemoryStore: Block broadcast_74 stored as values in memory (estimated size 14.2 KiB, free 433.4 MiB)\n",
      "24/02/26 21:50:30 INFO BlockManagerInfo: Removed broadcast_73_piece0 on host-192-168-2-136-de1:10005 in memory (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:50:30 INFO BlockManagerInfo: Removed broadcast_73_piece0 on 192.168.2.252:10005 in memory (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:50:30 INFO MemoryStore: Block broadcast_74_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.4 MiB)\n",
      "24/02/26 21:50:30 INFO BlockManagerInfo: Added broadcast_74_piece0 in memory on host-192-168-2-136-de1:10005 (size: 7.7 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:50:30 INFO SparkContext: Created broadcast 74 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/26 21:50:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 82 (PythonRDD[131] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/02/26 21:50:30 INFO TaskSchedulerImpl: Adding task set 82.0 with 1 tasks resource profile 0\n",
      "24/02/26 21:50:30 INFO TaskSetManager: Starting task 0.0 in stage 82.0 (TID 194) (192.168.2.252, executor 0, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/02/26 21:50:30 INFO BlockManagerInfo: Added broadcast_74_piece0 in memory on 192.168.2.252:10005 (size: 7.7 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:50:48 INFO TaskSetManager: Finished task 0.0 in stage 82.0 (TID 194) in 18084 ms on 192.168.2.252 (executor 0) (1/1)\n",
      "24/02/26 21:50:48 INFO DAGScheduler: ResultStage 82 (runJob at PythonRDD.scala:181) finished in 18.104 s\n",
      "24/02/26 21:50:48 INFO DAGScheduler: Job 58 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/26 21:50:48 INFO TaskSchedulerImpl: Removed TaskSet 82.0, whose tasks have all completed, from pool \n",
      "24/02/26 21:50:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 82: Stage finished\n",
      "24/02/26 21:50:48 INFO DAGScheduler: Job 58 finished: runJob at PythonRDD.scala:181, took 18.110686 s\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1066660,\n",
       "  (['kriser', 'skapar', 'alltid', 'möjligheter.'],\n",
       "   ['crises', 'invariably', 'create', 'opportunities.'])),\n",
       " (1159790,\n",
       "  (['debatten', 'är', 'härmed', 'avslutad.'],\n",
       "   ['the', 'debate', 'is', 'closed.'])),\n",
       " (907160,\n",
       "  (['-', 'betänkande:', 'christopher', 'heaton-harris'],\n",
       "   ['-', 'report:', 'christopher', 'heaton-harris'])),\n",
       " (1187360, (['(blandade', 'reaktioner)'], ['(mixed', 'reactions)'])),\n",
       " (1293000,\n",
       "  (['frågestunden', 'är', 'härmed', 'avslutad.'],\n",
       "   ['that', 'concludes', 'question', 'time.'])),\n",
       " (1386415, (['bra', 'jobbat!'], ['well', 'done!'])),\n",
       " (1712025,\n",
       "  (['jag', 'förklarar', 'debatten', 'avslutad.'],\n",
       "   ['the', 'debate', 'is', 'closed.'])),\n",
       " (974925,\n",
       "  (['debatten', 'är', 'härmed', 'avslutad.'],\n",
       "   ['the', 'debate', 'is', 'closed.'])),\n",
       " (1092995,\n",
       "  (['skriftliga', 'förklaringar', '(artikel', '142)'],\n",
       "   ['written', 'statements', '(rule', '142)'])),\n",
       " (1103760,\n",
       "  (['debatten', 'är', 'härmed', 'avslutad.'],\n",
       "   ['the', 'debate', 'is', 'closed.']))]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b29c00f8-679c-4357-baa7-b0414ef06506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Filter to exclude empty/missing lines\n",
    "# filtered_rdd = joined_rdd.filter(lambda x: x[1][0] and x[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81fb3eff-d5ad-4442-919d-7cecdc1d7f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 5: Filter sentences with a small number of words (optional)\n",
    "# You can add a filter here if needed\n",
    "\n",
    "# Step 6: Filter pairs with the same number of words\n",
    "# filtered_rdd = filtered_rdd.filter(lambda x: len(x[1][0]) == len(x[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e41d2309-e687-4477-8ed7-e9328711a6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Pair each word in the sentences\n",
    "word_pairs_rdd = filtered_rdd.map(lambda x: zip(x[1][0], x[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b1d1c9da-1dea-42b5-8d99-995acf61e5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/26 21:52:02 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/02/26 21:52:02 INFO DAGScheduler: Got job 60 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/02/26 21:52:02 INFO DAGScheduler: Final stage: ResultStage 86 (runJob at PythonRDD.scala:181)\n",
      "24/02/26 21:52:02 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 85)\n",
      "24/02/26 21:52:02 INFO DAGScheduler: Missing parents: List()\n",
      "24/02/26 21:52:02 INFO DAGScheduler: Submitting ResultStage 86 (PythonRDD[133] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/02/26 21:52:02 INFO MemoryStore: Block broadcast_76 stored as values in memory (estimated size 14.5 KiB, free 433.4 MiB)\n",
      "24/02/26 21:52:02 INFO BlockManagerInfo: Removed broadcast_75_piece0 on host-192-168-2-136-de1:10005 in memory (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:52:02 INFO MemoryStore: Block broadcast_76_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 433.4 MiB)\n",
      "24/02/26 21:52:02 INFO BlockManagerInfo: Removed broadcast_75_piece0 on 192.168.2.252:10005 in memory (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:52:02 INFO BlockManagerInfo: Added broadcast_76_piece0 in memory on host-192-168-2-136-de1:10005 (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/26 21:52:02 INFO SparkContext: Created broadcast 76 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/26 21:52:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 86 (PythonRDD[133] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/02/26 21:52:02 INFO TaskSchedulerImpl: Adding task set 86.0 with 1 tasks resource profile 0\n",
      "24/02/26 21:52:02 INFO TaskSetManager: Starting task 0.0 in stage 86.0 (TID 196) (192.168.2.250, executor 1, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/02/26 21:52:02 INFO BlockManagerInfo: Added broadcast_76_piece0 in memory on 192.168.2.250:10005 (size: 7.8 KiB, free: 434.3 MiB)\n",
      "[Stage 86:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('utgör', 'is'), ('detta', 'that'), ('ett', 'a'), ('problem?', 'problem?')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/26 21:52:21 INFO TaskSetManager: Finished task 0.0 in stage 86.0 (TID 196) in 19410 ms on 192.168.2.250 (executor 1) (1/1)\n",
      "24/02/26 21:52:21 INFO TaskSchedulerImpl: Removed TaskSet 86.0, whose tasks have all completed, from pool \n",
      "24/02/26 21:52:21 INFO DAGScheduler: ResultStage 86 (runJob at PythonRDD.scala:181) finished in 19.424 s\n",
      "24/02/26 21:52:21 INFO DAGScheduler: Job 60 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/26 21:52:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 86: Stage finished\n",
      "24/02/26 21:52:21 INFO DAGScheduler: Job 60 finished: runJob at PythonRDD.scala:181, took 19.430921 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(list(word_pairs_rdd.take(1)[0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9334c2e7-b8e5-49dd-a7fb-bb26b06c4241",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/26 22:07:03 INFO SparkContext: Starting job: takeOrdered at /tmp/ipykernel_31804/2986523124.py:6\n",
      "24/02/26 22:07:03 INFO DAGScheduler: Registering RDD 135 (reduceByKey at /tmp/ipykernel_31804/2986523124.py:3) as input to shuffle 14\n",
      "24/02/26 22:07:03 INFO DAGScheduler: Got job 61 (takeOrdered at /tmp/ipykernel_31804/2986523124.py:6) with 5 output partitions\n",
      "24/02/26 22:07:03 INFO DAGScheduler: Final stage: ResultStage 89 (takeOrdered at /tmp/ipykernel_31804/2986523124.py:6)\n",
      "24/02/26 22:07:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 88)\n",
      "24/02/26 22:07:03 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 88)\n",
      "24/02/26 22:07:03 INFO DAGScheduler: Submitting ShuffleMapStage 88 (PairwiseRDD[135] at reduceByKey at /tmp/ipykernel_31804/2986523124.py:3), which has no missing parents\n",
      "24/02/26 22:07:03 INFO MemoryStore: Block broadcast_77 stored as values in memory (estimated size 17.0 KiB, free 433.4 MiB)\n",
      "24/02/26 22:07:03 INFO BlockManagerInfo: Removed broadcast_76_piece0 on host-192-168-2-136-de1:10005 in memory (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/26 22:07:03 INFO MemoryStore: Block broadcast_77_piece0 stored as bytes in memory (estimated size 8.8 KiB, free 433.4 MiB)\n",
      "24/02/26 22:07:03 INFO BlockManagerInfo: Added broadcast_77_piece0 in memory on host-192-168-2-136-de1:10005 (size: 8.8 KiB, free: 434.3 MiB)\n",
      "24/02/26 22:07:03 INFO SparkContext: Created broadcast 77 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/26 22:07:03 INFO DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 88 (PairwiseRDD[135] at reduceByKey at /tmp/ipykernel_31804/2986523124.py:3) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\n",
      "24/02/26 22:07:03 INFO TaskSchedulerImpl: Adding task set 88.0 with 5 tasks resource profile 0\n",
      "24/02/26 22:07:03 INFO TaskSetManager: Starting task 0.0 in stage 88.0 (TID 197) (192.168.2.252, executor 0, partition 0, NODE_LOCAL, 7426 bytes) \n",
      "24/02/26 22:07:03 INFO TaskSetManager: Starting task 1.0 in stage 88.0 (TID 198) (192.168.2.250, executor 1, partition 1, NODE_LOCAL, 7426 bytes) \n",
      "24/02/26 22:07:03 INFO TaskSetManager: Starting task 2.0 in stage 88.0 (TID 199) (192.168.2.252, executor 0, partition 2, NODE_LOCAL, 7426 bytes) \n",
      "24/02/26 22:07:03 INFO TaskSetManager: Starting task 3.0 in stage 88.0 (TID 200) (192.168.2.250, executor 1, partition 3, NODE_LOCAL, 7426 bytes) \n",
      "24/02/26 22:07:03 INFO BlockManagerInfo: Removed broadcast_76_piece0 on 192.168.2.250:10005 in memory (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/26 22:07:03 INFO BlockManagerInfo: Added broadcast_77_piece0 in memory on 192.168.2.250:10005 (size: 8.8 KiB, free: 434.3 MiB)\n",
      "24/02/26 22:07:03 INFO BlockManagerInfo: Added broadcast_77_piece0 in memory on 192.168.2.252:10005 (size: 8.8 KiB, free: 434.3 MiB)\n",
      "24/02/26 22:07:28 INFO TaskSetManager: Starting task 4.0 in stage 88.0 (TID 201) (192.168.2.252, executor 0, partition 4, NODE_LOCAL, 7426 bytes) \n",
      "24/02/26 22:07:28 INFO TaskSetManager: Finished task 0.0 in stage 88.0 (TID 197) in 24793 ms on 192.168.2.252 (executor 0) (1/5)\n",
      "24/02/26 22:07:28 INFO TaskSetManager: Finished task 2.0 in stage 88.0 (TID 199) in 24954 ms on 192.168.2.252 (executor 0) (2/5)\n",
      "24/02/26 22:07:28 INFO TaskSetManager: Finished task 3.0 in stage 88.0 (TID 200) in 25471 ms on 192.168.2.250 (executor 1) (3/5)\n",
      "24/02/26 22:07:30 INFO TaskSetManager: Finished task 1.0 in stage 88.0 (TID 198) in 27209 ms on 192.168.2.250 (executor 1) (4/5)\n",
      "[Stage 88:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avslutad. - closed.: 2534\n",
      "är - is: 2380\n",
      "jag - the: 1324\n",
      "debatten - is: 1324\n",
      "förklarar - debate: 1317\n",
      "debatten - the: 1225\n",
      "härmed - is: 1215\n",
      "är - debate: 1187\n",
      "(artikel - (rule: 893\n",
      "det - that: 852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/26 22:07:51 INFO TaskSetManager: Finished task 4.0 in stage 88.0 (TID 201) in 23361 ms on 192.168.2.252 (executor 0) (5/5)\n",
      "24/02/26 22:07:51 INFO TaskSchedulerImpl: Removed TaskSet 88.0, whose tasks have all completed, from pool \n",
      "24/02/26 22:07:51 INFO DAGScheduler: ShuffleMapStage 88 (reduceByKey at /tmp/ipykernel_31804/2986523124.py:3) finished in 48.170 s\n",
      "24/02/26 22:07:51 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/02/26 22:07:51 INFO DAGScheduler: running: Set()\n",
      "24/02/26 22:07:51 INFO DAGScheduler: waiting: Set(ResultStage 89)\n",
      "24/02/26 22:07:51 INFO DAGScheduler: failed: Set()\n",
      "24/02/26 22:07:51 INFO DAGScheduler: Submitting ResultStage 89 (PythonRDD[138] at takeOrdered at /tmp/ipykernel_31804/2986523124.py:6), which has no missing parents\n",
      "24/02/26 22:07:51 INFO MemoryStore: Block broadcast_78 stored as values in memory (estimated size 11.2 KiB, free 433.4 MiB)\n",
      "24/02/26 22:07:51 INFO MemoryStore: Block broadcast_78_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 433.4 MiB)\n",
      "24/02/26 22:07:51 INFO BlockManagerInfo: Added broadcast_78_piece0 in memory on host-192-168-2-136-de1:10005 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/26 22:07:51 INFO SparkContext: Created broadcast 78 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/26 22:07:51 INFO DAGScheduler: Submitting 5 missing tasks from ResultStage 89 (PythonRDD[138] at takeOrdered at /tmp/ipykernel_31804/2986523124.py:6) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\n",
      "24/02/26 22:07:51 INFO TaskSchedulerImpl: Adding task set 89.0 with 5 tasks resource profile 0\n",
      "24/02/26 22:07:51 INFO TaskSetManager: Starting task 0.0 in stage 89.0 (TID 202) (192.168.2.250, executor 1, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/02/26 22:07:51 INFO TaskSetManager: Starting task 1.0 in stage 89.0 (TID 203) (192.168.2.252, executor 0, partition 1, NODE_LOCAL, 7437 bytes) \n",
      "24/02/26 22:07:51 INFO TaskSetManager: Starting task 2.0 in stage 89.0 (TID 204) (192.168.2.250, executor 1, partition 2, NODE_LOCAL, 7437 bytes) \n",
      "24/02/26 22:07:51 INFO TaskSetManager: Starting task 3.0 in stage 89.0 (TID 205) (192.168.2.252, executor 0, partition 3, NODE_LOCAL, 7437 bytes) \n",
      "24/02/26 22:07:51 INFO BlockManagerInfo: Added broadcast_78_piece0 in memory on 192.168.2.252:10005 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/26 22:07:51 INFO BlockManagerInfo: Added broadcast_78_piece0 in memory on 192.168.2.250:10005 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/26 22:07:51 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 14 to 192.168.2.252:48736\n",
      "24/02/26 22:07:51 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 14 to 192.168.2.250:57716\n",
      "24/02/26 22:07:51 INFO BlockManagerInfo: Removed broadcast_77_piece0 on host-192-168-2-136-de1:10005 in memory (size: 8.8 KiB, free: 434.3 MiB)\n",
      "24/02/26 22:07:51 INFO BlockManagerInfo: Removed broadcast_77_piece0 on 192.168.2.252:10005 in memory (size: 8.8 KiB, free: 434.3 MiB)\n",
      "24/02/26 22:07:51 INFO BlockManagerInfo: Removed broadcast_77_piece0 on 192.168.2.250:10005 in memory (size: 8.8 KiB, free: 434.3 MiB)\n",
      "24/02/26 22:07:51 INFO TaskSetManager: Starting task 4.0 in stage 89.0 (TID 206) (192.168.2.252, executor 0, partition 4, NODE_LOCAL, 7437 bytes) \n",
      "24/02/26 22:07:51 INFO TaskSetManager: Finished task 1.0 in stage 89.0 (TID 203) in 40 ms on 192.168.2.252 (executor 0) (1/5)\n",
      "24/02/26 22:07:51 INFO TaskSetManager: Finished task 3.0 in stage 89.0 (TID 205) in 41 ms on 192.168.2.252 (executor 0) (2/5)\n",
      "24/02/26 22:07:51 INFO TaskSetManager: Finished task 0.0 in stage 89.0 (TID 202) in 44 ms on 192.168.2.250 (executor 1) (3/5)\n",
      "24/02/26 22:07:51 INFO TaskSetManager: Finished task 2.0 in stage 89.0 (TID 204) in 43 ms on 192.168.2.250 (executor 1) (4/5)\n",
      "24/02/26 22:07:51 INFO TaskSetManager: Finished task 4.0 in stage 89.0 (TID 206) in 20 ms on 192.168.2.252 (executor 0) (5/5)\n",
      "24/02/26 22:07:51 INFO TaskSchedulerImpl: Removed TaskSet 89.0, whose tasks have all completed, from pool \n",
      "24/02/26 22:07:51 INFO DAGScheduler: ResultStage 89 (takeOrdered at /tmp/ipykernel_31804/2986523124.py:6) finished in 0.073 s\n",
      "24/02/26 22:07:51 INFO DAGScheduler: Job 61 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/26 22:07:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 89: Stage finished\n",
      "24/02/26 22:07:51 INFO DAGScheduler: Job 61 finished: takeOrdered at /tmp/ipykernel_31804/2986523124.py:6, took 48.253516 s\n",
      "24/02/26 22:08:00 INFO BlockManagerInfo: Removed broadcast_78_piece0 on host-192-168-2-136-de1:10005 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/26 22:08:00 INFO BlockManagerInfo: Removed broadcast_78_piece0 on 192.168.2.250:10005 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/26 22:08:00 INFO BlockManagerInfo: Removed broadcast_78_piece0 on 192.168.2.252:10005 in memory (size: 6.5 KiB, free: 434.3 MiB)\n"
     ]
    }
   ],
   "source": [
    "words_rdd = word_pairs_rdd.flatMap(lambda line: line)\n",
    "word_counts_rdd = words_rdd.map(lambda word: (word, 1))\n",
    "word_freq_rdd = word_counts_rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Print 10 of the most frequently occurring pairs of words\n",
    "top_pairs = word_freq_rdd.takeOrdered(10, key=lambda x: -x[1])\n",
    "for (word, translation), count in top_pairs:\n",
    "    print(f'{word} - {translation}: {count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ca90554-9314-4363-a079-901028a989f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/26 20:51:08 INFO SparkContext: Starting job: takeOrdered at /tmp/ipykernel_31804/2456824845.py:2\n",
      "24/02/26 20:51:08 INFO DAGScheduler: Registering RDD 28 (join at /tmp/ipykernel_31804/4098632872.py:6) as input to shuffle 3\n",
      "24/02/26 20:51:08 INFO DAGScheduler: Registering RDD 32 (reduceByKey at /tmp/ipykernel_31804/3108678048.py:2) as input to shuffle 2\n",
      "24/02/26 20:51:08 INFO DAGScheduler: Got job 12 (takeOrdered at /tmp/ipykernel_31804/2456824845.py:2) with 5 output partitions\n",
      "24/02/26 20:51:08 INFO DAGScheduler: Final stage: ResultStage 16 (takeOrdered at /tmp/ipykernel_31804/2456824845.py:2)\n",
      "24/02/26 20:51:08 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 15)\n",
      "24/02/26 20:51:08 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 15)\n",
      "24/02/26 20:51:08 INFO DAGScheduler: Submitting ShuffleMapStage 14 (PairwiseRDD[28] at join at /tmp/ipykernel_31804/4098632872.py:6), which has no missing parents\n",
      "24/02/26 20:51:08 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 19.3 KiB, free 433.9 MiB)\n",
      "24/02/26 20:51:08 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 8.8 KiB, free 433.9 MiB)\n",
      "24/02/26 20:51:08 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on host-192-168-2-136-de1:10005 (size: 8.8 KiB, free: 434.3 MiB)\n",
      "24/02/26 20:51:08 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/26 20:51:08 INFO DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 14 (PairwiseRDD[28] at join at /tmp/ipykernel_31804/4098632872.py:6) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\n",
      "24/02/26 20:51:08 INFO TaskSchedulerImpl: Adding task set 14.0 with 5 tasks resource profile 0\n",
      "24/02/26 20:51:08 INFO TaskSetManager: Starting task 4.0 in stage 14.0 (TID 29) (192.168.2.250, executor 1, partition 4, NODE_LOCAL, 7788 bytes) \n",
      "24/02/26 20:51:08 INFO BlockManagerInfo: Removed broadcast_15_piece0 on host-192-168-2-136-de1:10005 in memory (size: 5.1 KiB, free: 434.3 MiB)\n",
      "24/02/26 20:51:08 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 192.168.2.250:10005 in memory (size: 5.1 KiB, free: 434.3 MiB)\n",
      "24/02/26 20:51:08 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 192.168.2.250:10005 (size: 8.8 KiB, free: 434.3 MiB)\n",
      "24/02/26 20:51:12 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 30) (192.168.2.252, executor 0, partition 0, ANY, 7788 bytes) \n",
      "24/02/26 20:51:12 INFO TaskSetManager: Starting task 1.0 in stage 14.0 (TID 31) (192.168.2.250, executor 1, partition 1, ANY, 7788 bytes) \n",
      "24/02/26 20:51:12 INFO TaskSetManager: Starting task 2.0 in stage 14.0 (TID 32) (192.168.2.252, executor 0, partition 2, ANY, 7788 bytes) \n",
      "24/02/26 20:51:12 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 192.168.2.252:10005 (size: 8.8 KiB, free: 434.3 MiB)\n",
      "24/02/26 20:51:12 INFO TaskSetManager: Starting task 3.0 in stage 14.0 (TID 33) (192.168.2.250, executor 1, partition 3, ANY, 7788 bytes) \n",
      "24/02/26 20:51:12 WARN TaskSetManager: Lost task 4.0 in stage 14.0 (TID 29) (192.168.2.250 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n",
      "    merger.mergeValues(iterator)\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n",
      "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
      "TypeError: unhashable type: 'list'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "24/02/26 20:51:13 INFO TaskSetManager: Starting task 4.1 in stage 14.0 (TID 34) (192.168.2.252, executor 0, partition 4, ANY, 7788 bytes) \n",
      "24/02/26 20:51:13 WARN TaskSetManager: Lost task 2.0 in stage 14.0 (TID 32) (192.168.2.252 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n",
      "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
      "TypeError: unhashable type: 'list'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "24/02/26 20:51:16 INFO TaskSetManager: Starting task 2.1 in stage 14.0 (TID 35) (192.168.2.250, executor 1, partition 2, ANY, 7788 bytes) \n",
      "24/02/26 20:51:16 INFO TaskSetManager: Lost task 1.0 in stage 14.0 (TID 31) on 192.168.2.250, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n",
      "    merger.mergeValues(iterator)\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n",
      "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
      "TypeError: unhashable type: 'list'\n",
      ") [duplicate 1]\n",
      "24/02/26 20:51:16 INFO TaskSetManager: Lost task 0.0 in stage 14.0 (TID 30) on 192.168.2.252, executor 0: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n",
      "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
      "TypeError: unhashable type: 'list'\n",
      ") [duplicate 1]\n",
      "24/02/26 20:51:16 INFO TaskSetManager: Starting task 0.1 in stage 14.0 (TID 36) (192.168.2.252, executor 0, partition 0, ANY, 7788 bytes) \n",
      "24/02/26 20:51:16 INFO TaskSetManager: Starting task 1.1 in stage 14.0 (TID 37) (192.168.2.250, executor 1, partition 1, ANY, 7788 bytes) \n",
      "24/02/26 20:51:16 INFO TaskSetManager: Lost task 3.0 in stage 14.0 (TID 33) on 192.168.2.250, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n",
      "    merger.mergeValues(iterator)\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n",
      "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
      "TypeError: unhashable type: 'list'\n",
      ") [duplicate 2]\n",
      "24/02/26 20:51:17 INFO TaskSetManager: Starting task 3.1 in stage 14.0 (TID 38) (192.168.2.250, executor 1, partition 3, ANY, 7788 bytes) \n",
      "24/02/26 20:51:17 INFO TaskSetManager: Lost task 2.1 in stage 14.0 (TID 35) on 192.168.2.250, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n",
      "    merger.mergeValues(iterator)\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n",
      "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
      "TypeError: unhashable type: 'list'\n",
      ") [duplicate 3]\n",
      "24/02/26 20:51:17 INFO TaskSetManager: Starting task 2.2 in stage 14.0 (TID 39) (192.168.2.252, executor 0, partition 2, ANY, 7788 bytes) \n",
      "24/02/26 20:51:17 INFO TaskSetManager: Lost task 4.1 in stage 14.0 (TID 34) on 192.168.2.252, executor 0: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n",
      "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
      "TypeError: unhashable type: 'list'\n",
      ") [duplicate 2]\n",
      "24/02/26 20:51:18 INFO TaskSetManager: Starting task 4.2 in stage 14.0 (TID 40) (192.168.2.252, executor 0, partition 4, ANY, 7788 bytes) \n",
      "24/02/26 20:51:18 INFO TaskSetManager: Lost task 2.2 in stage 14.0 (TID 39) on 192.168.2.252, executor 0: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n",
      "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
      "TypeError: unhashable type: 'list'\n",
      ") [duplicate 3]\n",
      "24/02/26 20:51:20 INFO TaskSetManager: Starting task 2.3 in stage 14.0 (TID 41) (192.168.2.252, executor 0, partition 2, ANY, 7788 bytes) \n",
      "24/02/26 20:51:20 INFO TaskSetManager: Lost task 0.1 in stage 14.0 (TID 36) on 192.168.2.252, executor 0: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n",
      "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
      "TypeError: unhashable type: 'list'\n",
      ") [duplicate 4]\n",
      "24/02/26 20:51:21 INFO TaskSetManager: Starting task 0.2 in stage 14.0 (TID 42) (192.168.2.250, executor 1, partition 0, ANY, 7788 bytes) \n",
      "24/02/26 20:51:21 INFO TaskSetManager: Lost task 1.1 in stage 14.0 (TID 37) on 192.168.2.250, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n",
      "    merger.mergeValues(iterator)\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n",
      "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
      "TypeError: unhashable type: 'list'\n",
      ") [duplicate 4]\n",
      "24/02/26 20:51:21 INFO TaskSetManager: Starting task 1.2 in stage 14.0 (TID 43) (192.168.2.250, executor 1, partition 1, ANY, 7788 bytes) \n",
      "24/02/26 20:51:21 INFO TaskSetManager: Lost task 3.1 in stage 14.0 (TID 38) on 192.168.2.250, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n",
      "    merger.mergeValues(iterator)\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n",
      "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
      "TypeError: unhashable type: 'list'\n",
      ") [duplicate 5]\n",
      "24/02/26 20:51:21 INFO TaskSetManager: Starting task 3.2 in stage 14.0 (TID 44) (192.168.2.252, executor 0, partition 3, ANY, 7788 bytes) \n",
      "24/02/26 20:51:21 INFO TaskSetManager: Lost task 2.3 in stage 14.0 (TID 41) on 192.168.2.252, executor 0: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n",
      "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
      "TypeError: unhashable type: 'list'\n",
      ") [duplicate 5]\n",
      "24/02/26 20:51:21 ERROR TaskSetManager: Task 2 in stage 14.0 failed 4 times; aborting job\n",
      "24/02/26 20:51:21 INFO TaskSchedulerImpl: Cancelling stage 14\n",
      "24/02/26 20:51:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage cancelled: Job aborted due to stage failure: Task 2 in stage 14.0 failed 4 times, most recent failure: Lost task 2.3 in stage 14.0 (TID 41) (192.168.2.252 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n",
      "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
      "TypeError: unhashable type: 'list'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:\n",
      "24/02/26 20:51:21 INFO TaskSchedulerImpl: Stage 14 was cancelled\n",
      "24/02/26 20:51:21 INFO DAGScheduler: ShuffleMapStage 14 (join at /tmp/ipykernel_31804/4098632872.py:6) failed in 12.889 s due to Job aborted due to stage failure: Task 2 in stage 14.0 failed 4 times, most recent failure: Lost task 2.3 in stage 14.0 (TID 41) (192.168.2.252 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n",
      "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
      "TypeError: unhashable type: 'list'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:\n",
      "24/02/26 20:51:21 INFO DAGScheduler: Job 12 failed: takeOrdered at /tmp/ipykernel_31804/2456824845.py:2, took 12.908740 s\n",
      "24/02/26 20:51:21 WARN TaskSetManager: Lost task 4.2 in stage 14.0 (TID 40) (192.168.2.252 executor 0): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 14.0 failed 4 times, most recent failure: Lost task 2.3 in stage 14.0 (TID 41) (192.168.2.252 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n",
      "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
      "TypeError: unhashable type: 'list'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/02/26 20:51:21 WARN TaskSetManager: Lost task 3.2 in stage 14.0 (TID 44) (192.168.2.252 executor 0): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 14.0 failed 4 times, most recent failure: Lost task 2.3 in stage 14.0 (TID 41) (192.168.2.252 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n",
      "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
      "TypeError: unhashable type: 'list'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/02/26 20:51:21 WARN TaskSetManager: Lost task 0.2 in stage 14.0 (TID 42) (192.168.2.250 executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 14.0 failed 4 times, most recent failure: Lost task 2.3 in stage 14.0 (TID 41) (192.168.2.252 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n",
      "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
      "TypeError: unhashable type: 'list'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/02/26 20:51:21 WARN TaskSetManager: Lost task 1.2 in stage 14.0 (TID 43) (192.168.2.250 executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 14.0 failed 4 times, most recent failure: Lost task 2.3 in stage 14.0 (TID 41) (192.168.2.252 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n",
      "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
      "TypeError: unhashable type: 'list'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/02/26 20:51:21 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool \n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 14.0 failed 4 times, most recent failure: Lost task 2.3 in stage 14.0 (TID 41) (192.168.2.252 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\nTypeError: unhashable type: 'list'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\nTypeError: unhashable type: 'list'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Step 9: Print some of the most frequently occurring pairs of words\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m top_pairs \u001b[38;5;241m=\u001b[39m \u001b[43mword_translation_counts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtakeOrdered\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (word, translation), count \u001b[38;5;129;01min\u001b[39;00m top_pairs:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtranslation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/rdd.py:2778\u001b[0m, in \u001b[0;36mRDD.takeOrdered\u001b[0;34m(self, num, key)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(a: List[T], b: List[T]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[T]:\n\u001b[1;32m   2776\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m heapq\u001b[38;5;241m.\u001b[39mnsmallest(num, a \u001b[38;5;241m+\u001b[39m b, key)\n\u001b[0;32m-> 2778\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mheapq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnsmallest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerge\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/rdd.py:1924\u001b[0m, in \u001b[0;36mRDD.reduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m reduce(f, iterator, initial)\n\u001b[0;32m-> 1924\u001b[0m vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vals:\n\u001b[1;32m   1926\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reduce(f, vals)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 14.0 failed 4 times, most recent failure: Lost task 2.3 in stage 14.0 (TID 41) (192.168.2.252 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\nTypeError: unhashable type: 'list'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\nTypeError: unhashable type: 'list'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def count_word_frequency(lines):\n",
    "    #need to add all words in a single array:\n",
    "    words = lines.flatMap(lambda line: line)   \n",
    "    word_counts_rdd = words.map(lambda word: (word, 1)) #creating rdd for word counts\n",
    "\n",
    "    word_freq = word_counts_rdd.reduceByKey(lambda oldcount, increment:  oldcount + increment)\n",
    "    return word_freq\n",
    "    \n",
    "\n",
    "def sort_word_frequency_list(word_freq_rdd):\n",
    "    # Sort the word counts in descending order\n",
    "    sorted_word_frequency = word_freq_rdd.sortBy(lambda x: x[1], ascending=False)\n",
    "    return sorted_word_frequency\n",
    "\n",
    "orted_words = sort_word_frequency_list(count_word_frequency(word_pairs_rdd))\n",
    "\n",
    "# Step 8: Use reduceByKey to count occurrences of word-translation pairs\n",
    "word_translation_counts = word_pairs_rdd.flatMap(lambda x: x).map(lambda x: ((x[0], x[1]), 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Step 9: Print some of the most frequently occurring pairs of words\n",
    "top_pairs = word_translation_counts.takeOrdered(10, key=lambda x: -x[1])\n",
    "for (word, translation), count in top_pairs:\n",
    "    print(f'{word} - {translation}: {count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65eda738-e27a-469f-a4fd-900ec739f5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/21 13:10:22 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
      "24/02/21 13:10:22 INFO SparkUI: Stopped Spark web UI at http://host-192-168-2-13-de1:4040\n",
      "24/02/21 13:10:22 INFO StandaloneSchedulerBackend: Shutting down all executors\n",
      "24/02/21 13:10:22 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down\n",
      "24/02/21 13:10:22 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "24/02/21 13:10:22 INFO MemoryStore: MemoryStore cleared\n",
      "24/02/21 13:10:22 INFO BlockManager: BlockManager stopped\n",
      "24/02/21 13:10:22 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "24/02/21 13:10:22 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "24/02/21 13:10:22 INFO SparkContext: Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "# release the cores for another application!\n",
    "# spark_context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6167cac4-7534-4a8b-a217-f06a4a2b5df5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
