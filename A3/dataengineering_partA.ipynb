{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcb40689-fcac-4014-9f17-8111fd417fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/06 16:10:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/06 16:10:43 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.2.250:35464) with ID 1,  ResourceProfileId 0\n",
      "24/03/06 16:10:44 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.250:10005 with 366.3 MiB RAM, BlockManagerId(1, 192.168.2.250, 10005, None)\n",
      "24/03/06 16:10:44 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.2.250:35472) with ID 0,  ResourceProfileId 0\n",
      "24/03/06 16:10:44 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.250:10006 with 366.3 MiB RAM, BlockManagerId(0, 192.168.2.250, 10006, None)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://192.168.2.250:7077\") \\\n",
    "        .appName(\"AlmaLundbergSparkapplicationA3\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", False)\\\n",
    "        .config(\"spark.cores.max\", 4)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\",2)\\\n",
    "        .config(\"spark.driver.port\",9999)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# RDD API\n",
    "spark_context = spark_session.sparkContext\n",
    "\n",
    "spark_context.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffafb5a6-41c1-490f-a5ac-6bdcc88d364f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 16:10:56 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 221.5 KiB, free 434.2 MiB)\n",
      "24/03/06 16:10:56 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 434.2 MiB)\n",
      "24/03/06 16:10:56 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on host-192-168-2-147-de1:10005 (size: 32.6 KiB, free: 434.4 MiB)\n",
      "24/03/06 16:10:56 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
      "24/03/06 16:10:57 INFO FileInputFormat: Total input files to process : 1\n",
      "24/03/06 16:10:57 INFO NetworkTopology: Adding a new node: /default-rack/192.168.2.250:9866\n",
      "24/03/06 16:10:57 INFO SparkContext: Starting job: count at /tmp/ipykernel_14048/3631255496.py:4\n",
      "24/03/06 16:10:57 INFO DAGScheduler: Got job 0 (count at /tmp/ipykernel_14048/3631255496.py:4) with 2 output partitions\n",
      "24/03/06 16:10:57 INFO DAGScheduler: Final stage: ResultStage 0 (count at /tmp/ipykernel_14048/3631255496.py:4)\n",
      "24/03/06 16:10:57 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/06 16:10:57 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/06 16:10:57 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at count at /tmp/ipykernel_14048/3631255496.py:4), which has no missing parents\n",
      "24/03/06 16:10:57 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.1 KiB, free 434.1 MiB)\n",
      "24/03/06 16:10:57 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.1 MiB)\n",
      "24/03/06 16:10:57 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on host-192-168-2-147-de1:10005 (size: 5.5 KiB, free: 434.4 MiB)\n",
      "24/03/06 16:10:57 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 16:10:57 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[2] at count at /tmp/ipykernel_14048/3631255496.py:4) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/06 16:10:57 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n",
      "24/03/06 16:10:57 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 0) (192.168.2.250, executor 0, partition 1, NODE_LOCAL, 7690 bytes) \n",
      "24/03/06 16:10:58 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.250:10006 (size: 5.5 KiB, free: 366.3 MiB)\n",
      "24/03/06 16:10:58 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.250:10006 (size: 32.6 KiB, free: 366.3 MiB)\n",
      "24/03/06 16:11:00 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 1) (192.168.2.250, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/03/06 16:11:01 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 0) in 3703 ms on 192.168.2.250 (executor 0) (1/2)\n",
      "24/03/06 16:11:01 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 36357\n",
      "24/03/06 16:11:02 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 1) in 1121 ms on 192.168.2.250 (executor 0) (2/2)\n",
      "24/03/06 16:11:02 INFO DAGScheduler: ResultStage 0 (count at /tmp/ipykernel_14048/3631255496.py:4) finished in 4.419 s\n",
      "24/03/06 16:11:02 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "24/03/06 16:11:02 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/06 16:11:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "24/03/06 16:11:02 INFO DAGScheduler: Job 0 finished: count at /tmp/ipykernel_14048/3631255496.py:4, took 4.498053 s\n",
      "24/03/06 16:11:02 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 221.5 KiB, free 433.9 MiB)\n",
      "24/03/06 16:11:02 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 433.9 MiB)\n",
      "24/03/06 16:11:02 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on host-192-168-2-147-de1:10005 (size: 32.6 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:11:02 INFO SparkContext: Created broadcast 2 from textFile at NativeMethodAccessorImpl.java:0\n",
      "24/03/06 16:11:02 INFO FileInputFormat: Total input files to process : 1\n",
      "24/03/06 16:11:02 INFO SparkContext: Starting job: count at /tmp/ipykernel_14048/3631255496.py:8\n",
      "24/03/06 16:11:02 INFO DAGScheduler: Got job 1 (count at /tmp/ipykernel_14048/3631255496.py:8) with 3 output partitions\n",
      "24/03/06 16:11:02 INFO DAGScheduler: Final stage: ResultStage 1 (count at /tmp/ipykernel_14048/3631255496.py:8)\n",
      "24/03/06 16:11:02 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/06 16:11:02 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/06 16:11:02 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[5] at count at /tmp/ipykernel_14048/3631255496.py:8), which has no missing parents\n",
      "24/03/06 16:11:02 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 9.1 KiB, free 433.9 MiB)\n",
      "24/03/06 16:11:02 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 433.9 MiB)\n",
      "24/03/06 16:11:02 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on host-192-168-2-147-de1:10005 (size: 5.5 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:11:02 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 16:11:02 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 1 (PythonRDD[5] at count at /tmp/ipykernel_14048/3631255496.py:8) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/03/06 16:11:02 INFO TaskSchedulerImpl: Adding task set 1.0 with 3 tasks resource profile 0\n",
      "24/03/06 16:11:02 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2) (192.168.2.250, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/03/06 16:11:02 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3) (192.168.2.250, executor 1, partition 1, ANY, 7690 bytes) \n",
      "24/03/06 16:11:02 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 4) (192.168.2.250, executor 0, partition 2, ANY, 7690 bytes) \n",
      "24/03/06 16:11:02 INFO BlockManagerInfo: Removed broadcast_1_piece0 on host-192-168-2-147-de1:10005 in memory (size: 5.5 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:11:02 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.2.250:10006 in memory (size: 5.5 KiB, free: 366.3 MiB)\n",
      "24/03/06 16:11:02 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.250:10006 (size: 5.5 KiB, free: 366.3 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines in English file: 1862234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 16:11:02 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.250:10006 (size: 32.6 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:11:02 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.250:10005 (size: 5.5 KiB, free: 366.3 MiB)\n",
      "24/03/06 16:11:02 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 4) in 345 ms on 192.168.2.250 (executor 0) (1/3)\n",
      "24/03/06 16:11:02 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.250:10005 (size: 32.6 KiB, free: 366.3 MiB)\n",
      "24/03/06 16:11:03 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 1567 ms on 192.168.2.250 (executor 0) (2/3)\n",
      "[Stage 1:=======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines in Swedish file: 1862234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 16:11:05 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 3745 ms on 192.168.2.250 (executor 1) (3/3)\n",
      "24/03/06 16:11:05 INFO DAGScheduler: ResultStage 1 (count at /tmp/ipykernel_14048/3631255496.py:8) finished in 3.790 s\n",
      "24/03/06 16:11:05 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/06 16:11:05 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "24/03/06 16:11:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "24/03/06 16:11:05 INFO DAGScheduler: Job 1 finished: count at /tmp/ipykernel_14048/3631255496.py:8, took 3.809059 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Using map and reduce from the Spark API, and loading the text file from HDFS.\n",
    "\n",
    "lines_en = spark_context.textFile(\"hdfs://192.168.2.250:9000/europarl/europarl-v7.sv-en.en\")\n",
    "num_lines_en = lines_en.count()\n",
    "print(f\"Total lines in English file: {num_lines_en}\") \n",
    "\n",
    "lines_sv = spark_context.textFile(\"hdfs://192.168.2.250:9000/europarl/europarl-v7.sv-en.sv\")\n",
    "num_lines_sv = lines_sv.count()\n",
    "print(f\"Total lines in Swedish file: {num_lines_sv}\")\n",
    "\n",
    "\n",
    "# print(lines.first())\n",
    "# words = lines.map(lambda line: line.split(' '))\n",
    "# word_counts = words.map(lambda w: len(w))\n",
    "# total_words = word_counts.reduce(add)\n",
    "# print(f'total words= {total_words}') \n",
    "\n",
    "#lines = spark_context.textFile(\"/home/ubuntu/i_have_a_dream.txt\")\n",
    "# print(lines.first())\n",
    "# words = lines.map(lambda line: line.split(' '))\n",
    "# word_counts = words.map(lambda w: len(w))\n",
    "# total_words = word_counts.reduce(add)\n",
    "# print(f'total words= {total_words}')  \n",
    "# ... the same number of words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0cbef32-5bfa-4190-8bc1-1c4b972a00ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total lines in English file: 1862234\n",
    "# Total lines in Swedish file: 1862234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d574749-5a3d-4e1c-b3e9-efef1a078fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions in English file: 2\n",
      "Number of partitions in Swedish file: 3\n"
     ]
    }
   ],
   "source": [
    "# Counting nmr of partitions in the English file\n",
    "num_partitions_en = lines_en.getNumPartitions()\n",
    "print(f\"Number of partitions in English file: {num_partitions_en}\")\n",
    "\n",
    "# Counting nmr of partitions in the Swedish file\n",
    "num_partitions_sv = lines_sv.getNumPartitions()\n",
    "print(f\"Number of partitions in Swedish file: {num_partitions_sv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d864eddf-cc03-4058-82c3-5d3f11ad1b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of partitions in English file: 2\n",
    "# Number of partitions in Swedish file: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa87b66a-46a1-49eb-9f87-4666a12f237e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(line):\n",
    "    return line.lower().split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2a51f26-9c3c-466c-a409-a5b5bb5abb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_lines_en = lines_en.map(preprocess_text)\n",
    "processed_lines_sv = lines_sv.map(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae1159d-545c-4285-a260-a9f770979069",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cee82ee-1bbb-4521-b47e-c3c4982f9b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 16:12:03 INFO SparkContext: Starting job: count at /tmp/ipykernel_14048/226898882.py:3\n",
      "24/03/06 16:12:03 INFO DAGScheduler: Got job 2 (count at /tmp/ipykernel_14048/226898882.py:3) with 2 output partitions\n",
      "24/03/06 16:12:03 INFO DAGScheduler: Final stage: ResultStage 2 (count at /tmp/ipykernel_14048/226898882.py:3)\n",
      "24/03/06 16:12:03 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/06 16:12:03 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/06 16:12:03 INFO DAGScheduler: Submitting ResultStage 2 (PythonRDD[6] at count at /tmp/ipykernel_14048/226898882.py:3), which has no missing parents\n",
      "24/03/06 16:12:03 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 9.6 KiB, free 433.9 MiB)\n",
      "24/03/06 16:12:03 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 433.9 MiB)\n",
      "24/03/06 16:12:03 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on host-192-168-2-147-de1:10005 (size: 5.7 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:12:03 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 16:12:03 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (PythonRDD[6] at count at /tmp/ipykernel_14048/226898882.py:3) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/06 16:12:03 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks resource profile 0\n",
      "24/03/06 16:12:03 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5) (192.168.2.250, executor 0, partition 1, NODE_LOCAL, 7690 bytes) \n",
      "24/03/06 16:12:03 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.250:10006 (size: 5.7 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:12:05 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 2133 ms on 192.168.2.250 (executor 0) (1/2)\n",
      "24/03/06 16:12:06 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 6) (192.168.2.250, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/03/06 16:12:09 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 6) in 2224 ms on 192.168.2.250 (executor 0) (2/2)\n",
      "24/03/06 16:12:09 INFO DAGScheduler: ResultStage 2 (count at /tmp/ipykernel_14048/226898882.py:3) finished in 5.890 s\n",
      "24/03/06 16:12:09 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/06 16:12:09 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "24/03/06 16:12:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "24/03/06 16:12:09 INFO DAGScheduler: Job 2 finished: count at /tmp/ipykernel_14048/226898882.py:3, took 5.900829 s\n",
      "24/03/06 16:12:09 INFO SparkContext: Starting job: count at /tmp/ipykernel_14048/226898882.py:4\n",
      "24/03/06 16:12:09 INFO DAGScheduler: Got job 3 (count at /tmp/ipykernel_14048/226898882.py:4) with 3 output partitions\n",
      "24/03/06 16:12:09 INFO DAGScheduler: Final stage: ResultStage 3 (count at /tmp/ipykernel_14048/226898882.py:4)\n",
      "24/03/06 16:12:09 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/06 16:12:09 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/06 16:12:09 INFO DAGScheduler: Submitting ResultStage 3 (PythonRDD[7] at count at /tmp/ipykernel_14048/226898882.py:4), which has no missing parents\n",
      "24/03/06 16:12:09 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 9.6 KiB, free 433.9 MiB)\n",
      "24/03/06 16:12:09 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 433.9 MiB)\n",
      "24/03/06 16:12:09 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on host-192-168-2-147-de1:10005 (size: 5.7 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:12:09 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.2.250:10006 in memory (size: 5.7 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:12:09 INFO BlockManagerInfo: Removed broadcast_4_piece0 on host-192-168-2-147-de1:10005 in memory (size: 5.7 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:12:09 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 16:12:09 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 3 (PythonRDD[7] at count at /tmp/ipykernel_14048/226898882.py:4) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/03/06 16:12:09 INFO TaskSchedulerImpl: Adding task set 3.0 with 3 tasks resource profile 0\n",
      "24/03/06 16:12:09 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 7) (192.168.2.250, executor 1, partition 0, ANY, 7690 bytes) \n",
      "24/03/06 16:12:09 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 8) (192.168.2.250, executor 0, partition 1, ANY, 7690 bytes) \n",
      "24/03/06 16:12:09 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 9) (192.168.2.250, executor 1, partition 2, ANY, 7690 bytes) \n",
      "24/03/06 16:12:09 INFO BlockManagerInfo: Removed broadcast_3_piece0 on host-192-168-2-147-de1:10005 in memory (size: 5.5 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:12:09 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.2.250:10006 in memory (size: 5.5 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:12:09 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.2.250:10005 in memory (size: 5.5 KiB, free: 366.3 MiB)\n",
      "24/03/06 16:12:09 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.250:10006 (size: 5.7 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:12:09 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.250:10005 (size: 5.7 KiB, free: 366.3 MiB)\n",
      "24/03/06 16:12:09 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 9) in 573 ms on 192.168.2.250 (executor 1) (1/3)\n",
      "[Stage 3:===================>                                       (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed line count in English: 1862234\n",
      "Processed line count in Swedish: 1862234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 16:12:12 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 7) in 3168 ms on 192.168.2.250 (executor 1) (2/3)\n",
      "24/03/06 16:12:12 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 8) in 3198 ms on 192.168.2.250 (executor 0) (3/3)\n",
      "24/03/06 16:12:12 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "24/03/06 16:12:12 INFO DAGScheduler: ResultStage 3 (count at /tmp/ipykernel_14048/226898882.py:4) finished in 3.240 s\n",
      "24/03/06 16:12:12 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/06 16:12:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "24/03/06 16:12:12 INFO DAGScheduler: Job 3 finished: count at /tmp/ipykernel_14048/226898882.py:4, took 3.247926 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Verify that the line counts still match after the pre-processing:\n",
    "\n",
    "processed_count_en = processed_lines_en.count()\n",
    "processed_count_sv = processed_lines_sv.count()\n",
    "print(f\"Processed line count in English: {processed_count_en}\")\n",
    "print(f\"Processed line count in Swedish: {processed_count_sv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "35e03978-73f7-4c4e-916b-27748e5eab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processed line count in English: 1862234\n",
    "# Processed line count in Swedish: 1862234\n",
    "\n",
    "# The line count still match after pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8790a495-45aa-4bee-9422-a09f1de9a51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 16:12:15 INFO SparkContext: Starting job: takeOrdered at /tmp/ipykernel_14048/2132416953.py:9\n",
      "24/03/06 16:12:15 INFO DAGScheduler: Registering RDD 9 (reduceByKey at /tmp/ipykernel_14048/2132416953.py:6) as input to shuffle 0\n",
      "24/03/06 16:12:15 INFO DAGScheduler: Got job 4 (takeOrdered at /tmp/ipykernel_14048/2132416953.py:9) with 2 output partitions\n",
      "24/03/06 16:12:15 INFO DAGScheduler: Final stage: ResultStage 5 (takeOrdered at /tmp/ipykernel_14048/2132416953.py:9)\n",
      "24/03/06 16:12:15 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\n",
      "24/03/06 16:12:15 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 4)\n",
      "24/03/06 16:12:15 INFO DAGScheduler: Submitting ShuffleMapStage 4 (PairwiseRDD[9] at reduceByKey at /tmp/ipykernel_14048/2132416953.py:6), which has no missing parents\n",
      "24/03/06 16:12:15 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 12.9 KiB, free 433.9 MiB)\n",
      "24/03/06 16:12:15 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 433.9 MiB)\n",
      "24/03/06 16:12:15 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.2.250:10005 in memory (size: 5.7 KiB, free: 366.3 MiB)\n",
      "24/03/06 16:12:15 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.2.250:10006 in memory (size: 5.7 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:12:15 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on host-192-168-2-147-de1:10005 (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:12:15 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 16:12:15 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 4 (PairwiseRDD[9] at reduceByKey at /tmp/ipykernel_14048/2132416953.py:6) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/06 16:12:15 INFO TaskSchedulerImpl: Adding task set 4.0 with 2 tasks resource profile 0\n",
      "24/03/06 16:12:15 INFO BlockManagerInfo: Removed broadcast_5_piece0 on host-192-168-2-147-de1:10005 in memory (size: 5.7 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:12:15 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 10) (192.168.2.250, executor 1, partition 1, NODE_LOCAL, 7679 bytes) \n",
      "24/03/06 16:12:16 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.2.250:10005 (size: 7.8 KiB, free: 366.3 MiB)\n",
      "24/03/06 16:12:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.250:10005 (size: 32.6 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:12:19 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 11) (192.168.2.250, executor 0, partition 0, ANY, 7679 bytes) \n",
      "24/03/06 16:12:19 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.2.250:10006 (size: 7.8 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:12:30 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 10) in 14706 ms on 192.168.2.250 (executor 1) (1/2)\n",
      "24/03/06 16:12:34 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 11) in 14265 ms on 192.168.2.250 (executor 0) (2/2)\n",
      "24/03/06 16:12:34 INFO DAGScheduler: ShuffleMapStage 4 (reduceByKey at /tmp/ipykernel_14048/2132416953.py:6) finished in 18.258 s\n",
      "24/03/06 16:12:34 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "24/03/06 16:12:34 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/03/06 16:12:34 INFO DAGScheduler: running: Set()\n",
      "24/03/06 16:12:34 INFO DAGScheduler: waiting: Set(ResultStage 5)\n",
      "24/03/06 16:12:34 INFO DAGScheduler: failed: Set()\n",
      "24/03/06 16:12:34 INFO DAGScheduler: Submitting ResultStage 5 (PythonRDD[16] at takeOrdered at /tmp/ipykernel_14048/2132416953.py:9), which has no missing parents\n",
      "24/03/06 16:12:34 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 11.1 KiB, free 433.9 MiB)\n",
      "24/03/06 16:12:34 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 433.9 MiB)\n",
      "24/03/06 16:12:34 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on host-192-168-2-147-de1:10005 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:12:34 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 16:12:34 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 5 (PythonRDD[16] at takeOrdered at /tmp/ipykernel_14048/2132416953.py:9) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/06 16:12:34 INFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks resource profile 0\n",
      "24/03/06 16:12:34 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 12) (192.168.2.250, executor 1, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/03/06 16:12:34 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 13) (192.168.2.250, executor 0, partition 1, NODE_LOCAL, 7437 bytes) \n",
      "24/03/06 16:12:34 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.2.250:10005 (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:12:34 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.2.250:10006 (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:12:34 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.2.250:35472\n",
      "24/03/06 16:12:34 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.2.250:35464\n",
      "24/03/06 16:12:34 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 12) in 354 ms on 192.168.2.250 (executor 1) (1/2)\n",
      "24/03/06 16:12:34 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 13) in 367 ms on 192.168.2.250 (executor 0) (2/2)\n",
      "24/03/06 16:12:34 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "24/03/06 16:12:34 INFO DAGScheduler: ResultStage 5 (takeOrdered at /tmp/ipykernel_14048/2132416953.py:9) finished in 0.399 s\n",
      "24/03/06 16:12:34 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/06 16:12:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
      "24/03/06 16:12:34 INFO DAGScheduler: Job 4 finished: takeOrdered at /tmp/ipykernel_14048/2132416953.py:9, took 18.732719 s\n",
      "24/03/06 16:12:34 INFO SparkContext: Starting job: takeOrdered at /tmp/ipykernel_14048/2132416953.py:10\n",
      "24/03/06 16:12:34 INFO DAGScheduler: Registering RDD 13 (reduceByKey at /tmp/ipykernel_14048/2132416953.py:7) as input to shuffle 1\n",
      "24/03/06 16:12:34 INFO DAGScheduler: Got job 5 (takeOrdered at /tmp/ipykernel_14048/2132416953.py:10) with 3 output partitions\n",
      "24/03/06 16:12:34 INFO DAGScheduler: Final stage: ResultStage 7 (takeOrdered at /tmp/ipykernel_14048/2132416953.py:10)\n",
      "24/03/06 16:12:34 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)\n",
      "24/03/06 16:12:34 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 6)\n",
      "24/03/06 16:12:34 INFO DAGScheduler: Submitting ShuffleMapStage 6 (PairwiseRDD[13] at reduceByKey at /tmp/ipykernel_14048/2132416953.py:7), which has no missing parents\n",
      "24/03/06 16:12:34 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 12.9 KiB, free 433.9 MiB)\n",
      "24/03/06 16:12:34 INFO BlockManagerInfo: Removed broadcast_7_piece0 on host-192-168-2-147-de1:10005 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:12:34 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 433.9 MiB)\n",
      "24/03/06 16:12:34 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on host-192-168-2-147-de1:10005 (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:12:34 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 192.168.2.250:10005 in memory (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:12:34 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 192.168.2.250:10006 in memory (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:12:34 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 16:12:34 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 6 (PairwiseRDD[13] at reduceByKey at /tmp/ipykernel_14048/2132416953.py:7) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/03/06 16:12:34 INFO TaskSchedulerImpl: Adding task set 6.0 with 3 tasks resource profile 0\n",
      "24/03/06 16:12:34 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 14) (192.168.2.250, executor 1, partition 0, ANY, 7679 bytes) \n",
      "24/03/06 16:12:34 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 15) (192.168.2.250, executor 0, partition 1, ANY, 7679 bytes) \n",
      "24/03/06 16:12:34 INFO TaskSetManager: Starting task 2.0 in stage 6.0 (TID 16) (192.168.2.250, executor 1, partition 2, ANY, 7679 bytes) \n",
      "24/03/06 16:12:34 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.2.250:10006 in memory (size: 7.8 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:12:34 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.2.250:10005 in memory (size: 7.8 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:12:34 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.2.250:10006 (size: 7.8 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:12:34 INFO BlockManagerInfo: Removed broadcast_6_piece0 on host-192-168-2-147-de1:10005 in memory (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:12:34 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.2.250:10005 (size: 7.8 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:12:36 INFO TaskSetManager: Finished task 2.0 in stage 6.0 (TID 16) in 2146 ms on 192.168.2.250 (executor 1) (1/3)\n",
      "24/03/06 16:12:48 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 14) in 14222 ms on 192.168.2.250 (executor 1) (2/3)\n",
      "24/03/06 16:12:49 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 15) in 14421 ms on 192.168.2.250 (executor 0) (3/3)\n",
      "24/03/06 16:12:49 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "24/03/06 16:12:49 INFO DAGScheduler: ShuffleMapStage 6 (reduceByKey at /tmp/ipykernel_14048/2132416953.py:7) finished in 14.455 s\n",
      "24/03/06 16:12:49 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/03/06 16:12:49 INFO DAGScheduler: running: Set()\n",
      "24/03/06 16:12:49 INFO DAGScheduler: waiting: Set(ResultStage 7)\n",
      "24/03/06 16:12:49 INFO DAGScheduler: failed: Set()\n",
      "24/03/06 16:12:49 INFO DAGScheduler: Submitting ResultStage 7 (PythonRDD[17] at takeOrdered at /tmp/ipykernel_14048/2132416953.py:10), which has no missing parents\n",
      "24/03/06 16:12:49 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 11.1 KiB, free 433.9 MiB)\n",
      "24/03/06 16:12:49 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 433.9 MiB)\n",
      "24/03/06 16:12:49 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on host-192-168-2-147-de1:10005 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:12:49 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 16:12:49 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 7 (PythonRDD[17] at takeOrdered at /tmp/ipykernel_14048/2132416953.py:10) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/03/06 16:12:49 INFO TaskSchedulerImpl: Adding task set 7.0 with 3 tasks resource profile 0\n",
      "24/03/06 16:12:49 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 17) (192.168.2.250, executor 1, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/03/06 16:12:49 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 18) (192.168.2.250, executor 0, partition 1, NODE_LOCAL, 7437 bytes) \n",
      "24/03/06 16:12:49 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 19) (192.168.2.250, executor 1, partition 2, NODE_LOCAL, 7437 bytes) \n",
      "24/03/06 16:12:49 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.2.250:10005 (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:12:49 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.2.250:10006 (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:12:49 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.2.250:35464\n",
      "24/03/06 16:12:49 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.2.250:35472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 words in English:\n",
      "the: 3498375\n",
      "of: 1659758\n",
      "to: 1539760\n",
      "and: 1288401\n",
      "in: 1085993\n",
      "that: 797516\n",
      "a: 773522\n",
      "is: 758050\n",
      "for: 534242\n",
      "we: 522849\n",
      "\n",
      "Top 10 words in Swedish:\n",
      "att: 1706293\n",
      "och: 1344830\n",
      "i: 1050774\n",
      "det: 924866\n",
      "som: 913276\n",
      "för: 908680\n",
      "av: 738068\n",
      "är: 694381\n",
      "en: 620310\n",
      "vi: 539797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 16:12:49 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 18) in 324 ms on 192.168.2.250 (executor 0) (1/3)\n",
      "24/03/06 16:12:49 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 17) in 345 ms on 192.168.2.250 (executor 1) (2/3)\n",
      "24/03/06 16:12:49 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 19) in 379 ms on 192.168.2.250 (executor 1) (3/3)\n",
      "24/03/06 16:12:49 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "24/03/06 16:12:49 INFO DAGScheduler: ResultStage 7 (takeOrdered at /tmp/ipykernel_14048/2132416953.py:10) finished in 0.395 s\n",
      "24/03/06 16:12:49 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/06 16:12:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
      "24/03/06 16:12:49 INFO DAGScheduler: Job 5 finished: takeOrdered at /tmp/ipykernel_14048/2132416953.py:10, took 14.872719 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check 10 most frequent words in English and Swedish\n",
    "\n",
    "words_en = processed_lines_en.flatMap(lambda line: line)\n",
    "words_sv = processed_lines_sv.flatMap(lambda line: line)\n",
    "\n",
    "word_counts_en = words_en.map(lambda word: (word, 1)).reduceByKey(add)\n",
    "word_counts_sv = words_sv.map(lambda word: (word, 1)).reduceByKey(add)\n",
    "\n",
    "top_10_words_en = word_counts_en.takeOrdered(10, key=lambda x: -x[1])\n",
    "top_10_words_sv = word_counts_sv.takeOrdered(10, key=lambda x: -x[1])\n",
    "\n",
    "print(\"\\nTop 10 words in English:\")\n",
    "for word, count in top_10_words_en:\n",
    "    print(f'{word}: {count}')\n",
    "    \n",
    "print(\"\\nTop 10 words in Swedish:\")\n",
    "for word, count in top_10_words_sv:\n",
    "    print(f'{word}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9b84eae1-31fb-4c21-9258-1c814012defb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBS!!! KOLLA DETTA SÅ DET STÄMMER, DETTA ÄR OM DET ÄR SPLIT(' ') MEN SAGAS RESULTAT ÄR OM DET ÄR SPLIT() i preprocessing delen\n",
    "# Top 10 words in English:\n",
    "# the: 3498375\n",
    "# of: 1659758\n",
    "# to: 1539760\n",
    "# and: 1288401\n",
    "# in: 1085993\n",
    "# that: 797516\n",
    "# a: 773522\n",
    "# is: 758050\n",
    "# for: 534242\n",
    "# we: 522849\n",
    "\n",
    "# Top 10 words in Swedish:\n",
    "# att: 1706293\n",
    "# och: 1344830\n",
    "# i: 1050774\n",
    "# det: 924866\n",
    "# som: 913276\n",
    "# för: 908680\n",
    "# av: 738068\n",
    "# är: 694381\n",
    "# en: 620310\n",
    "# vi: 539797\n",
    "\n",
    "# The results are reasonable, as the most frequent words are common words used to build up sentences (konjunctions, prepositions etc)\n",
    "# In the Swedish translation for example, the words also match most of the top 10 most frequent words in the Swedish language like i, och, att, det, som, en, är, av, för "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "435ab700-0ceb-4bc7-accf-e58f56667379",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 16:13:16 INFO SparkContext: Starting job: zipWithIndex at /tmp/ipykernel_14048/4241438500.py:2\n",
      "24/03/06 16:13:16 INFO DAGScheduler: Got job 6 (zipWithIndex at /tmp/ipykernel_14048/4241438500.py:2) with 3 output partitions\n",
      "24/03/06 16:13:16 INFO DAGScheduler: Final stage: ResultStage 8 (zipWithIndex at /tmp/ipykernel_14048/4241438500.py:2)\n",
      "24/03/06 16:13:16 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/06 16:13:16 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/06 16:13:16 INFO DAGScheduler: Submitting ResultStage 8 (PythonRDD[18] at zipWithIndex at /tmp/ipykernel_14048/4241438500.py:2), which has no missing parents\n",
      "24/03/06 16:13:16 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 8.3 KiB, free 433.9 MiB)\n",
      "24/03/06 16:13:16 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 433.9 MiB)\n",
      "24/03/06 16:13:16 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on host-192-168-2-147-de1:10005 (size: 5.1 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:13:16 INFO BlockManagerInfo: Removed broadcast_9_piece0 on host-192-168-2-147-de1:10005 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:13:16 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 16:13:16 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 8 (PythonRDD[18] at zipWithIndex at /tmp/ipykernel_14048/4241438500.py:2) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/03/06 16:13:16 INFO TaskSchedulerImpl: Adding task set 8.0 with 3 tasks resource profile 0\n",
      "24/03/06 16:13:16 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 20) (192.168.2.250, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/03/06 16:13:16 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 21) (192.168.2.250, executor 1, partition 1, ANY, 7690 bytes) \n",
      "24/03/06 16:13:16 INFO TaskSetManager: Starting task 2.0 in stage 8.0 (TID 22) (192.168.2.250, executor 0, partition 2, ANY, 7690 bytes) \n",
      "24/03/06 16:13:16 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 192.168.2.250:10005 in memory (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:13:16 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 192.168.2.250:10006 in memory (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:13:16 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 192.168.2.250:10005 in memory (size: 7.8 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:13:16 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 192.168.2.250:10006 in memory (size: 7.8 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:13:16 INFO BlockManagerInfo: Removed broadcast_8_piece0 on host-192-168-2-147-de1:10005 in memory (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:13:16 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 192.168.2.250:10005 (size: 5.1 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:13:16 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 192.168.2.250:10006 (size: 5.1 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:13:17 INFO TaskSetManager: Finished task 2.0 in stage 8.0 (TID 22) in 502 ms on 192.168.2.250 (executor 0) (1/3)\n",
      "24/03/06 16:13:19 INFO TaskSetManager: Finished task 1.0 in stage 8.0 (TID 21) in 2868 ms on 192.168.2.250 (executor 1) (2/3)\n",
      "24/03/06 16:13:19 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 20) in 3054 ms on 192.168.2.250 (executor 0) (3/3)\n",
      "24/03/06 16:13:19 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "24/03/06 16:13:19 INFO DAGScheduler: ResultStage 8 (zipWithIndex at /tmp/ipykernel_14048/4241438500.py:2) finished in 3.069 s\n",
      "24/03/06 16:13:19 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/06 16:13:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished\n",
      "24/03/06 16:13:19 INFO DAGScheduler: Job 6 finished: zipWithIndex at /tmp/ipykernel_14048/4241438500.py:2, took 3.075145 s\n",
      "24/03/06 16:13:19 INFO SparkContext: Starting job: zipWithIndex at /tmp/ipykernel_14048/4241438500.py:3\n",
      "24/03/06 16:13:19 INFO DAGScheduler: Got job 7 (zipWithIndex at /tmp/ipykernel_14048/4241438500.py:3) with 2 output partitions\n",
      "24/03/06 16:13:19 INFO DAGScheduler: Final stage: ResultStage 9 (zipWithIndex at /tmp/ipykernel_14048/4241438500.py:3)\n",
      "24/03/06 16:13:19 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/06 16:13:19 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/06 16:13:19 INFO DAGScheduler: Submitting ResultStage 9 (PythonRDD[19] at zipWithIndex at /tmp/ipykernel_14048/4241438500.py:3), which has no missing parents\n",
      "24/03/06 16:13:19 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 8.3 KiB, free 433.9 MiB)\n",
      "24/03/06 16:13:19 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 433.9 MiB)\n",
      "24/03/06 16:13:19 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 192.168.2.250:10005 in memory (size: 5.1 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:13:19 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 192.168.2.250:10006 in memory (size: 5.1 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:13:19 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on host-192-168-2-147-de1:10005 (size: 5.1 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:13:19 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 16:13:19 INFO BlockManagerInfo: Removed broadcast_10_piece0 on host-192-168-2-147-de1:10005 in memory (size: 5.1 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:13:19 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 9 (PythonRDD[19] at zipWithIndex at /tmp/ipykernel_14048/4241438500.py:3) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/06 16:13:19 INFO TaskSchedulerImpl: Adding task set 9.0 with 2 tasks resource profile 0\n",
      "24/03/06 16:13:19 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 23) (192.168.2.250, executor 0, partition 1, NODE_LOCAL, 7690 bytes) \n",
      "24/03/06 16:13:19 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 192.168.2.250:10006 (size: 5.1 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:13:21 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 23) in 2020 ms on 192.168.2.250 (executor 0) (1/2)\n",
      "24/03/06 16:13:22 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 24) (192.168.2.250, executor 1, partition 0, ANY, 7690 bytes) \n",
      "24/03/06 16:13:22 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 192.168.2.250:10005 (size: 5.1 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:13:25 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 24) in 2103 ms on 192.168.2.250 (executor 1) (2/2)\n",
      "24/03/06 16:13:25 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "24/03/06 16:13:25 INFO DAGScheduler: ResultStage 9 (zipWithIndex at /tmp/ipykernel_14048/4241438500.py:3) finished in 5.364 s\n",
      "24/03/06 16:13:25 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/06 16:13:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
      "24/03/06 16:13:25 INFO DAGScheduler: Job 7 finished: zipWithIndex at /tmp/ipykernel_14048/4241438500.py:3, took 5.372343 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Assign a unique index to each line in both RDDs and swap so that the line number is the key\n",
    "sv_swapped = processed_lines_sv.zipWithIndex().map(lambda x: (x[1], x[0]))\n",
    "en_swapped = processed_lines_en.zipWithIndex().map(lambda x: (x[1], x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4a3fa84-d2e9-4e52-8013-08ddf55670ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 16:13:57 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/06 16:13:57 INFO DAGScheduler: Got job 8 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/06 16:13:57 INFO DAGScheduler: Final stage: ResultStage 10 (runJob at PythonRDD.scala:181)\n",
      "24/03/06 16:13:57 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/06 16:13:57 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/06 16:13:57 INFO DAGScheduler: Submitting ResultStage 10 (PythonRDD[20] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/06 16:13:57 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 9.2 KiB, free 433.9 MiB)\n",
      "24/03/06 16:13:57 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 5.6 KiB, free 433.9 MiB)\n",
      "24/03/06 16:13:57 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on host-192-168-2-147-de1:10005 (size: 5.6 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:13:57 INFO BlockManagerInfo: Removed broadcast_11_piece0 on host-192-168-2-147-de1:10005 in memory (size: 5.1 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:13:57 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 16:13:57 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 192.168.2.250:10006 in memory (size: 5.1 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:13:57 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 192.168.2.250:10005 in memory (size: 5.1 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:13:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (PythonRDD[20] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/06 16:13:57 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
      "24/03/06 16:13:57 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 25) (192.168.2.250, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/03/06 16:13:57 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 192.168.2.250:10006 (size: 5.6 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:13:57 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 25) in 55 ms on 192.168.2.250 (executor 0) (1/1)\n",
      "24/03/06 16:13:57 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
      "24/03/06 16:13:57 INFO DAGScheduler: ResultStage 10 (runJob at PythonRDD.scala:181) finished in 0.082 s\n",
      "24/03/06 16:13:57 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/06 16:13:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished\n",
      "24/03/06 16:13:57 INFO DAGScheduler: Job 8 finished: runJob at PythonRDD.scala:181, took 0.087319 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, ['återupptagande', 'av', 'sessionen']),\n",
       " (1,\n",
       "  ['jag',\n",
       "   'förklarar',\n",
       "   'europaparlamentets',\n",
       "   'session',\n",
       "   'återupptagen',\n",
       "   'efter',\n",
       "   'avbrottet',\n",
       "   'den',\n",
       "   '17',\n",
       "   'december.',\n",
       "   'jag',\n",
       "   'vill',\n",
       "   'på',\n",
       "   'nytt',\n",
       "   'önska',\n",
       "   'er',\n",
       "   'ett',\n",
       "   'gott',\n",
       "   'nytt',\n",
       "   'år',\n",
       "   'och',\n",
       "   'jag',\n",
       "   'hoppas',\n",
       "   'att',\n",
       "   'ni',\n",
       "   'haft',\n",
       "   'en',\n",
       "   'trevlig',\n",
       "   'semester.']),\n",
       " (2,\n",
       "  ['som',\n",
       "   'ni',\n",
       "   'kunnat',\n",
       "   'konstatera',\n",
       "   'ägde',\n",
       "   '\"den',\n",
       "   'stora',\n",
       "   'år',\n",
       "   '2000-buggen\"',\n",
       "   'aldrig',\n",
       "   'rum.',\n",
       "   'däremot',\n",
       "   'har',\n",
       "   'invånarna',\n",
       "   'i',\n",
       "   'ett',\n",
       "   'antal',\n",
       "   'av',\n",
       "   'våra',\n",
       "   'medlemsländer',\n",
       "   'drabbats',\n",
       "   'av',\n",
       "   'naturkatastrofer',\n",
       "   'som',\n",
       "   'verkligen',\n",
       "   'varit',\n",
       "   'förskräckliga.']),\n",
       " (3,\n",
       "  ['ni',\n",
       "   'har',\n",
       "   'begärt',\n",
       "   'en',\n",
       "   'debatt',\n",
       "   'i',\n",
       "   'ämnet',\n",
       "   'under',\n",
       "   'sammanträdesperiodens',\n",
       "   'kommande',\n",
       "   'dagar.']),\n",
       " (4,\n",
       "  ['till',\n",
       "   'dess',\n",
       "   'vill',\n",
       "   'jag',\n",
       "   'att',\n",
       "   'vi,',\n",
       "   'som',\n",
       "   'ett',\n",
       "   'antal',\n",
       "   'kolleger',\n",
       "   'begärt,',\n",
       "   'håller',\n",
       "   'en',\n",
       "   'tyst',\n",
       "   'minut',\n",
       "   'för',\n",
       "   'offren',\n",
       "   'för',\n",
       "   'bl.a.',\n",
       "   'stormarna',\n",
       "   'i',\n",
       "   'de',\n",
       "   'länder',\n",
       "   'i',\n",
       "   'europeiska',\n",
       "   'unionen',\n",
       "   'som',\n",
       "   'drabbats.']),\n",
       " (5, ['jag', 'ber', 'er', 'resa', 'er', 'för', 'en', 'tyst', 'minut.']),\n",
       " (6, ['(parlamentet', 'höll', 'en', 'tyst', 'minut.)']),\n",
       " (7, ['fru', 'talman!', 'det', 'gäller', 'en', 'ordningsfråga.']),\n",
       " (8,\n",
       "  ['ni',\n",
       "   'känner',\n",
       "   'till',\n",
       "   'från',\n",
       "   'media',\n",
       "   'att',\n",
       "   'det',\n",
       "   'skett',\n",
       "   'en',\n",
       "   'rad',\n",
       "   'bombexplosioner',\n",
       "   'och',\n",
       "   'mord',\n",
       "   'i',\n",
       "   'sri',\n",
       "   'lanka.']),\n",
       " (9,\n",
       "  ['en',\n",
       "   'av',\n",
       "   'de',\n",
       "   'personer',\n",
       "   'som',\n",
       "   'mycket',\n",
       "   'nyligen',\n",
       "   'mördades',\n",
       "   'i',\n",
       "   'sri',\n",
       "   'lanka',\n",
       "   'var',\n",
       "   'kumar',\n",
       "   'ponnambalam,',\n",
       "   'som',\n",
       "   'besökte',\n",
       "   'europaparlamentet',\n",
       "   'för',\n",
       "   'bara',\n",
       "   'några',\n",
       "   'månader',\n",
       "   'sedan.'])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See how it looks, so everything was done correctly\n",
    "sv_swapped.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2df64d79-ba28-46ec-9d10-8234fb07b93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 16:14:06 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/06 16:14:06 INFO DAGScheduler: Got job 9 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/06 16:14:06 INFO DAGScheduler: Final stage: ResultStage 11 (runJob at PythonRDD.scala:181)\n",
      "24/03/06 16:14:06 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/06 16:14:06 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/06 16:14:06 INFO DAGScheduler: Submitting ResultStage 11 (PythonRDD[21] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/06 16:14:06 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 9.2 KiB, free 433.9 MiB)\n",
      "24/03/06 16:14:06 INFO BlockManagerInfo: Removed broadcast_12_piece0 on host-192-168-2-147-de1:10005 in memory (size: 5.6 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:14:06 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 5.6 KiB, free 433.9 MiB)\n",
      "24/03/06 16:14:06 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 192.168.2.250:10006 in memory (size: 5.6 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:14:06 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on host-192-168-2-147-de1:10005 (size: 5.6 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:14:06 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 16:14:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (PythonRDD[21] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/06 16:14:06 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0\n",
      "24/03/06 16:14:06 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 26) (192.168.2.250, executor 1, partition 0, ANY, 7690 bytes) \n",
      "24/03/06 16:14:06 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 192.168.2.250:10005 (size: 5.6 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:14:06 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 26) in 53 ms on 192.168.2.250 (executor 1) (1/1)\n",
      "24/03/06 16:14:06 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "24/03/06 16:14:06 INFO DAGScheduler: ResultStage 11 (runJob at PythonRDD.scala:181) finished in 0.084 s\n",
      "24/03/06 16:14:06 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/06 16:14:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished\n",
      "24/03/06 16:14:06 INFO DAGScheduler: Job 9 finished: runJob at PythonRDD.scala:181, took 0.089945 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, ['resumption', 'of', 'the', 'session']),\n",
       " (1,\n",
       "  ['i',\n",
       "   'declare',\n",
       "   'resumed',\n",
       "   'the',\n",
       "   'session',\n",
       "   'of',\n",
       "   'the',\n",
       "   'european',\n",
       "   'parliament',\n",
       "   'adjourned',\n",
       "   'on',\n",
       "   'friday',\n",
       "   '17',\n",
       "   'december',\n",
       "   '1999,',\n",
       "   'and',\n",
       "   'i',\n",
       "   'would',\n",
       "   'like',\n",
       "   'once',\n",
       "   'again',\n",
       "   'to',\n",
       "   'wish',\n",
       "   'you',\n",
       "   'a',\n",
       "   'happy',\n",
       "   'new',\n",
       "   'year',\n",
       "   'in',\n",
       "   'the',\n",
       "   'hope',\n",
       "   'that',\n",
       "   'you',\n",
       "   'enjoyed',\n",
       "   'a',\n",
       "   'pleasant',\n",
       "   'festive',\n",
       "   'period.']),\n",
       " (2,\n",
       "  ['although,',\n",
       "   'as',\n",
       "   'you',\n",
       "   'will',\n",
       "   'have',\n",
       "   'seen,',\n",
       "   'the',\n",
       "   'dreaded',\n",
       "   \"'millennium\",\n",
       "   \"bug'\",\n",
       "   'failed',\n",
       "   'to',\n",
       "   'materialise,',\n",
       "   'still',\n",
       "   'the',\n",
       "   'people',\n",
       "   'in',\n",
       "   'a',\n",
       "   'number',\n",
       "   'of',\n",
       "   'countries',\n",
       "   'suffered',\n",
       "   'a',\n",
       "   'series',\n",
       "   'of',\n",
       "   'natural',\n",
       "   'disasters',\n",
       "   'that',\n",
       "   'truly',\n",
       "   'were',\n",
       "   'dreadful.']),\n",
       " (3,\n",
       "  ['you',\n",
       "   'have',\n",
       "   'requested',\n",
       "   'a',\n",
       "   'debate',\n",
       "   'on',\n",
       "   'this',\n",
       "   'subject',\n",
       "   'in',\n",
       "   'the',\n",
       "   'course',\n",
       "   'of',\n",
       "   'the',\n",
       "   'next',\n",
       "   'few',\n",
       "   'days,',\n",
       "   'during',\n",
       "   'this',\n",
       "   'part-session.']),\n",
       " (4,\n",
       "  ['in',\n",
       "   'the',\n",
       "   'meantime,',\n",
       "   'i',\n",
       "   'should',\n",
       "   'like',\n",
       "   'to',\n",
       "   'observe',\n",
       "   'a',\n",
       "   \"minute'\",\n",
       "   's',\n",
       "   'silence,',\n",
       "   'as',\n",
       "   'a',\n",
       "   'number',\n",
       "   'of',\n",
       "   'members',\n",
       "   'have',\n",
       "   'requested,',\n",
       "   'on',\n",
       "   'behalf',\n",
       "   'of',\n",
       "   'all',\n",
       "   'the',\n",
       "   'victims',\n",
       "   'concerned,',\n",
       "   'particularly',\n",
       "   'those',\n",
       "   'of',\n",
       "   'the',\n",
       "   'terrible',\n",
       "   'storms,',\n",
       "   'in',\n",
       "   'the',\n",
       "   'various',\n",
       "   'countries',\n",
       "   'of',\n",
       "   'the',\n",
       "   'european',\n",
       "   'union.']),\n",
       " (5, ['please', 'rise,', 'then,', 'for', 'this', \"minute'\", 's', 'silence.']),\n",
       " (6,\n",
       "  ['(the',\n",
       "   'house',\n",
       "   'rose',\n",
       "   'and',\n",
       "   'observed',\n",
       "   'a',\n",
       "   \"minute'\",\n",
       "   's',\n",
       "   'silence)']),\n",
       " (7, ['madam', 'president,', 'on', 'a', 'point', 'of', 'order.']),\n",
       " (8,\n",
       "  ['you',\n",
       "   'will',\n",
       "   'be',\n",
       "   'aware',\n",
       "   'from',\n",
       "   'the',\n",
       "   'press',\n",
       "   'and',\n",
       "   'television',\n",
       "   'that',\n",
       "   'there',\n",
       "   'have',\n",
       "   'been',\n",
       "   'a',\n",
       "   'number',\n",
       "   'of',\n",
       "   'bomb',\n",
       "   'explosions',\n",
       "   'and',\n",
       "   'killings',\n",
       "   'in',\n",
       "   'sri',\n",
       "   'lanka.']),\n",
       " (9,\n",
       "  ['one',\n",
       "   'of',\n",
       "   'the',\n",
       "   'people',\n",
       "   'assassinated',\n",
       "   'very',\n",
       "   'recently',\n",
       "   'in',\n",
       "   'sri',\n",
       "   'lanka',\n",
       "   'was',\n",
       "   'mr',\n",
       "   'kumar',\n",
       "   'ponnambalam,',\n",
       "   'who',\n",
       "   'had',\n",
       "   'visited',\n",
       "   'the',\n",
       "   'european',\n",
       "   'parliament',\n",
       "   'just',\n",
       "   'a',\n",
       "   'few',\n",
       "   'months',\n",
       "   'ago.'])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See how it looks, so everything was done correctly\n",
    "en_swapped.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eba067bb-52db-4d8a-995f-008db52af32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the two RDDs together on the line number key\n",
    "sv_en_joined_rdd = sv_swapped.join(en_swapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15c67527-577e-436b-9c6d-c1d83cd0469f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 16:14:16 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/06 16:14:16 INFO DAGScheduler: Registering RDD 26 (join at /tmp/ipykernel_14048/242331830.py:2) as input to shuffle 2\n",
      "24/03/06 16:14:16 INFO DAGScheduler: Got job 10 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/06 16:14:16 INFO DAGScheduler: Final stage: ResultStage 13 (runJob at PythonRDD.scala:181)\n",
      "24/03/06 16:14:16 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\n",
      "24/03/06 16:14:16 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 12)\n",
      "24/03/06 16:14:16 INFO DAGScheduler: Submitting ShuffleMapStage 12 (PairwiseRDD[26] at join at /tmp/ipykernel_14048/242331830.py:2), which has no missing parents\n",
      "24/03/06 16:14:16 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 18.5 KiB, free 433.9 MiB)\n",
      "24/03/06 16:14:16 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 8.7 KiB, free 433.9 MiB)\n",
      "24/03/06 16:14:16 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on host-192-168-2-147-de1:10005 (size: 8.7 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:14:16 INFO BlockManagerInfo: Removed broadcast_13_piece0 on host-192-168-2-147-de1:10005 in memory (size: 5.6 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:14:16 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 192.168.2.250:10005 in memory (size: 5.6 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:14:17 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 16:14:17 INFO DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 12 (PairwiseRDD[26] at join at /tmp/ipykernel_14048/242331830.py:2) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\n",
      "24/03/06 16:14:17 INFO TaskSchedulerImpl: Adding task set 12.0 with 5 tasks resource profile 0\n",
      "24/03/06 16:14:17 INFO TaskSetManager: Starting task 4.0 in stage 12.0 (TID 27) (192.168.2.250, executor 0, partition 4, NODE_LOCAL, 7788 bytes) \n",
      "24/03/06 16:14:17 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 192.168.2.250:10006 (size: 8.7 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:14:20 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 28) (192.168.2.250, executor 1, partition 0, ANY, 7788 bytes) \n",
      "24/03/06 16:14:20 INFO TaskSetManager: Starting task 1.0 in stage 12.0 (TID 29) (192.168.2.250, executor 0, partition 1, ANY, 7788 bytes) \n",
      "24/03/06 16:14:20 INFO TaskSetManager: Starting task 2.0 in stage 12.0 (TID 30) (192.168.2.250, executor 1, partition 2, ANY, 7788 bytes) \n",
      "24/03/06 16:14:20 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 192.168.2.250:10005 (size: 8.7 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:14:22 INFO TaskSetManager: Starting task 3.0 in stage 12.0 (TID 31) (192.168.2.250, executor 1, partition 3, ANY, 7788 bytes) \n",
      "24/03/06 16:14:22 INFO TaskSetManager: Finished task 2.0 in stage 12.0 (TID 30) in 1766 ms on 192.168.2.250 (executor 1) (1/5)\n",
      "24/03/06 16:14:56 INFO TaskSetManager: Finished task 4.0 in stage 12.0 (TID 27) in 39159 ms on 192.168.2.250 (executor 0) (2/5)\n",
      "24/03/06 16:14:57 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 28) in 36497 ms on 192.168.2.250 (executor 1) (3/5)\n",
      "24/03/06 16:14:57 INFO TaskSetManager: Finished task 1.0 in stage 12.0 (TID 29) in 36739 ms on 192.168.2.250 (executor 0) (4/5)\n",
      "24/03/06 16:14:59 INFO TaskSetManager: Finished task 3.0 in stage 12.0 (TID 31) in 37016 ms on 192.168.2.250 (executor 1) (5/5)\n",
      "24/03/06 16:14:59 INFO DAGScheduler: ShuffleMapStage 12 (join at /tmp/ipykernel_14048/242331830.py:2) finished in 42.732 s\n",
      "24/03/06 16:14:59 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
      "24/03/06 16:14:59 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/03/06 16:14:59 INFO DAGScheduler: running: Set()\n",
      "24/03/06 16:14:59 INFO DAGScheduler: waiting: Set(ResultStage 13)\n",
      "24/03/06 16:14:59 INFO DAGScheduler: failed: Set()\n",
      "24/03/06 16:14:59 INFO DAGScheduler: Submitting ResultStage 13 (PythonRDD[29] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/06 16:14:59 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 12.6 KiB, free 433.9 MiB)\n",
      "24/03/06 16:14:59 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 433.9 MiB)\n",
      "24/03/06 16:14:59 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on host-192-168-2-147-de1:10005 (size: 7.2 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:14:59 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 16:14:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (PythonRDD[29] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/06 16:14:59 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0\n",
      "24/03/06 16:14:59 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 32) (192.168.2.250, executor 0, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/03/06 16:14:59 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 192.168.2.250:10006 (size: 7.2 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:14:59 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 192.168.2.250:35472\n",
      "24/03/06 16:15:18 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 32) in 18920 ms on 192.168.2.250 (executor 0) (1/1)\n",
      "24/03/06 16:15:18 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
      "24/03/06 16:15:18 INFO DAGScheduler: ResultStage 13 (runJob at PythonRDD.scala:181) finished in 18.943 s\n",
      "24/03/06 16:15:18 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/06 16:15:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished\n",
      "24/03/06 16:15:18 INFO DAGScheduler: Job 10 finished: runJob at PythonRDD.scala:181, took 61.696480 s\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(865815,\n",
       "  (['tillsammans',\n",
       "    'med',\n",
       "    'vice',\n",
       "    'ordförande',\n",
       "    'günter',\n",
       "    'verheugen',\n",
       "    'och',\n",
       "    'ett',\n",
       "    'antal',\n",
       "    'kolleger',\n",
       "    'i',\n",
       "    'kommissionen',\n",
       "    'kommer',\n",
       "    'vi',\n",
       "    'att',\n",
       "    'se',\n",
       "    'till',\n",
       "    'att',\n",
       "    'det',\n",
       "    'ges',\n",
       "    'nödvändiga,',\n",
       "    'proportionella',\n",
       "    'och',\n",
       "    'lämpliga',\n",
       "    'svar',\n",
       "    'på',\n",
       "    'de',\n",
       "    'frågor',\n",
       "    'som',\n",
       "    'har',\n",
       "    'uppstått',\n",
       "    'när',\n",
       "    'det',\n",
       "    'gäller',\n",
       "    'allmänhetens',\n",
       "    'förtroende',\n",
       "    'för',\n",
       "    'förvaltningen',\n",
       "    'av',\n",
       "    'global',\n",
       "    'produktsäkerhet.'],\n",
       "   ['together',\n",
       "    'with',\n",
       "    'vice-president',\n",
       "    'verheugen',\n",
       "    'and',\n",
       "    'a',\n",
       "    'number',\n",
       "    'of',\n",
       "    'fellow',\n",
       "    'commissioners,',\n",
       "    'we',\n",
       "    'will',\n",
       "    'ensure',\n",
       "    'the',\n",
       "    'necessary,',\n",
       "    'proportionate',\n",
       "    'and',\n",
       "    'appropriate',\n",
       "    'response',\n",
       "    'to',\n",
       "    'the',\n",
       "    'issues',\n",
       "    'that',\n",
       "    'have',\n",
       "    'surfaced',\n",
       "    'with',\n",
       "    'regard',\n",
       "    'to',\n",
       "    'public',\n",
       "    'confidence',\n",
       "    'in',\n",
       "    'the',\n",
       "    'governance',\n",
       "    'of',\n",
       "    'global',\n",
       "    'product',\n",
       "    'safety.'])),\n",
       " (868125,\n",
       "  (['som',\n",
       "    'namnet',\n",
       "    'antyder',\n",
       "    'kommer',\n",
       "    'europeiska',\n",
       "    'institutet',\n",
       "    'för',\n",
       "    'innovation',\n",
       "    'och',\n",
       "    'teknik',\n",
       "    'att',\n",
       "    'inrikta',\n",
       "    'sig',\n",
       "    'på',\n",
       "    'innovation.'],\n",
       "   ['the',\n",
       "    'european',\n",
       "    'institute',\n",
       "    'of',\n",
       "    'innovation',\n",
       "    'and',\n",
       "    'technology',\n",
       "    'will,',\n",
       "    'as',\n",
       "    'its',\n",
       "    'name',\n",
       "    'suggests,',\n",
       "    'focus',\n",
       "    'on',\n",
       "    'innovation.'])),\n",
       " (869500,\n",
       "  (['utvecklingen',\n",
       "    'av',\n",
       "    'och',\n",
       "    'framgången',\n",
       "    'med',\n",
       "    'en',\n",
       "    'laglig',\n",
       "    'invandringspolitik',\n",
       "    'beror',\n",
       "    'på',\n",
       "    'en',\n",
       "    'ständig',\n",
       "    'kamp',\n",
       "    'mot',\n",
       "    'medaljens',\n",
       "    'baksida:',\n",
       "    'illegal',\n",
       "    'invandring.'],\n",
       "   ['the',\n",
       "    'development',\n",
       "    'and',\n",
       "    'success',\n",
       "    'of',\n",
       "    'a',\n",
       "    'legal',\n",
       "    'immigration',\n",
       "    'policy',\n",
       "    'largely',\n",
       "    'depend',\n",
       "    'on',\n",
       "    'a',\n",
       "    'constant',\n",
       "    'fight',\n",
       "    'against',\n",
       "    'the',\n",
       "    'other',\n",
       "    'side',\n",
       "    'of',\n",
       "    'the',\n",
       "    'coin:',\n",
       "    'illegal',\n",
       "    'immigration.'])),\n",
       " (875220,\n",
       "  (['vi',\n",
       "    'bör',\n",
       "    'alltså',\n",
       "    'inte',\n",
       "    'ha',\n",
       "    'någon',\n",
       "    'trojansk',\n",
       "    'häst',\n",
       "    'som',\n",
       "    'under',\n",
       "    'den',\n",
       "    'så',\n",
       "    'kallade',\n",
       "    'subsidiaritetsprincipens',\n",
       "    'täckmantel',\n",
       "    'utgör',\n",
       "    'ett',\n",
       "    'hot',\n",
       "    'mot',\n",
       "    'medlemsstaternas',\n",
       "    'suveränitet.'],\n",
       "   ['that',\n",
       "    'is,',\n",
       "    'we',\n",
       "    'should',\n",
       "    'not',\n",
       "    'have',\n",
       "    'a',\n",
       "    'trojan',\n",
       "    'horse,',\n",
       "    'under',\n",
       "    'the',\n",
       "    'guise',\n",
       "    'of',\n",
       "    'the',\n",
       "    'so-called',\n",
       "    'principle',\n",
       "    'of',\n",
       "    'subsidiarity,',\n",
       "    'posing',\n",
       "    'a',\n",
       "    'threat',\n",
       "    'to',\n",
       "    'the',\n",
       "    'sovereignty',\n",
       "    'of',\n",
       "    'the',\n",
       "    'member',\n",
       "    'states.'])),\n",
       " (876040,\n",
       "  (['för',\n",
       "    'det',\n",
       "    'andra',\n",
       "    'innehåller',\n",
       "    'det',\n",
       "    'viktiga',\n",
       "    'råd',\n",
       "    'om',\n",
       "    'hur',\n",
       "    'tull',\n",
       "    'ska',\n",
       "    'kunna',\n",
       "    'indrivas',\n",
       "    'med',\n",
       "    'maximal',\n",
       "    'effektivitet.'],\n",
       "   ['secondly,',\n",
       "    'it',\n",
       "    'provides',\n",
       "    'important',\n",
       "    'advice',\n",
       "    'on',\n",
       "    'how',\n",
       "    'to',\n",
       "    'collect',\n",
       "    'duty',\n",
       "    'with',\n",
       "    'maximum',\n",
       "    'efficiency.'])),\n",
       " (877810,\n",
       "  (['de', 'är', 'också', 'helt', 'onödiga.'],\n",
       "   ['they', 'are', 'also', 'completely', 'unnecessary.'])),\n",
       " (878350,\n",
       "  (['annars',\n",
       "    'är',\n",
       "    'vi',\n",
       "    'på',\n",
       "    'väg',\n",
       "    'rakt',\n",
       "    'in',\n",
       "    'i',\n",
       "    'ett',\n",
       "    'kommunikativt',\n",
       "    'kaos.'],\n",
       "   ['otherwise',\n",
       "    'we',\n",
       "    'are',\n",
       "    'heading',\n",
       "    'towards',\n",
       "    'communicative',\n",
       "    'chaos.'])),\n",
       " (883530,\n",
       "  (['kommissionsledamoten',\n",
       "    'har',\n",
       "    'redan',\n",
       "    'nämnt',\n",
       "    'artikel',\n",
       "    '301',\n",
       "    'i',\n",
       "    'den',\n",
       "    'turkiska',\n",
       "    'brottsbalken',\n",
       "    'samt',\n",
       "    'andra',\n",
       "    'reformer',\n",
       "    'som',\n",
       "    'garanterar',\n",
       "    'fullständig',\n",
       "    'och',\n",
       "    'genuin',\n",
       "    'yttrandefrihet',\n",
       "    'och',\n",
       "    'åsiktspluralism',\n",
       "    'i',\n",
       "    'turkiet.'],\n",
       "   ['the',\n",
       "    'commissioner',\n",
       "    'has',\n",
       "    'already',\n",
       "    'mentioned',\n",
       "    'article',\n",
       "    '301',\n",
       "    'of',\n",
       "    'the',\n",
       "    'turkish',\n",
       "    'penal',\n",
       "    'code',\n",
       "    'and',\n",
       "    'other',\n",
       "    'reforms',\n",
       "    'to',\n",
       "    'guarantee',\n",
       "    'full',\n",
       "    'and',\n",
       "    'genuine',\n",
       "    'freedom',\n",
       "    'of',\n",
       "    'expression',\n",
       "    'and',\n",
       "    'plurality',\n",
       "    'of',\n",
       "    'opinion',\n",
       "    'in',\n",
       "    'turkey.'])),\n",
       " (884905,\n",
       "  (['det',\n",
       "    'är',\n",
       "    'därför',\n",
       "    'som',\n",
       "    'det',\n",
       "    'också',\n",
       "    'spelar',\n",
       "    'roll',\n",
       "    'om',\n",
       "    'jag',\n",
       "    'kör',\n",
       "    'en',\n",
       "    'stor',\n",
       "    'bil',\n",
       "    'eller',\n",
       "    'liten',\n",
       "    'bil.'],\n",
       "   ['that',\n",
       "    'is',\n",
       "    'why',\n",
       "    'it',\n",
       "    'also',\n",
       "    'makes',\n",
       "    'a',\n",
       "    'difference',\n",
       "    'whether',\n",
       "    'i',\n",
       "    'drive',\n",
       "    'a',\n",
       "    'large',\n",
       "    'car',\n",
       "    'or',\n",
       "    'a',\n",
       "    'small',\n",
       "    'car.'])),\n",
       " (885445,\n",
       "  (['fullt',\n",
       "    'samarbete',\n",
       "    'med',\n",
       "    'internationella',\n",
       "    'krigsförbrytartribunalen',\n",
       "    'för',\n",
       "    'f.d.',\n",
       "    'jugoslavien',\n",
       "    'i',\n",
       "    'haag',\n",
       "    'är',\n",
       "    'en',\n",
       "    'viktig',\n",
       "    'förutsättning.'],\n",
       "   ['full',\n",
       "    'cooperation',\n",
       "    'with',\n",
       "    'the',\n",
       "    'international',\n",
       "    'criminal',\n",
       "    'tribunal',\n",
       "    'for',\n",
       "    'the',\n",
       "    'former',\n",
       "    'yugoslavia',\n",
       "    'in',\n",
       "    'the',\n",
       "    'hague',\n",
       "    'is',\n",
       "    'an',\n",
       "    'important',\n",
       "    'prerequisite.']))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See how it looks\n",
    "sv_en_joined_rdd.take(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eca87084-218f-4a1c-b235-6b06ab2f8b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter function for the joined RDD according to lab requirements\n",
    "def filter(lines):\n",
    "    #Filter to exclude empty/missing lines\n",
    "    filtered_rdd = lines.filter(lambda x: len(x[1][0]) > 1 and len(x[1][1]) > 1)\n",
    "\n",
    "    #Filter sentences with a small number of words (optional)\n",
    "    threshold = 5\n",
    "    filtered_rdd = filtered_rdd.filter(lambda x: len(x[1][0]) < threshold and len(x[1][1]) < threshold)\n",
    "\n",
    "    # Filter pairs with the same number of words\n",
    "    filtered_rdd = filtered_rdd.filter(lambda x: len(x[1][0]) == len(x[1][1]))\n",
    "    return filtered_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17b8cd1-ea42-43bd-bb8e-29cd22d5e661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call on the filter function to filter the joined RDD\n",
    "filtered_rdd = filter(sv_en_joined_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "953d4d5a-0b86-4534-bf0c-8d272fe09315",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 16:15:39 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/06 16:15:39 INFO DAGScheduler: Got job 11 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/06 16:15:39 INFO DAGScheduler: Final stage: ResultStage 15 (runJob at PythonRDD.scala:181)\n",
      "24/03/06 16:15:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)\n",
      "24/03/06 16:15:39 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/06 16:15:39 INFO DAGScheduler: Submitting ResultStage 15 (PythonRDD[30] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/06 16:15:39 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 14.2 KiB, free 433.8 MiB)\n",
      "24/03/06 16:15:39 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)\n",
      "24/03/06 16:15:39 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on host-192-168-2-147-de1:10005 (size: 7.7 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:15:39 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 16:15:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (PythonRDD[30] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/06 16:15:39 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0\n",
      "24/03/06 16:15:39 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 192.168.2.250:10005 in memory (size: 8.7 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:15:39 INFO BlockManagerInfo: Removed broadcast_14_piece0 on host-192-168-2-147-de1:10005 in memory (size: 8.7 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:15:39 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 192.168.2.250:10006 in memory (size: 8.7 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:15:39 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 33) (192.168.2.250, executor 0, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/03/06 16:15:39 INFO BlockManagerInfo: Removed broadcast_15_piece0 on host-192-168-2-147-de1:10005 in memory (size: 7.2 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:15:39 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 192.168.2.250:10006 in memory (size: 7.2 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:15:39 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 192.168.2.250:10006 (size: 7.7 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:15:58 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 33) in 18910 ms on 192.168.2.250 (executor 0) (1/1)\n",
      "24/03/06 16:15:58 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool \n",
      "24/03/06 16:15:58 INFO DAGScheduler: ResultStage 15 (runJob at PythonRDD.scala:181) finished in 18.933 s\n",
      "24/03/06 16:15:58 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/06 16:15:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished\n",
      "24/03/06 16:15:58 INFO DAGScheduler: Job 11 finished: runJob at PythonRDD.scala:181, took 18.941523 s\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(916685, (['det', 'är', 'inte', 'rätt.'], ['that', 'is', 'not', 'right.'])),\n",
       " (950055, (['(tumult', 'och', 'applåder)'], ['(uproar', 'and', 'applause)'])),\n",
       " (1030115, (['inga', 'ändringar.'], ['no', 'amendments'])),\n",
       " (1093240,\n",
       "  (['skriftliga', 'förklaringar', '(artikel', '142)'],\n",
       "   ['written', 'statements', '(rule', '142)'])),\n",
       " (1114365, (['i', 'vilket', 'forum?'], ['in', 'which', 'forum?'])),\n",
       " (1118050, (['detta', 'måste', 'ändras.'], ['this', 'must', 'change.'])),\n",
       " (1491205,\n",
       "  (['debatten', 'är', 'härmed', 'avslutad.'],\n",
       "   ['the', 'debate', 'is', 'closed.'])),\n",
       " (1426720,\n",
       "  (['debatten', 'är', 'härmed', 'avslutad.'],\n",
       "   ['the', 'debate', 'is', 'closed.'])),\n",
       " (1581900,\n",
       "  (['finns', 'det', 'några', 'synpunkter?'],\n",
       "   ['are', 'there', 'any', 'comments?'])),\n",
       " (993145, (['varför', 'inte?'], ['why', 'not?']))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See how it looks, if it was filtered properly\n",
    "filtered_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8870f63-f06d-46e5-ab1f-20ae96219ab4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 16:34:29 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/06 16:34:29 INFO DAGScheduler: Got job 14 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/06 16:34:29 INFO DAGScheduler: Final stage: ResultStage 22 (runJob at PythonRDD.scala:181)\n",
      "24/03/06 16:34:29 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)\n",
      "24/03/06 16:34:29 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/06 16:34:29 INFO DAGScheduler: Submitting ResultStage 22 (PythonRDD[37] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/06 16:34:29 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 14.5 KiB, free 433.8 MiB)\n",
      "24/03/06 16:34:29 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 433.8 MiB)\n",
      "24/03/06 16:34:29 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on host-192-168-2-147-de1:10005 (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:34:29 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 16:34:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (PythonRDD[37] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/06 16:34:29 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0\n",
      "24/03/06 16:34:29 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 192.168.2.250:10005 in memory (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:34:29 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 192.168.2.250:10006 in memory (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:34:29 INFO BlockManagerInfo: Removed broadcast_19_piece0 on host-192-168-2-147-de1:10005 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:34:29 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 45) (192.168.2.250, executor 0, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/03/06 16:34:29 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 192.168.2.250:10006 in memory (size: 8.8 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:34:29 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 192.168.2.250:10005 in memory (size: 8.8 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:34:29 INFO BlockManagerInfo: Removed broadcast_18_piece0 on host-192-168-2-147-de1:10005 in memory (size: 8.8 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:34:29 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 192.168.2.250:10006 (size: 7.8 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:34:29 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 192.168.2.250:35472\n",
      "[Stage 22:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['skriftliga', 'written']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 16:34:48 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 45) in 18587 ms on 192.168.2.250 (executor 0) (1/1)\n",
      "24/03/06 16:34:48 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool \n",
      "24/03/06 16:34:48 INFO DAGScheduler: ResultStage 22 (runJob at PythonRDD.scala:181) finished in 18.620 s\n",
      "24/03/06 16:34:48 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/06 16:34:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished\n",
      "24/03/06 16:34:48 INFO DAGScheduler: Job 14 finished: runJob at PythonRDD.scala:181, took 18.629988 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# word_pairs_rdd = filtered_rdd.flatMap(lambda x: zip(x[1][0], x[1][1]))\n",
    "\n",
    "# print(list(word_pairs_rdd.take(1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa16d262-d827-46c8-ac6e-85f09d3b426b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 16:35:36 INFO SparkContext: Starting job: takeOrdered at /tmp/ipykernel_14048/2215384202.py:3\n",
      "24/03/06 16:35:36 INFO DAGScheduler: Registering RDD 39 (reduceByKey at /tmp/ipykernel_14048/2215384202.py:1) as input to shuffle 4\n",
      "24/03/06 16:35:36 INFO DAGScheduler: Got job 15 (takeOrdered at /tmp/ipykernel_14048/2215384202.py:3) with 5 output partitions\n",
      "24/03/06 16:35:36 INFO DAGScheduler: Final stage: ResultStage 25 (takeOrdered at /tmp/ipykernel_14048/2215384202.py:3)\n",
      "24/03/06 16:35:36 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 24)\n",
      "24/03/06 16:35:36 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 24)\n",
      "24/03/06 16:35:36 INFO DAGScheduler: Submitting ShuffleMapStage 24 (PairwiseRDD[39] at reduceByKey at /tmp/ipykernel_14048/2215384202.py:1), which has no missing parents\n",
      "24/03/06 16:35:36 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 16.7 KiB, free 433.9 MiB)\n",
      "24/03/06 16:35:36 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 8.8 KiB, free 433.9 MiB)\n",
      "24/03/06 16:35:36 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on host-192-168-2-147-de1:10005 (size: 8.8 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:35:36 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 16:35:36 INFO DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 24 (PairwiseRDD[39] at reduceByKey at /tmp/ipykernel_14048/2215384202.py:1) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\n",
      "24/03/06 16:35:36 INFO TaskSchedulerImpl: Adding task set 24.0 with 5 tasks resource profile 0\n",
      "24/03/06 16:35:36 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 46) (192.168.2.250, executor 1, partition 0, NODE_LOCAL, 7426 bytes) \n",
      "24/03/06 16:35:36 INFO TaskSetManager: Starting task 1.0 in stage 24.0 (TID 47) (192.168.2.250, executor 0, partition 1, NODE_LOCAL, 7426 bytes) \n",
      "24/03/06 16:35:36 INFO TaskSetManager: Starting task 2.0 in stage 24.0 (TID 48) (192.168.2.250, executor 1, partition 2, NODE_LOCAL, 7426 bytes) \n",
      "24/03/06 16:35:36 INFO TaskSetManager: Starting task 3.0 in stage 24.0 (TID 49) (192.168.2.250, executor 0, partition 3, NODE_LOCAL, 7426 bytes) \n",
      "24/03/06 16:35:36 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 192.168.2.250:10006 in memory (size: 7.8 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:35:36 INFO BlockManagerInfo: Removed broadcast_20_piece0 on host-192-168-2-147-de1:10005 in memory (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:35:36 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 192.168.2.250:10006 (size: 8.8 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:35:36 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 192.168.2.250:10005 (size: 8.8 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:35:36 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 192.168.2.250:35464\n",
      "24/03/06 16:36:04 INFO TaskSetManager: Starting task 4.0 in stage 24.0 (TID 50) (192.168.2.250, executor 1, partition 4, NODE_LOCAL, 7426 bytes) \n",
      "24/03/06 16:36:04 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 46) in 28415 ms on 192.168.2.250 (executor 1) (1/5)\n",
      "24/03/06 16:36:06 INFO TaskSetManager: Finished task 1.0 in stage 24.0 (TID 47) in 29938 ms on 192.168.2.250 (executor 0) (2/5)\n",
      "24/03/06 16:36:06 INFO TaskSetManager: Finished task 3.0 in stage 24.0 (TID 49) in 30107 ms on 192.168.2.250 (executor 0) (3/5)\n",
      "24/03/06 16:36:08 INFO TaskSetManager: Finished task 2.0 in stage 24.0 (TID 48) in 32378 ms on 192.168.2.250 (executor 1) (4/5)\n",
      "[Stage 24:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top word translation pairs: [(('avslutad.', 'closed.'), 2534), (('är', 'is'), 2380), (('jag', 'the'), 1324), (('debatten', 'is'), 1324), (('förklarar', 'debate'), 1317), (('debatten', 'the'), 1225), (('härmed', 'is'), 1215), (('är', 'debate'), 1187), (('(artikel', '(rule'), 893), (('det', 'that'), 852)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 16:36:31 INFO TaskSetManager: Finished task 4.0 in stage 24.0 (TID 50) in 26901 ms on 192.168.2.250 (executor 1) (5/5)\n",
      "24/03/06 16:36:31 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool \n",
      "24/03/06 16:36:31 INFO DAGScheduler: ShuffleMapStage 24 (reduceByKey at /tmp/ipykernel_14048/2215384202.py:1) finished in 55.331 s\n",
      "24/03/06 16:36:31 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/03/06 16:36:31 INFO DAGScheduler: running: Set()\n",
      "24/03/06 16:36:31 INFO DAGScheduler: waiting: Set(ResultStage 25)\n",
      "24/03/06 16:36:31 INFO DAGScheduler: failed: Set()\n",
      "24/03/06 16:36:31 INFO DAGScheduler: Submitting ResultStage 25 (PythonRDD[42] at takeOrdered at /tmp/ipykernel_14048/2215384202.py:3), which has no missing parents\n",
      "24/03/06 16:36:31 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 11.2 KiB, free 433.9 MiB)\n",
      "24/03/06 16:36:31 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 433.9 MiB)\n",
      "24/03/06 16:36:31 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on host-192-168-2-147-de1:10005 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:36:31 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 16:36:31 INFO DAGScheduler: Submitting 5 missing tasks from ResultStage 25 (PythonRDD[42] at takeOrdered at /tmp/ipykernel_14048/2215384202.py:3) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\n",
      "24/03/06 16:36:31 INFO TaskSchedulerImpl: Adding task set 25.0 with 5 tasks resource profile 0\n",
      "24/03/06 16:36:31 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 51) (192.168.2.250, executor 1, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/03/06 16:36:31 INFO TaskSetManager: Starting task 1.0 in stage 25.0 (TID 52) (192.168.2.250, executor 0, partition 1, NODE_LOCAL, 7437 bytes) \n",
      "24/03/06 16:36:31 INFO TaskSetManager: Starting task 2.0 in stage 25.0 (TID 53) (192.168.2.250, executor 1, partition 2, NODE_LOCAL, 7437 bytes) \n",
      "24/03/06 16:36:31 INFO TaskSetManager: Starting task 3.0 in stage 25.0 (TID 54) (192.168.2.250, executor 0, partition 3, NODE_LOCAL, 7437 bytes) \n",
      "24/03/06 16:36:31 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 192.168.2.250:10006 (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:36:31 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 192.168.2.250:35472\n",
      "24/03/06 16:36:31 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 192.168.2.250:10005 (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:36:31 INFO BlockManagerInfo: Removed broadcast_21_piece0 on host-192-168-2-147-de1:10005 in memory (size: 8.8 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:36:31 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 192.168.2.250:10005 in memory (size: 8.8 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:36:31 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 192.168.2.250:35464\n",
      "24/03/06 16:36:31 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 192.168.2.250:10006 in memory (size: 8.8 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:36:31 INFO TaskSetManager: Starting task 4.0 in stage 25.0 (TID 55) (192.168.2.250, executor 0, partition 4, NODE_LOCAL, 7437 bytes) \n",
      "24/03/06 16:36:31 INFO TaskSetManager: Finished task 3.0 in stage 25.0 (TID 54) in 59 ms on 192.168.2.250 (executor 0) (1/5)\n",
      "24/03/06 16:36:31 INFO TaskSetManager: Finished task 1.0 in stage 25.0 (TID 52) in 64 ms on 192.168.2.250 (executor 0) (2/5)\n",
      "24/03/06 16:36:31 INFO TaskSetManager: Finished task 2.0 in stage 25.0 (TID 53) in 76 ms on 192.168.2.250 (executor 1) (3/5)\n",
      "24/03/06 16:36:31 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 51) in 82 ms on 192.168.2.250 (executor 1) (4/5)\n",
      "24/03/06 16:36:31 INFO TaskSetManager: Finished task 4.0 in stage 25.0 (TID 55) in 31 ms on 192.168.2.250 (executor 0) (5/5)\n",
      "24/03/06 16:36:31 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool \n",
      "24/03/06 16:36:31 INFO DAGScheduler: ResultStage 25 (takeOrdered at /tmp/ipykernel_14048/2215384202.py:3) finished in 0.105 s\n",
      "24/03/06 16:36:31 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/06 16:36:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished\n",
      "24/03/06 16:36:31 INFO DAGScheduler: Job 15 finished: takeOrdered at /tmp/ipykernel_14048/2215384202.py:3, took 55.452538 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# word_pair_counts = word_pairs_rdd.map(lambda pair: (pair, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# top_word_pairs = word_pair_counts.takeOrdered(10, key=lambda x: -x[1])\n",
    "# print(\"Top word translation pairs:\", top_word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e41d2309-e687-4477-8ed7-e9328711a6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair words in the order they appear in each sentence pair\n",
    "word_pairs_rdd = filtered_rdd.map(lambda x: zip(x[1][0], x[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1d1c9da-1dea-42b5-8d99-995acf61e5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 16:37:52 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/06 16:37:52 INFO DAGScheduler: Got job 16 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/06 16:37:52 INFO DAGScheduler: Final stage: ResultStage 27 (runJob at PythonRDD.scala:181)\n",
      "24/03/06 16:37:52 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 26)\n",
      "24/03/06 16:37:52 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/06 16:37:52 INFO DAGScheduler: Submitting ResultStage 27 (PythonRDD[43] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/06 16:37:52 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 14.5 KiB, free 433.9 MiB)\n",
      "24/03/06 16:37:52 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 433.9 MiB)\n",
      "24/03/06 16:37:52 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on host-192-168-2-147-de1:10005 (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:37:52 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 16:37:52 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 192.168.2.250:10006 in memory (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:37:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (PythonRDD[43] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/06 16:37:52 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0\n",
      "24/03/06 16:37:52 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 56) (192.168.2.250, executor 1, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/03/06 16:37:52 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 192.168.2.250:10005 in memory (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:37:52 INFO BlockManagerInfo: Removed broadcast_22_piece0 on host-192-168-2-147-de1:10005 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:37:52 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 192.168.2.250:10005 (size: 7.8 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:37:52 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 192.168.2.250:35464\n",
      "24/03/06 16:37:56 WARN TaskSetManager: Lost task 0.0 in stage 27.0 (TID 56) (192.168.2.250 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  [Previous line repeated 4 more times]\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4183, in groupByKey\n",
      "    merger.mergeCombiners(it)\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 299, in mergeCombiners\n",
      "    self._spill()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 753, in _spill\n",
      "    os.makedirs(path)\n",
      "  File \"/usr/lib/python3.8/os.py\", line 213, in makedirs\n",
      "    makedirs(head, exist_ok=exist_ok)\n",
      "  File \"/usr/lib/python3.8/os.py\", line 213, in makedirs\n",
      "    makedirs(head, exist_ok=exist_ok)\n",
      "  File \"/usr/lib/python3.8/os.py\", line 223, in makedirs\n",
      "    mkdir(name, mode)\n",
      "OSError: [Errno 28] No space left on device: '/tmp/spark-5cd138a3-4681-40c6-9165-6406769c5be2/executor-094bd768-d4ea-411e-af3e-bc9fa8d3b8d9/blockmgr-f59b52f8-8e08-49db-a5c5-9a06f3e580fd/python/2835641'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "24/03/06 16:37:56 INFO TaskSetManager: Starting task 0.1 in stage 27.0 (TID 57) (192.168.2.250, executor 1, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/03/06 16:38:00 WARN TaskSetManager: Lost task 0.1 in stage 27.0 (TID 57) (192.168.2.250 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  [Previous line repeated 4 more times]\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4183, in groupByKey\n",
      "    merger.mergeCombiners(it)\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 299, in mergeCombiners\n",
      "    self._spill()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 753, in _spill\n",
      "    os.makedirs(path)\n",
      "  File \"/usr/lib/python3.8/os.py\", line 213, in makedirs\n",
      "    makedirs(head, exist_ok=exist_ok)\n",
      "  File \"/usr/lib/python3.8/os.py\", line 213, in makedirs\n",
      "    makedirs(head, exist_ok=exist_ok)\n",
      "  File \"/usr/lib/python3.8/os.py\", line 223, in makedirs\n",
      "    mkdir(name, mode)\n",
      "OSError: [Errno 28] No space left on device: '/tmp/spark-5cd138a3-4681-40c6-9165-6406769c5be2/executor-094bd768-d4ea-411e-af3e-bc9fa8d3b8d9/blockmgr-f59b52f8-8e08-49db-a5c5-9a06f3e580fd/python/2835679'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "24/03/06 16:38:00 INFO TaskSetManager: Starting task 0.2 in stage 27.0 (TID 58) (192.168.2.250, executor 0, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/03/06 16:38:00 WARN TaskSetManager: Lost task 0.2 in stage 27.0 (TID 58) (192.168.2.250 executor 0): java.nio.file.FileSystemException: /tmp/spark-5cd138a3-4681-40c6-9165-6406769c5be2/executor-094bd768-d4ea-411e-af3e-bc9fa8d3b8d9/blockmgr-378ea358-fea2-473c-85ca-1ba5cabf146b/02: No space left on device\n",
      "\tat sun.nio.fs.UnixException.translateToIOException(UnixException.java:91)\n",
      "\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n",
      "\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n",
      "\tat sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)\n",
      "\tat java.nio.file.Files.createDirectory(Files.java:674)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:108)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.containsBlock(DiskBlockManager.scala:157)\n",
      "\tat org.apache.spark.storage.DiskStore.contains(DiskStore.scala:154)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$getCurrentBlockStatus(BlockManager.scala:885)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:2075)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1555)\n",
      "\tat org.apache.spark.storage.BlockManager$BlockStoreUpdater.save(BlockManager.scala:380)\n",
      "\tat org.apache.spark.storage.BlockManager.putBytes(BlockManager.scala:1468)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBlocks$1(TorrentBroadcast.scala:215)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.readBlocks(TorrentBroadcast.scala:192)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$4(TorrentBroadcast.scala:281)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$2(TorrentBroadcast.scala:257)\n",
      "\tat org.apache.spark.util.KeyLock.withLock(KeyLock.scala:64)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$1(TorrentBroadcast.scala:252)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\n",
      "\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:94)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:252)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:109)\n",
      "\tat org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "24/03/06 16:38:00 INFO TaskSetManager: Starting task 0.3 in stage 27.0 (TID 59) (192.168.2.250, executor 0, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/03/06 16:38:00 INFO TaskSetManager: Lost task 0.3 in stage 27.0 (TID 59) on 192.168.2.250, executor 0: java.nio.file.FileSystemException (/tmp/spark-5cd138a3-4681-40c6-9165-6406769c5be2/executor-094bd768-d4ea-411e-af3e-bc9fa8d3b8d9/blockmgr-378ea358-fea2-473c-85ca-1ba5cabf146b/02: No space left on device) [duplicate 1]\n",
      "24/03/06 16:38:00 ERROR TaskSetManager: Task 0 in stage 27.0 failed 4 times; aborting job\n",
      "24/03/06 16:38:00 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool \n",
      "24/03/06 16:38:00 INFO TaskSchedulerImpl: Cancelling stage 27\n",
      "24/03/06 16:38:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 27.0 failed 4 times, most recent failure: Lost task 0.3 in stage 27.0 (TID 59) (192.168.2.250 executor 0): java.nio.file.FileSystemException: /tmp/spark-5cd138a3-4681-40c6-9165-6406769c5be2/executor-094bd768-d4ea-411e-af3e-bc9fa8d3b8d9/blockmgr-378ea358-fea2-473c-85ca-1ba5cabf146b/02: No space left on device\n",
      "\tat sun.nio.fs.UnixException.translateToIOException(UnixException.java:91)\n",
      "\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n",
      "\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n",
      "\tat sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)\n",
      "\tat java.nio.file.Files.createDirectory(Files.java:674)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:108)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.containsBlock(DiskBlockManager.scala:157)\n",
      "\tat org.apache.spark.storage.DiskStore.contains(DiskStore.scala:154)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$getCurrentBlockStatus(BlockManager.scala:885)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:2075)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1555)\n",
      "\tat org.apache.spark.storage.BlockManager$BlockStoreUpdater.save(BlockManager.scala:380)\n",
      "\tat org.apache.spark.storage.BlockManager.putBytes(BlockManager.scala:1468)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBlocks$1(TorrentBroadcast.scala:215)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.readBlocks(TorrentBroadcast.scala:192)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$4(TorrentBroadcast.scala:281)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$2(TorrentBroadcast.scala:257)\n",
      "\tat org.apache.spark.util.KeyLock.withLock(KeyLock.scala:64)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$1(TorrentBroadcast.scala:252)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\n",
      "\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:94)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:252)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:109)\n",
      "\tat org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "24/03/06 16:38:00 INFO DAGScheduler: ResultStage 27 (runJob at PythonRDD.scala:181) failed in 8.547 s due to Job aborted due to stage failure: Task 0 in stage 27.0 failed 4 times, most recent failure: Lost task 0.3 in stage 27.0 (TID 59) (192.168.2.250 executor 0): java.nio.file.FileSystemException: /tmp/spark-5cd138a3-4681-40c6-9165-6406769c5be2/executor-094bd768-d4ea-411e-af3e-bc9fa8d3b8d9/blockmgr-378ea358-fea2-473c-85ca-1ba5cabf146b/02: No space left on device\n",
      "\tat sun.nio.fs.UnixException.translateToIOException(UnixException.java:91)\n",
      "\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n",
      "\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n",
      "\tat sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)\n",
      "\tat java.nio.file.Files.createDirectory(Files.java:674)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:108)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.containsBlock(DiskBlockManager.scala:157)\n",
      "\tat org.apache.spark.storage.DiskStore.contains(DiskStore.scala:154)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$getCurrentBlockStatus(BlockManager.scala:885)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:2075)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1555)\n",
      "\tat org.apache.spark.storage.BlockManager$BlockStoreUpdater.save(BlockManager.scala:380)\n",
      "\tat org.apache.spark.storage.BlockManager.putBytes(BlockManager.scala:1468)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBlocks$1(TorrentBroadcast.scala:215)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.readBlocks(TorrentBroadcast.scala:192)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$4(TorrentBroadcast.scala:281)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$2(TorrentBroadcast.scala:257)\n",
      "\tat org.apache.spark.util.KeyLock.withLock(KeyLock.scala:64)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$1(TorrentBroadcast.scala:252)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\n",
      "\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:94)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:252)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:109)\n",
      "\tat org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "24/03/06 16:38:00 INFO DAGScheduler: Job 16 failed: runJob at PythonRDD.scala:181, took 8.557402 s\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 27.0 failed 4 times, most recent failure: Lost task 0.3 in stage 27.0 (TID 59) (192.168.2.250 executor 0): java.nio.file.FileSystemException: /tmp/spark-5cd138a3-4681-40c6-9165-6406769c5be2/executor-094bd768-d4ea-411e-af3e-bc9fa8d3b8d9/blockmgr-378ea358-fea2-473c-85ca-1ba5cabf146b/02: No space left on device\n\tat sun.nio.fs.UnixException.translateToIOException(UnixException.java:91)\n\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n\tat sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)\n\tat java.nio.file.Files.createDirectory(Files.java:674)\n\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:108)\n\tat org.apache.spark.storage.DiskBlockManager.containsBlock(DiskBlockManager.scala:157)\n\tat org.apache.spark.storage.DiskStore.contains(DiskStore.scala:154)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$getCurrentBlockStatus(BlockManager.scala:885)\n\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:2075)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1555)\n\tat org.apache.spark.storage.BlockManager$BlockStoreUpdater.save(BlockManager.scala:380)\n\tat org.apache.spark.storage.BlockManager.putBytes(BlockManager.scala:1468)\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBlocks$1(TorrentBroadcast.scala:215)\n\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.broadcast.TorrentBroadcast.readBlocks(TorrentBroadcast.scala:192)\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$4(TorrentBroadcast.scala:281)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$2(TorrentBroadcast.scala:257)\n\tat org.apache.spark.util.KeyLock.withLock(KeyLock.scala:64)\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$1(TorrentBroadcast.scala:252)\n\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\n\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:94)\n\tat org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:252)\n\tat org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:109)\n\tat org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.nio.file.FileSystemException: /tmp/spark-5cd138a3-4681-40c6-9165-6406769c5be2/executor-094bd768-d4ea-411e-af3e-bc9fa8d3b8d9/blockmgr-378ea358-fea2-473c-85ca-1ba5cabf146b/02: No space left on device\n\tat sun.nio.fs.UnixException.translateToIOException(UnixException.java:91)\n\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n\tat sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)\n\tat java.nio.file.Files.createDirectory(Files.java:674)\n\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:108)\n\tat org.apache.spark.storage.DiskBlockManager.containsBlock(DiskBlockManager.scala:157)\n\tat org.apache.spark.storage.DiskStore.contains(DiskStore.scala:154)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$getCurrentBlockStatus(BlockManager.scala:885)\n\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:2075)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1555)\n\tat org.apache.spark.storage.BlockManager$BlockStoreUpdater.save(BlockManager.scala:380)\n\tat org.apache.spark.storage.BlockManager.putBytes(BlockManager.scala:1468)\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBlocks$1(TorrentBroadcast.scala:215)\n\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.broadcast.TorrentBroadcast.readBlocks(TorrentBroadcast.scala:192)\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$4(TorrentBroadcast.scala:281)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$2(TorrentBroadcast.scala:257)\n\tat org.apache.spark.util.KeyLock.withLock(KeyLock.scala:64)\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$1(TorrentBroadcast.scala:252)\n\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\n\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:94)\n\tat org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:252)\n\tat org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:109)\n\tat org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# See how it looks\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[43mword_pairs_rdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m])) \n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/rdd.py:2855\u001b[0m, in \u001b[0;36mRDD.take\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   2852\u001b[0m         taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2854\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned \u001b[38;5;241m+\u001b[39m numPartsToTry, totalParts))\n\u001b[0;32m-> 2855\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2857\u001b[0m items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m   2858\u001b[0m partsScanned \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m numPartsToTry\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/context.py:2510\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   2508\u001b[0m mappedRDD \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[1;32m   2509\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2510\u001b[0m sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmappedRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2511\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 27.0 failed 4 times, most recent failure: Lost task 0.3 in stage 27.0 (TID 59) (192.168.2.250 executor 0): java.nio.file.FileSystemException: /tmp/spark-5cd138a3-4681-40c6-9165-6406769c5be2/executor-094bd768-d4ea-411e-af3e-bc9fa8d3b8d9/blockmgr-378ea358-fea2-473c-85ca-1ba5cabf146b/02: No space left on device\n\tat sun.nio.fs.UnixException.translateToIOException(UnixException.java:91)\n\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n\tat sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)\n\tat java.nio.file.Files.createDirectory(Files.java:674)\n\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:108)\n\tat org.apache.spark.storage.DiskBlockManager.containsBlock(DiskBlockManager.scala:157)\n\tat org.apache.spark.storage.DiskStore.contains(DiskStore.scala:154)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$getCurrentBlockStatus(BlockManager.scala:885)\n\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:2075)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1555)\n\tat org.apache.spark.storage.BlockManager$BlockStoreUpdater.save(BlockManager.scala:380)\n\tat org.apache.spark.storage.BlockManager.putBytes(BlockManager.scala:1468)\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBlocks$1(TorrentBroadcast.scala:215)\n\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.broadcast.TorrentBroadcast.readBlocks(TorrentBroadcast.scala:192)\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$4(TorrentBroadcast.scala:281)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$2(TorrentBroadcast.scala:257)\n\tat org.apache.spark.util.KeyLock.withLock(KeyLock.scala:64)\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$1(TorrentBroadcast.scala:252)\n\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\n\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:94)\n\tat org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:252)\n\tat org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:109)\n\tat org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.nio.file.FileSystemException: /tmp/spark-5cd138a3-4681-40c6-9165-6406769c5be2/executor-094bd768-d4ea-411e-af3e-bc9fa8d3b8d9/blockmgr-378ea358-fea2-473c-85ca-1ba5cabf146b/02: No space left on device\n\tat sun.nio.fs.UnixException.translateToIOException(UnixException.java:91)\n\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n\tat sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)\n\tat java.nio.file.Files.createDirectory(Files.java:674)\n\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:108)\n\tat org.apache.spark.storage.DiskBlockManager.containsBlock(DiskBlockManager.scala:157)\n\tat org.apache.spark.storage.DiskStore.contains(DiskStore.scala:154)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$getCurrentBlockStatus(BlockManager.scala:885)\n\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:2075)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1555)\n\tat org.apache.spark.storage.BlockManager$BlockStoreUpdater.save(BlockManager.scala:380)\n\tat org.apache.spark.storage.BlockManager.putBytes(BlockManager.scala:1468)\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBlocks$1(TorrentBroadcast.scala:215)\n\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.broadcast.TorrentBroadcast.readBlocks(TorrentBroadcast.scala:192)\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$4(TorrentBroadcast.scala:281)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$2(TorrentBroadcast.scala:257)\n\tat org.apache.spark.util.KeyLock.withLock(KeyLock.scala:64)\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$1(TorrentBroadcast.scala:252)\n\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\n\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:94)\n\tat org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:252)\n\tat org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:109)\n\tat org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "# See how it looks\n",
    "print(list(word_pairs_rdd.take(1)[0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9334c2e7-b8e5-49dd-a7fb-bb26b06c4241",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 16:17:21 INFO SparkContext: Starting job: takeOrdered at /tmp/ipykernel_14048/2986523124.py:6\n",
      "24/03/06 16:17:21 INFO DAGScheduler: Registering RDD 33 (reduceByKey at /tmp/ipykernel_14048/2986523124.py:3) as input to shuffle 3\n",
      "24/03/06 16:17:21 INFO DAGScheduler: Got job 13 (takeOrdered at /tmp/ipykernel_14048/2986523124.py:6) with 5 output partitions\n",
      "24/03/06 16:17:21 INFO DAGScheduler: Final stage: ResultStage 20 (takeOrdered at /tmp/ipykernel_14048/2986523124.py:6)\n",
      "24/03/06 16:17:21 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 19)\n",
      "24/03/06 16:17:21 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 19)\n",
      "24/03/06 16:17:21 INFO DAGScheduler: Submitting ShuffleMapStage 19 (PairwiseRDD[33] at reduceByKey at /tmp/ipykernel_14048/2986523124.py:3), which has no missing parents\n",
      "24/03/06 16:17:21 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 17.0 KiB, free 433.9 MiB)\n",
      "24/03/06 16:17:21 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 8.8 KiB, free 433.9 MiB)\n",
      "24/03/06 16:17:21 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on host-192-168-2-147-de1:10005 (size: 8.8 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:17:21 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 16:17:21 INFO DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 19 (PairwiseRDD[33] at reduceByKey at /tmp/ipykernel_14048/2986523124.py:3) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\n",
      "24/03/06 16:17:21 INFO TaskSchedulerImpl: Adding task set 19.0 with 5 tasks resource profile 0\n",
      "24/03/06 16:17:21 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 192.168.2.250:10005 in memory (size: 7.8 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:17:21 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 35) (192.168.2.250, executor 1, partition 0, NODE_LOCAL, 7426 bytes) \n",
      "24/03/06 16:17:21 INFO BlockManagerInfo: Removed broadcast_17_piece0 on host-192-168-2-147-de1:10005 in memory (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:17:21 INFO TaskSetManager: Starting task 1.0 in stage 19.0 (TID 36) (192.168.2.250, executor 0, partition 1, NODE_LOCAL, 7426 bytes) \n",
      "24/03/06 16:17:21 INFO TaskSetManager: Starting task 2.0 in stage 19.0 (TID 37) (192.168.2.250, executor 1, partition 2, NODE_LOCAL, 7426 bytes) \n",
      "24/03/06 16:17:21 INFO TaskSetManager: Starting task 3.0 in stage 19.0 (TID 38) (192.168.2.250, executor 0, partition 3, NODE_LOCAL, 7426 bytes) \n",
      "24/03/06 16:17:21 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 192.168.2.250:10005 (size: 8.8 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:17:21 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 192.168.2.250:10006 (size: 8.8 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:17:48 INFO TaskSetManager: Starting task 4.0 in stage 19.0 (TID 39) (192.168.2.250, executor 0, partition 4, NODE_LOCAL, 7426 bytes) \n",
      "24/03/06 16:17:48 INFO TaskSetManager: Finished task 3.0 in stage 19.0 (TID 38) in 27010 ms on 192.168.2.250 (executor 0) (1/5)\n",
      "24/03/06 16:17:48 INFO TaskSetManager: Finished task 1.0 in stage 19.0 (TID 36) in 27025 ms on 192.168.2.250 (executor 0) (2/5)\n",
      "24/03/06 16:17:49 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 35) in 27186 ms on 192.168.2.250 (executor 1) (3/5)\n",
      "24/03/06 16:17:49 INFO TaskSetManager: Finished task 2.0 in stage 19.0 (TID 37) in 27190 ms on 192.168.2.250 (executor 1) (4/5)\n",
      "[Stage 19:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avslutad. - closed.: 2534\n",
      "är - is: 2380\n",
      "jag - the: 1324\n",
      "debatten - is: 1324\n",
      "förklarar - debate: 1317\n",
      "debatten - the: 1225\n",
      "härmed - is: 1215\n",
      "är - debate: 1187\n",
      "(artikel - (rule: 893\n",
      "det - that: 852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 16:18:13 INFO TaskSetManager: Finished task 4.0 in stage 19.0 (TID 39) in 24094 ms on 192.168.2.250 (executor 0) (5/5)\n",
      "24/03/06 16:18:13 INFO DAGScheduler: ShuffleMapStage 19 (reduceByKey at /tmp/ipykernel_14048/2986523124.py:3) finished in 51.122 s\n",
      "24/03/06 16:18:13 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/03/06 16:18:13 INFO DAGScheduler: running: Set()\n",
      "24/03/06 16:18:13 INFO DAGScheduler: waiting: Set(ResultStage 20)\n",
      "24/03/06 16:18:13 INFO DAGScheduler: failed: Set()\n",
      "24/03/06 16:18:13 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool \n",
      "24/03/06 16:18:13 INFO DAGScheduler: Submitting ResultStage 20 (PythonRDD[36] at takeOrdered at /tmp/ipykernel_14048/2986523124.py:6), which has no missing parents\n",
      "24/03/06 16:18:13 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 11.2 KiB, free 433.9 MiB)\n",
      "24/03/06 16:18:13 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 433.9 MiB)\n",
      "24/03/06 16:18:13 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on host-192-168-2-147-de1:10005 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:18:13 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 16:18:13 INFO DAGScheduler: Submitting 5 missing tasks from ResultStage 20 (PythonRDD[36] at takeOrdered at /tmp/ipykernel_14048/2986523124.py:6) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\n",
      "24/03/06 16:18:13 INFO TaskSchedulerImpl: Adding task set 20.0 with 5 tasks resource profile 0\n",
      "24/03/06 16:18:13 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 40) (192.168.2.250, executor 1, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/03/06 16:18:13 INFO TaskSetManager: Starting task 1.0 in stage 20.0 (TID 41) (192.168.2.250, executor 0, partition 1, NODE_LOCAL, 7437 bytes) \n",
      "24/03/06 16:18:13 INFO TaskSetManager: Starting task 2.0 in stage 20.0 (TID 42) (192.168.2.250, executor 1, partition 2, NODE_LOCAL, 7437 bytes) \n",
      "24/03/06 16:18:13 INFO TaskSetManager: Starting task 3.0 in stage 20.0 (TID 43) (192.168.2.250, executor 0, partition 3, NODE_LOCAL, 7437 bytes) \n",
      "24/03/06 16:18:13 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 192.168.2.250:10006 (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:18:13 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 192.168.2.250:10005 (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:18:13 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 192.168.2.250:35472\n",
      "24/03/06 16:18:13 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 192.168.2.250:35464\n",
      "24/03/06 16:18:13 INFO TaskSetManager: Starting task 4.0 in stage 20.0 (TID 44) (192.168.2.250, executor 1, partition 4, NODE_LOCAL, 7437 bytes) \n",
      "24/03/06 16:18:13 INFO TaskSetManager: Finished task 2.0 in stage 20.0 (TID 42) in 53 ms on 192.168.2.250 (executor 1) (1/5)\n",
      "24/03/06 16:18:13 INFO TaskSetManager: Finished task 1.0 in stage 20.0 (TID 41) in 56 ms on 192.168.2.250 (executor 0) (2/5)\n",
      "24/03/06 16:18:13 INFO TaskSetManager: Finished task 3.0 in stage 20.0 (TID 43) in 57 ms on 192.168.2.250 (executor 0) (3/5)\n",
      "24/03/06 16:18:13 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 40) in 66 ms on 192.168.2.250 (executor 1) (4/5)\n",
      "24/03/06 16:18:13 INFO TaskSetManager: Finished task 4.0 in stage 20.0 (TID 44) in 30 ms on 192.168.2.250 (executor 1) (5/5)\n",
      "24/03/06 16:18:13 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool \n",
      "24/03/06 16:18:13 INFO DAGScheduler: ResultStage 20 (takeOrdered at /tmp/ipykernel_14048/2986523124.py:6) finished in 0.101 s\n",
      "24/03/06 16:18:13 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/06 16:18:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished\n",
      "24/03/06 16:18:13 INFO DAGScheduler: Job 13 finished: takeOrdered at /tmp/ipykernel_14048/2986523124.py:6, took 51.239821 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count the frequency of each word-translation pair\n",
    "words_rdd = word_pairs_rdd.flatMap(lambda line: line)\n",
    "word_counts_rdd = words_rdd.map(lambda word: (word, 1))\n",
    "word_freq_rdd = word_counts_rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Print 10 of the most frequently occurring pairs of words\n",
    "top_pairs = word_freq_rdd.takeOrdered(10, key=lambda x: -x[1])\n",
    "for (word, translation), count in top_pairs:\n",
    "    print(f'{word} - {translation}: {count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ca90554-9314-4363-a079-901028a989f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/26 20:51:08 INFO SparkContext: Starting job: takeOrdered at /tmp/ipykernel_31804/2456824845.py:2\n",
      "24/02/26 20:51:08 INFO DAGScheduler: Registering RDD 28 (join at /tmp/ipykernel_31804/4098632872.py:6) as input to shuffle 3\n",
      "24/02/26 20:51:08 INFO DAGScheduler: Registering RDD 32 (reduceByKey at /tmp/ipykernel_31804/3108678048.py:2) as input to shuffle 2\n",
      "24/02/26 20:51:08 INFO DAGScheduler: Got job 12 (takeOrdered at /tmp/ipykernel_31804/2456824845.py:2) with 5 output partitions\n",
      "24/02/26 20:51:08 INFO DAGScheduler: Final stage: ResultStage 16 (takeOrdered at /tmp/ipykernel_31804/2456824845.py:2)\n",
      "24/02/26 20:51:08 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 15)\n",
      "24/02/26 20:51:08 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 15)\n",
      "24/02/26 20:51:08 INFO DAGScheduler: Submitting ShuffleMapStage 14 (PairwiseRDD[28] at join at /tmp/ipykernel_31804/4098632872.py:6), which has no missing parents\n",
      "24/02/26 20:51:08 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 19.3 KiB, free 433.9 MiB)\n",
      "24/02/26 20:51:08 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 8.8 KiB, free 433.9 MiB)\n",
      "24/02/26 20:51:08 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on host-192-168-2-136-de1:10005 (size: 8.8 KiB, free: 434.3 MiB)\n",
      "24/02/26 20:51:08 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/26 20:51:08 INFO DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 14 (PairwiseRDD[28] at join at /tmp/ipykernel_31804/4098632872.py:6) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\n",
      "24/02/26 20:51:08 INFO TaskSchedulerImpl: Adding task set 14.0 with 5 tasks resource profile 0\n",
      "24/02/26 20:51:08 INFO TaskSetManager: Starting task 4.0 in stage 14.0 (TID 29) (192.168.2.250, executor 1, partition 4, NODE_LOCAL, 7788 bytes) \n",
      "24/02/26 20:51:08 INFO BlockManagerInfo: Removed broadcast_15_piece0 on host-192-168-2-136-de1:10005 in memory (size: 5.1 KiB, free: 434.3 MiB)\n",
      "24/02/26 20:51:08 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 192.168.2.250:10005 in memory (size: 5.1 KiB, free: 434.3 MiB)\n",
      "24/02/26 20:51:08 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 192.168.2.250:10005 (size: 8.8 KiB, free: 434.3 MiB)\n",
      "24/02/26 20:51:12 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 30) (192.168.2.252, executor 0, partition 0, ANY, 7788 bytes) \n",
      "24/02/26 20:51:12 INFO TaskSetManager: Starting task 1.0 in stage 14.0 (TID 31) (192.168.2.250, executor 1, partition 1, ANY, 7788 bytes) \n",
      "24/02/26 20:51:12 INFO TaskSetManager: Starting task 2.0 in stage 14.0 (TID 32) (192.168.2.252, executor 0, partition 2, ANY, 7788 bytes) \n",
      "24/02/26 20:51:12 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 192.168.2.252:10005 (size: 8.8 KiB, free: 434.3 MiB)\n",
      "24/02/26 20:51:12 INFO TaskSetManager: Starting task 3.0 in stage 14.0 (TID 33) (192.168.2.250, executor 1, partition 3, ANY, 7788 bytes) \n",
      "24/02/26 20:51:12 WARN TaskSetManager: Lost task 4.0 in stage 14.0 (TID 29) (192.168.2.250 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n",
      "    merger.mergeValues(iterator)\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n",
      "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
      "TypeError: unhashable type: 'list'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "24/02/26 20:51:13 INFO TaskSetManager: Starting task 4.1 in stage 14.0 (TID 34) (192.168.2.252, executor 0, partition 4, ANY, 7788 bytes) \n",
      "24/02/26 20:51:13 WARN TaskSetManager: Lost task 2.0 in stage 14.0 (TID 32) (192.168.2.252 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n",
      "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
      "TypeError: unhashable type: 'list'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "24/02/26 20:51:16 INFO TaskSetManager: Starting task 2.1 in stage 14.0 (TID 35) (192.168.2.250, executor 1, partition 2, ANY, 7788 bytes) \n",
      "24/02/26 20:51:16 INFO TaskSetManager: Lost task 1.0 in stage 14.0 (TID 31) on 192.168.2.250, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n",
      "    merger.mergeValues(iterator)\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n",
      "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
      "TypeError: unhashable type: 'list'\n",
      ") [duplicate 1]\n",
      "24/02/26 20:51:16 INFO TaskSetManager: Lost task 0.0 in stage 14.0 (TID 30) on 192.168.2.252, executor 0: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n",
      "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
      "TypeError: unhashable type: 'list'\n",
      ") [duplicate 1]\n",
      "24/02/26 20:51:16 INFO TaskSetManager: Starting task 0.1 in stage 14.0 (TID 36) (192.168.2.252, executor 0, partition 0, ANY, 7788 bytes) \n",
      "24/02/26 20:51:16 INFO TaskSetManager: Starting task 1.1 in stage 14.0 (TID 37) (192.168.2.250, executor 1, partition 1, ANY, 7788 bytes) \n",
      "24/02/26 20:51:16 INFO TaskSetManager: Lost task 3.0 in stage 14.0 (TID 33) on 192.168.2.250, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n",
      "    merger.mergeValues(iterator)\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n",
      "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
      "TypeError: unhashable type: 'list'\n",
      ") [duplicate 2]\n",
      "24/02/26 20:51:17 INFO TaskSetManager: Starting task 3.1 in stage 14.0 (TID 38) (192.168.2.250, executor 1, partition 3, ANY, 7788 bytes) \n",
      "24/02/26 20:51:17 INFO TaskSetManager: Lost task 2.1 in stage 14.0 (TID 35) on 192.168.2.250, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n",
      "    merger.mergeValues(iterator)\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n",
      "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
      "TypeError: unhashable type: 'list'\n",
      ") [duplicate 3]\n",
      "24/02/26 20:51:17 INFO TaskSetManager: Starting task 2.2 in stage 14.0 (TID 39) (192.168.2.252, executor 0, partition 2, ANY, 7788 bytes) \n",
      "24/02/26 20:51:17 INFO TaskSetManager: Lost task 4.1 in stage 14.0 (TID 34) on 192.168.2.252, executor 0: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n",
      "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
      "TypeError: unhashable type: 'list'\n",
      ") [duplicate 2]\n",
      "24/02/26 20:51:18 INFO TaskSetManager: Starting task 4.2 in stage 14.0 (TID 40) (192.168.2.252, executor 0, partition 4, ANY, 7788 bytes) \n",
      "24/02/26 20:51:18 INFO TaskSetManager: Lost task 2.2 in stage 14.0 (TID 39) on 192.168.2.252, executor 0: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n",
      "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
      "TypeError: unhashable type: 'list'\n",
      ") [duplicate 3]\n",
      "24/02/26 20:51:20 INFO TaskSetManager: Starting task 2.3 in stage 14.0 (TID 41) (192.168.2.252, executor 0, partition 2, ANY, 7788 bytes) \n",
      "24/02/26 20:51:20 INFO TaskSetManager: Lost task 0.1 in stage 14.0 (TID 36) on 192.168.2.252, executor 0: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n",
      "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
      "TypeError: unhashable type: 'list'\n",
      ") [duplicate 4]\n",
      "24/02/26 20:51:21 INFO TaskSetManager: Starting task 0.2 in stage 14.0 (TID 42) (192.168.2.250, executor 1, partition 0, ANY, 7788 bytes) \n",
      "24/02/26 20:51:21 INFO TaskSetManager: Lost task 1.1 in stage 14.0 (TID 37) on 192.168.2.250, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n",
      "    merger.mergeValues(iterator)\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n",
      "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
      "TypeError: unhashable type: 'list'\n",
      ") [duplicate 4]\n",
      "24/02/26 20:51:21 INFO TaskSetManager: Starting task 1.2 in stage 14.0 (TID 43) (192.168.2.250, executor 1, partition 1, ANY, 7788 bytes) \n",
      "24/02/26 20:51:21 INFO TaskSetManager: Lost task 3.1 in stage 14.0 (TID 38) on 192.168.2.250, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n",
      "    merger.mergeValues(iterator)\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n",
      "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
      "TypeError: unhashable type: 'list'\n",
      ") [duplicate 5]\n",
      "24/02/26 20:51:21 INFO TaskSetManager: Starting task 3.2 in stage 14.0 (TID 44) (192.168.2.252, executor 0, partition 3, ANY, 7788 bytes) \n",
      "24/02/26 20:51:21 INFO TaskSetManager: Lost task 2.3 in stage 14.0 (TID 41) on 192.168.2.252, executor 0: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n",
      "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
      "TypeError: unhashable type: 'list'\n",
      ") [duplicate 5]\n",
      "24/02/26 20:51:21 ERROR TaskSetManager: Task 2 in stage 14.0 failed 4 times; aborting job\n",
      "24/02/26 20:51:21 INFO TaskSchedulerImpl: Cancelling stage 14\n",
      "24/02/26 20:51:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage cancelled: Job aborted due to stage failure: Task 2 in stage 14.0 failed 4 times, most recent failure: Lost task 2.3 in stage 14.0 (TID 41) (192.168.2.252 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n",
      "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
      "TypeError: unhashable type: 'list'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:\n",
      "24/02/26 20:51:21 INFO TaskSchedulerImpl: Stage 14 was cancelled\n",
      "24/02/26 20:51:21 INFO DAGScheduler: ShuffleMapStage 14 (join at /tmp/ipykernel_31804/4098632872.py:6) failed in 12.889 s due to Job aborted due to stage failure: Task 2 in stage 14.0 failed 4 times, most recent failure: Lost task 2.3 in stage 14.0 (TID 41) (192.168.2.252 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n",
      "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
      "TypeError: unhashable type: 'list'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:\n",
      "24/02/26 20:51:21 INFO DAGScheduler: Job 12 failed: takeOrdered at /tmp/ipykernel_31804/2456824845.py:2, took 12.908740 s\n",
      "24/02/26 20:51:21 WARN TaskSetManager: Lost task 4.2 in stage 14.0 (TID 40) (192.168.2.252 executor 0): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 14.0 failed 4 times, most recent failure: Lost task 2.3 in stage 14.0 (TID 41) (192.168.2.252 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n",
      "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
      "TypeError: unhashable type: 'list'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/02/26 20:51:21 WARN TaskSetManager: Lost task 3.2 in stage 14.0 (TID 44) (192.168.2.252 executor 0): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 14.0 failed 4 times, most recent failure: Lost task 2.3 in stage 14.0 (TID 41) (192.168.2.252 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n",
      "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
      "TypeError: unhashable type: 'list'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/02/26 20:51:21 WARN TaskSetManager: Lost task 0.2 in stage 14.0 (TID 42) (192.168.2.250 executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 14.0 failed 4 times, most recent failure: Lost task 2.3 in stage 14.0 (TID 41) (192.168.2.252 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n",
      "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
      "TypeError: unhashable type: 'list'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/02/26 20:51:21 WARN TaskSetManager: Lost task 1.2 in stage 14.0 (TID 43) (192.168.2.250 executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 14.0 failed 4 times, most recent failure: Lost task 2.3 in stage 14.0 (TID 41) (192.168.2.252 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n",
      "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
      "TypeError: unhashable type: 'list'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/02/26 20:51:21 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool \n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 14.0 failed 4 times, most recent failure: Lost task 2.3 in stage 14.0 (TID 41) (192.168.2.252 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\nTypeError: unhashable type: 'list'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\nTypeError: unhashable type: 'list'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Step 9: Print some of the most frequently occurring pairs of words\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m top_pairs \u001b[38;5;241m=\u001b[39m \u001b[43mword_translation_counts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtakeOrdered\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (word, translation), count \u001b[38;5;129;01min\u001b[39;00m top_pairs:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtranslation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/rdd.py:2778\u001b[0m, in \u001b[0;36mRDD.takeOrdered\u001b[0;34m(self, num, key)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(a: List[T], b: List[T]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[T]:\n\u001b[1;32m   2776\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m heapq\u001b[38;5;241m.\u001b[39mnsmallest(num, a \u001b[38;5;241m+\u001b[39m b, key)\n\u001b[0;32m-> 2778\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mheapq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnsmallest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerge\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/rdd.py:1924\u001b[0m, in \u001b[0;36mRDD.reduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m reduce(f, iterator, initial)\n\u001b[0;32m-> 1924\u001b[0m vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vals:\n\u001b[1;32m   1926\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reduce(f, vals)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 14.0 failed 4 times, most recent failure: Lost task 2.3 in stage 14.0 (TID 41) (192.168.2.252 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\nTypeError: unhashable type: 'list'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4175, in combine\n  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\nTypeError: unhashable type: 'list'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def count_word_frequency(lines):\n",
    "    #need to add all words in a single array:\n",
    "    words = lines.flatMap(lambda line: line)   \n",
    "    word_counts_rdd = words.map(lambda word: (word, 1)) #creating rdd for word counts\n",
    "\n",
    "    word_freq = word_counts_rdd.reduceByKey(lambda oldcount, increment:  oldcount + increment)\n",
    "    return word_freq\n",
    "    \n",
    "\n",
    "def sort_word_frequency_list(word_freq_rdd):\n",
    "    # Sort the word counts in descending order\n",
    "    sorted_word_frequency = word_freq_rdd.sortBy(lambda x: x[1], ascending=False)\n",
    "    return sorted_word_frequency\n",
    "\n",
    "orted_words = sort_word_frequency_list(count_word_frequency(word_pairs_rdd))\n",
    "\n",
    "# Step 8: Use reduceByKey to count occurrences of word-translation pairs\n",
    "word_translation_counts = word_pairs_rdd.flatMap(lambda x: x).map(lambda x: ((x[0], x[1]), 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Step 9: Print some of the most frequently occurring pairs of words\n",
    "top_pairs = word_translation_counts.takeOrdered(10, key=lambda x: -x[1])\n",
    "for (word, translation), count in top_pairs:\n",
    "    print(f'{word} - {translation}: {count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65eda738-e27a-469f-a4fd-900ec739f5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/21 13:10:22 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
      "24/02/21 13:10:22 INFO SparkUI: Stopped Spark web UI at http://host-192-168-2-13-de1:4040\n",
      "24/02/21 13:10:22 INFO StandaloneSchedulerBackend: Shutting down all executors\n",
      "24/02/21 13:10:22 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down\n",
      "24/02/21 13:10:22 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "24/02/21 13:10:22 INFO MemoryStore: MemoryStore cleared\n",
      "24/02/21 13:10:22 INFO BlockManager: BlockManager stopped\n",
      "24/02/21 13:10:22 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "24/02/21 13:10:22 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "24/02/21 13:10:22 INFO SparkContext: Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "# release the cores for another application!\n",
    "# spark_context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6167cac4-7534-4a8b-a217-f06a4a2b5df5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
