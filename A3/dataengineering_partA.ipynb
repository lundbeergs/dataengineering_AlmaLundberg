{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcb40689-fcac-4014-9f17-8111fd417fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/06 17:12:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/06 17:12:53 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.2.250:51014) with ID 1,  ResourceProfileId 0\n",
      "24/03/06 17:12:53 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.2.250:51008) with ID 0,  ResourceProfileId 0\n",
      "24/03/06 17:12:54 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.250:10005 with 366.3 MiB RAM, BlockManagerId(1, 192.168.2.250, 10005, None)\n",
      "24/03/06 17:12:54 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.250:10006 with 366.3 MiB RAM, BlockManagerId(0, 192.168.2.250, 10006, None)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://192.168.2.250:7077\") \\\n",
    "        .appName(\"AlmaLundbergSparkapplicationA3\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", False)\\\n",
    "        .config(\"spark.cores.max\", 4)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\",2)\\\n",
    "        .config(\"spark.driver.port\",9999)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# RDD API\n",
    "spark_context = spark_session.sparkContext\n",
    "\n",
    "spark_context.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffafb5a6-41c1-490f-a5ac-6bdcc88d364f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 16:40:50 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 221.5 KiB, free 434.2 MiB)\n",
      "24/03/06 16:40:50 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 434.2 MiB)\n",
      "24/03/06 16:40:50 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on host-192-168-2-147-de1:10005 (size: 32.6 KiB, free: 434.4 MiB)\n",
      "24/03/06 16:40:50 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
      "24/03/06 16:40:51 INFO FileInputFormat: Total input files to process : 1\n",
      "24/03/06 16:40:51 INFO NetworkTopology: Adding a new node: /default-rack/192.168.2.250:9866\n",
      "24/03/06 16:40:51 INFO SparkContext: Starting job: count at /tmp/ipykernel_14848/3631255496.py:4\n",
      "24/03/06 16:40:51 INFO DAGScheduler: Got job 0 (count at /tmp/ipykernel_14848/3631255496.py:4) with 2 output partitions\n",
      "24/03/06 16:40:51 INFO DAGScheduler: Final stage: ResultStage 0 (count at /tmp/ipykernel_14848/3631255496.py:4)\n",
      "24/03/06 16:40:51 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/06 16:40:51 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/06 16:40:51 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at count at /tmp/ipykernel_14848/3631255496.py:4), which has no missing parents\n",
      "24/03/06 16:40:51 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.1 KiB, free 434.1 MiB)\n",
      "24/03/06 16:40:51 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.1 MiB)\n",
      "24/03/06 16:40:51 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on host-192-168-2-147-de1:10005 (size: 5.5 KiB, free: 434.4 MiB)\n",
      "24/03/06 16:40:51 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 16:40:51 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[2] at count at /tmp/ipykernel_14848/3631255496.py:4) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/06 16:40:51 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n",
      "24/03/06 16:40:51 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 0) (192.168.2.250, executor 0, partition 1, NODE_LOCAL, 7690 bytes) \n",
      "24/03/06 16:40:52 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.250:10005 (size: 5.5 KiB, free: 366.3 MiB)\n",
      "24/03/06 16:40:52 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.250:10005 (size: 32.6 KiB, free: 366.3 MiB)\n",
      "24/03/06 16:40:55 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 1) (192.168.2.250, executor 1, partition 0, ANY, 7690 bytes) \n",
      "24/03/06 16:40:55 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.250:10006 (size: 5.5 KiB, free: 366.3 MiB)\n",
      "24/03/06 16:40:55 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 0) in 3825 ms on 192.168.2.250 (executor 0) (1/2)\n",
      "24/03/06 16:40:55 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 43797\n",
      "24/03/06 16:40:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.250:10006 (size: 32.6 KiB, free: 366.3 MiB)\n",
      "24/03/06 16:40:58 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 1) in 3507 ms on 192.168.2.250 (executor 1) (2/2)\n",
      "24/03/06 16:40:58 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "24/03/06 16:40:58 INFO DAGScheduler: ResultStage 0 (count at /tmp/ipykernel_14848/3631255496.py:4) finished in 6.708 s\n",
      "24/03/06 16:40:58 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/06 16:40:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "24/03/06 16:40:58 INFO DAGScheduler: Job 0 finished: count at /tmp/ipykernel_14848/3631255496.py:4, took 6.780160 s\n",
      "24/03/06 16:40:58 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 221.5 KiB, free 433.9 MiB)\n",
      "24/03/06 16:40:58 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 433.9 MiB)\n",
      "24/03/06 16:40:58 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on host-192-168-2-147-de1:10005 (size: 32.6 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:40:58 INFO SparkContext: Created broadcast 2 from textFile at NativeMethodAccessorImpl.java:0\n",
      "24/03/06 16:40:58 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.2.250:10006 in memory (size: 5.5 KiB, free: 366.3 MiB)\n",
      "24/03/06 16:40:58 INFO BlockManagerInfo: Removed broadcast_1_piece0 on host-192-168-2-147-de1:10005 in memory (size: 5.5 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:40:58 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.2.250:10005 in memory (size: 5.5 KiB, free: 366.3 MiB)\n",
      "24/03/06 16:40:58 INFO FileInputFormat: Total input files to process : 1\n",
      "24/03/06 16:40:58 INFO SparkContext: Starting job: count at /tmp/ipykernel_14848/3631255496.py:8\n",
      "24/03/06 16:40:58 INFO DAGScheduler: Got job 1 (count at /tmp/ipykernel_14848/3631255496.py:8) with 3 output partitions\n",
      "24/03/06 16:40:58 INFO DAGScheduler: Final stage: ResultStage 1 (count at /tmp/ipykernel_14848/3631255496.py:8)\n",
      "24/03/06 16:40:58 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/06 16:40:58 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/06 16:40:58 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[5] at count at /tmp/ipykernel_14848/3631255496.py:8), which has no missing parents\n",
      "24/03/06 16:40:58 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 9.1 KiB, free 433.9 MiB)\n",
      "24/03/06 16:40:58 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 433.9 MiB)\n",
      "24/03/06 16:40:58 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on host-192-168-2-147-de1:10005 (size: 5.5 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:40:58 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 16:40:58 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 1 (PythonRDD[5] at count at /tmp/ipykernel_14848/3631255496.py:8) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/03/06 16:40:58 INFO TaskSchedulerImpl: Adding task set 1.0 with 3 tasks resource profile 0\n",
      "24/03/06 16:40:58 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2) (192.168.2.250, executor 1, partition 0, ANY, 7690 bytes) \n",
      "24/03/06 16:40:58 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3) (192.168.2.250, executor 0, partition 1, ANY, 7690 bytes) \n",
      "24/03/06 16:40:58 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 4) (192.168.2.250, executor 1, partition 2, ANY, 7690 bytes) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines in English file: 1862234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 16:40:58 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.250:10005 (size: 5.5 KiB, free: 366.3 MiB)\n",
      "24/03/06 16:40:58 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.250:10006 (size: 5.5 KiB, free: 366.3 MiB)\n",
      "24/03/06 16:40:58 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.250:10005 (size: 32.6 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:40:58 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.250:10006 (size: 32.6 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:40:59 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 4) in 327 ms on 192.168.2.250 (executor 1) (1/3)\n",
      "[Stage 1:===================>                                       (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines in Swedish file: 1862234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 16:41:00 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 1466 ms on 192.168.2.250 (executor 0) (2/3)\n",
      "24/03/06 16:41:00 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 1614 ms on 192.168.2.250 (executor 1) (3/3)\n",
      "24/03/06 16:41:00 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "24/03/06 16:41:00 INFO DAGScheduler: ResultStage 1 (count at /tmp/ipykernel_14848/3631255496.py:8) finished in 1.647 s\n",
      "24/03/06 16:41:00 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/06 16:41:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "24/03/06 16:41:00 INFO DAGScheduler: Job 1 finished: count at /tmp/ipykernel_14848/3631255496.py:8, took 1.655664 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Using map and reduce from the Spark API, and loading the text file from HDFS.\n",
    "\n",
    "lines_en = spark_context.textFile(\"hdfs://192.168.2.250:9000/europarl/europarl-v7.sv-en.en\")\n",
    "num_lines_en = lines_en.count()\n",
    "print(f\"Total lines in English file: {num_lines_en}\") \n",
    "\n",
    "lines_sv = spark_context.textFile(\"hdfs://192.168.2.250:9000/europarl/europarl-v7.sv-en.sv\")\n",
    "num_lines_sv = lines_sv.count()\n",
    "print(f\"Total lines in Swedish file: {num_lines_sv}\")\n",
    "\n",
    "\n",
    "# print(lines.first())\n",
    "# words = lines.map(lambda line: line.split(' '))\n",
    "# word_counts = words.map(lambda w: len(w))\n",
    "# total_words = word_counts.reduce(add)\n",
    "# print(f'total words= {total_words}') \n",
    "\n",
    "#lines = spark_context.textFile(\"/home/ubuntu/i_have_a_dream.txt\")\n",
    "# print(lines.first())\n",
    "# words = lines.map(lambda line: line.split(' '))\n",
    "# word_counts = words.map(lambda w: len(w))\n",
    "# total_words = word_counts.reduce(add)\n",
    "# print(f'total words= {total_words}')  \n",
    "# ... the same number of words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0cbef32-5bfa-4190-8bc1-1c4b972a00ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total lines in English file: 1862234\n",
    "# Total lines in Swedish file: 1862234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d574749-5a3d-4e1c-b3e9-efef1a078fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions in English file: 2\n",
      "Number of partitions in Swedish file: 3\n"
     ]
    }
   ],
   "source": [
    "# Counting nmr of partitions in the English file\n",
    "num_partitions_en = lines_en.getNumPartitions()\n",
    "print(f\"Number of partitions in English file: {num_partitions_en}\")\n",
    "\n",
    "# Counting nmr of partitions in the Swedish file\n",
    "num_partitions_sv = lines_sv.getNumPartitions()\n",
    "print(f\"Number of partitions in Swedish file: {num_partitions_sv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d864eddf-cc03-4058-82c3-5d3f11ad1b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of partitions in English file: 2\n",
    "# Number of partitions in Swedish file: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa87b66a-46a1-49eb-9f87-4666a12f237e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(line):\n",
    "    return line.lower().split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2a51f26-9c3c-466c-a409-a5b5bb5abb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_lines_en = lines_en.map(preprocess_text)\n",
    "processed_lines_sv = lines_sv.map(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cee82ee-1bbb-4521-b47e-c3c4982f9b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 16:41:36 INFO SparkContext: Starting job: count at /tmp/ipykernel_14848/226898882.py:3\n",
      "24/03/06 16:41:36 INFO DAGScheduler: Got job 2 (count at /tmp/ipykernel_14848/226898882.py:3) with 2 output partitions\n",
      "24/03/06 16:41:36 INFO DAGScheduler: Final stage: ResultStage 2 (count at /tmp/ipykernel_14848/226898882.py:3)\n",
      "24/03/06 16:41:36 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/06 16:41:36 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/06 16:41:36 INFO DAGScheduler: Submitting ResultStage 2 (PythonRDD[6] at count at /tmp/ipykernel_14848/226898882.py:3), which has no missing parents\n",
      "24/03/06 16:41:36 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 9.6 KiB, free 433.9 MiB)\n",
      "24/03/06 16:41:36 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 433.9 MiB)\n",
      "24/03/06 16:41:36 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on host-192-168-2-147-de1:10005 (size: 5.7 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:41:36 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 16:41:36 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (PythonRDD[6] at count at /tmp/ipykernel_14848/226898882.py:3) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/06 16:41:36 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks resource profile 0\n",
      "24/03/06 16:41:36 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.2.250:10005 in memory (size: 5.5 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:41:36 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.2.250:10006 in memory (size: 5.5 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:41:36 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5) (192.168.2.250, executor 0, partition 1, NODE_LOCAL, 7690 bytes) \n",
      "24/03/06 16:41:36 INFO BlockManagerInfo: Removed broadcast_3_piece0 on host-192-168-2-147-de1:10005 in memory (size: 5.5 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:41:36 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.250:10005 (size: 5.7 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:41:39 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 2269 ms on 192.168.2.250 (executor 0) (1/2)\n",
      "24/03/06 16:41:40 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 6) (192.168.2.250, executor 1, partition 0, ANY, 7690 bytes) \n",
      "24/03/06 16:41:40 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.250:10006 (size: 5.7 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:41:42 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 6) in 2278 ms on 192.168.2.250 (executor 1) (2/2)\n",
      "24/03/06 16:41:42 INFO DAGScheduler: ResultStage 2 (count at /tmp/ipykernel_14848/226898882.py:3) finished in 5.369 s\n",
      "24/03/06 16:41:42 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/06 16:41:42 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "24/03/06 16:41:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "24/03/06 16:41:42 INFO DAGScheduler: Job 2 finished: count at /tmp/ipykernel_14848/226898882.py:3, took 5.379692 s\n",
      "24/03/06 16:41:42 INFO SparkContext: Starting job: count at /tmp/ipykernel_14848/226898882.py:4\n",
      "24/03/06 16:41:42 INFO DAGScheduler: Got job 3 (count at /tmp/ipykernel_14848/226898882.py:4) with 3 output partitions\n",
      "24/03/06 16:41:42 INFO DAGScheduler: Final stage: ResultStage 3 (count at /tmp/ipykernel_14848/226898882.py:4)\n",
      "24/03/06 16:41:42 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/06 16:41:42 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/06 16:41:42 INFO DAGScheduler: Submitting ResultStage 3 (PythonRDD[7] at count at /tmp/ipykernel_14848/226898882.py:4), which has no missing parents\n",
      "24/03/06 16:41:42 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 9.6 KiB, free 433.9 MiB)\n",
      "24/03/06 16:41:42 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 433.9 MiB)\n",
      "24/03/06 16:41:42 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on host-192-168-2-147-de1:10005 (size: 5.7 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:41:42 INFO BlockManagerInfo: Removed broadcast_4_piece0 on host-192-168-2-147-de1:10005 in memory (size: 5.7 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:41:42 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 16:41:42 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 3 (PythonRDD[7] at count at /tmp/ipykernel_14848/226898882.py:4) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/03/06 16:41:42 INFO TaskSchedulerImpl: Adding task set 3.0 with 3 tasks resource profile 0\n",
      "24/03/06 16:41:42 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.2.250:10006 in memory (size: 5.7 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:41:42 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 7) (192.168.2.250, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/03/06 16:41:42 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 8) (192.168.2.250, executor 1, partition 1, ANY, 7690 bytes) \n",
      "24/03/06 16:41:42 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 9) (192.168.2.250, executor 0, partition 2, ANY, 7690 bytes) \n",
      "24/03/06 16:41:42 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.2.250:10005 in memory (size: 5.7 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:41:42 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.250:10006 (size: 5.7 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:41:42 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.250:10005 (size: 5.7 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:41:42 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 9) in 468 ms on 192.168.2.250 (executor 0) (1/3)\n",
      "[Stage 3:===================>                                       (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed line count in English: 1862234\n",
      "Processed line count in Swedish: 1862234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 16:41:45 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 8) in 3058 ms on 192.168.2.250 (executor 1) (2/3)\n",
      "24/03/06 16:41:45 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 7) in 3113 ms on 192.168.2.250 (executor 0) (3/3)\n",
      "24/03/06 16:41:45 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "24/03/06 16:41:45 INFO DAGScheduler: ResultStage 3 (count at /tmp/ipykernel_14848/226898882.py:4) finished in 3.133 s\n",
      "24/03/06 16:41:45 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/06 16:41:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "24/03/06 16:41:45 INFO DAGScheduler: Job 3 finished: count at /tmp/ipykernel_14848/226898882.py:4, took 3.137989 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Verify that the line counts still match after the pre-processing:\n",
    "\n",
    "processed_count_en = processed_lines_en.count()\n",
    "processed_count_sv = processed_lines_sv.count()\n",
    "print(f\"Processed line count in English: {processed_count_en}\")\n",
    "print(f\"Processed line count in Swedish: {processed_count_sv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "35e03978-73f7-4c4e-916b-27748e5eab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processed line count in English: 1862234\n",
    "# Processed line count in Swedish: 1862234\n",
    "\n",
    "# The line count still match after pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8790a495-45aa-4bee-9422-a09f1de9a51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 16:41:48 INFO SparkContext: Starting job: takeOrdered at /tmp/ipykernel_14848/2132416953.py:9\n",
      "24/03/06 16:41:48 INFO DAGScheduler: Registering RDD 9 (reduceByKey at /tmp/ipykernel_14848/2132416953.py:6) as input to shuffle 0\n",
      "24/03/06 16:41:48 INFO DAGScheduler: Got job 4 (takeOrdered at /tmp/ipykernel_14848/2132416953.py:9) with 2 output partitions\n",
      "24/03/06 16:41:48 INFO DAGScheduler: Final stage: ResultStage 5 (takeOrdered at /tmp/ipykernel_14848/2132416953.py:9)\n",
      "24/03/06 16:41:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\n",
      "24/03/06 16:41:48 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 4)\n",
      "24/03/06 16:41:48 INFO DAGScheduler: Submitting ShuffleMapStage 4 (PairwiseRDD[9] at reduceByKey at /tmp/ipykernel_14848/2132416953.py:6), which has no missing parents\n",
      "24/03/06 16:41:48 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 12.9 KiB, free 433.9 MiB)\n",
      "24/03/06 16:41:48 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 433.9 MiB)\n",
      "24/03/06 16:41:48 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on host-192-168-2-147-de1:10005 (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:41:48 INFO BlockManagerInfo: Removed broadcast_5_piece0 on host-192-168-2-147-de1:10005 in memory (size: 5.7 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:41:48 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.2.250:10005 in memory (size: 5.7 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:41:48 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.2.250:10006 in memory (size: 5.7 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:41:48 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 16:41:48 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 4 (PairwiseRDD[9] at reduceByKey at /tmp/ipykernel_14848/2132416953.py:6) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/06 16:41:48 INFO TaskSchedulerImpl: Adding task set 4.0 with 2 tasks resource profile 0\n",
      "24/03/06 16:41:48 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 10) (192.168.2.250, executor 1, partition 1, NODE_LOCAL, 7679 bytes) \n",
      "24/03/06 16:41:48 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.2.250:10006 (size: 7.8 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:41:52 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 11) (192.168.2.250, executor 1, partition 0, ANY, 7679 bytes) \n",
      "24/03/06 16:42:03 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 10) in 14588 ms on 192.168.2.250 (executor 1) (1/2)\n",
      "24/03/06 16:42:06 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 11) in 14088 ms on 192.168.2.250 (executor 1) (2/2)\n",
      "24/03/06 16:42:06 INFO DAGScheduler: ShuffleMapStage 4 (reduceByKey at /tmp/ipykernel_14848/2132416953.py:6) finished in 17.608 s\n",
      "24/03/06 16:42:06 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/03/06 16:42:06 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "24/03/06 16:42:06 INFO DAGScheduler: running: Set()\n",
      "24/03/06 16:42:06 INFO DAGScheduler: waiting: Set(ResultStage 5)\n",
      "24/03/06 16:42:06 INFO DAGScheduler: failed: Set()\n",
      "24/03/06 16:42:06 INFO DAGScheduler: Submitting ResultStage 5 (PythonRDD[16] at takeOrdered at /tmp/ipykernel_14848/2132416953.py:9), which has no missing parents\n",
      "24/03/06 16:42:06 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 11.1 KiB, free 433.9 MiB)\n",
      "24/03/06 16:42:06 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 433.9 MiB)\n",
      "24/03/06 16:42:06 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on host-192-168-2-147-de1:10005 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:42:06 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 16:42:06 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 5 (PythonRDD[16] at takeOrdered at /tmp/ipykernel_14848/2132416953.py:9) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/06 16:42:06 INFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks resource profile 0\n",
      "24/03/06 16:42:06 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 12) (192.168.2.250, executor 1, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/03/06 16:42:06 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 13) (192.168.2.250, executor 0, partition 1, NODE_LOCAL, 7437 bytes) \n",
      "24/03/06 16:42:06 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.2.250:10005 (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:42:06 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.2.250:10006 (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:42:06 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.2.250:36934\n",
      "24/03/06 16:42:06 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.2.250:36950\n",
      "24/03/06 16:42:06 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.2.250:10006 in memory (size: 7.8 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:42:06 INFO BlockManagerInfo: Removed broadcast_6_piece0 on host-192-168-2-147-de1:10005 in memory (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:42:06 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 12) in 418 ms on 192.168.2.250 (executor 1) (1/2)\n",
      "24/03/06 16:42:06 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 13) in 680 ms on 192.168.2.250 (executor 0) (2/2)\n",
      "24/03/06 16:42:06 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "24/03/06 16:42:06 INFO DAGScheduler: ResultStage 5 (takeOrdered at /tmp/ipykernel_14848/2132416953.py:9) finished in 0.722 s\n",
      "24/03/06 16:42:06 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/06 16:42:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
      "24/03/06 16:42:06 INFO DAGScheduler: Job 4 finished: takeOrdered at /tmp/ipykernel_14848/2132416953.py:9, took 18.385829 s\n",
      "24/03/06 16:42:06 INFO SparkContext: Starting job: takeOrdered at /tmp/ipykernel_14848/2132416953.py:10\n",
      "24/03/06 16:42:06 INFO DAGScheduler: Registering RDD 13 (reduceByKey at /tmp/ipykernel_14848/2132416953.py:7) as input to shuffle 1\n",
      "24/03/06 16:42:06 INFO DAGScheduler: Got job 5 (takeOrdered at /tmp/ipykernel_14848/2132416953.py:10) with 3 output partitions\n",
      "24/03/06 16:42:06 INFO DAGScheduler: Final stage: ResultStage 7 (takeOrdered at /tmp/ipykernel_14848/2132416953.py:10)\n",
      "24/03/06 16:42:06 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)\n",
      "24/03/06 16:42:06 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 6)\n",
      "24/03/06 16:42:06 INFO DAGScheduler: Submitting ShuffleMapStage 6 (PairwiseRDD[13] at reduceByKey at /tmp/ipykernel_14848/2132416953.py:7), which has no missing parents\n",
      "24/03/06 16:42:06 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 12.9 KiB, free 433.9 MiB)\n",
      "24/03/06 16:42:06 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 192.168.2.250:10005 in memory (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:42:06 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 192.168.2.250:10006 in memory (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:42:06 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 433.9 MiB)\n",
      "24/03/06 16:42:06 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on host-192-168-2-147-de1:10005 (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:42:06 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 16:42:06 INFO BlockManagerInfo: Removed broadcast_7_piece0 on host-192-168-2-147-de1:10005 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:42:06 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 6 (PairwiseRDD[13] at reduceByKey at /tmp/ipykernel_14848/2132416953.py:7) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/03/06 16:42:06 INFO TaskSchedulerImpl: Adding task set 6.0 with 3 tasks resource profile 0\n",
      "24/03/06 16:42:06 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 14) (192.168.2.250, executor 1, partition 0, ANY, 7679 bytes) \n",
      "24/03/06 16:42:06 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 15) (192.168.2.250, executor 0, partition 1, ANY, 7679 bytes) \n",
      "24/03/06 16:42:06 INFO TaskSetManager: Starting task 2.0 in stage 6.0 (TID 16) (192.168.2.250, executor 1, partition 2, ANY, 7679 bytes) \n",
      "24/03/06 16:42:06 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.2.250:10006 (size: 7.8 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:42:06 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.2.250:10005 (size: 7.8 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:42:09 INFO TaskSetManager: Finished task 2.0 in stage 6.0 (TID 16) in 2094 ms on 192.168.2.250 (executor 1) (1/3)\n",
      "24/03/06 16:42:21 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 14) in 14407 ms on 192.168.2.250 (executor 1) (2/3)\n",
      "24/03/06 16:42:21 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 15) in 14810 ms on 192.168.2.250 (executor 0) (3/3)\n",
      "24/03/06 16:42:21 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "24/03/06 16:42:21 INFO DAGScheduler: ShuffleMapStage 6 (reduceByKey at /tmp/ipykernel_14848/2132416953.py:7) finished in 14.841 s\n",
      "24/03/06 16:42:21 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/03/06 16:42:21 INFO DAGScheduler: running: Set()\n",
      "24/03/06 16:42:21 INFO DAGScheduler: waiting: Set(ResultStage 7)\n",
      "24/03/06 16:42:21 INFO DAGScheduler: failed: Set()\n",
      "24/03/06 16:42:21 INFO DAGScheduler: Submitting ResultStage 7 (PythonRDD[17] at takeOrdered at /tmp/ipykernel_14848/2132416953.py:10), which has no missing parents\n",
      "24/03/06 16:42:21 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 11.1 KiB, free 433.9 MiB)\n",
      "24/03/06 16:42:21 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 433.9 MiB)\n",
      "24/03/06 16:42:21 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on host-192-168-2-147-de1:10005 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:42:21 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 16:42:21 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 7 (PythonRDD[17] at takeOrdered at /tmp/ipykernel_14848/2132416953.py:10) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/03/06 16:42:21 INFO TaskSchedulerImpl: Adding task set 7.0 with 3 tasks resource profile 0\n",
      "24/03/06 16:42:21 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 17) (192.168.2.250, executor 1, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/03/06 16:42:21 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 18) (192.168.2.250, executor 0, partition 1, NODE_LOCAL, 7437 bytes) \n",
      "24/03/06 16:42:21 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 19) (192.168.2.250, executor 1, partition 2, NODE_LOCAL, 7437 bytes) \n",
      "24/03/06 16:42:21 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.2.250:10006 (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:42:21 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.2.250:10005 (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:42:21 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.2.250:36934\n",
      "24/03/06 16:42:21 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.2.250:36950\n",
      "24/03/06 16:42:21 INFO BlockManagerInfo: Removed broadcast_8_piece0 on host-192-168-2-147-de1:10005 in memory (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:42:21 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 192.168.2.250:10006 in memory (size: 7.8 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:42:21 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 192.168.2.250:10005 in memory (size: 7.8 KiB, free: 366.2 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 words in English:\n",
      "the: 3498375\n",
      "of: 1659758\n",
      "to: 1539760\n",
      "and: 1288401\n",
      "in: 1085993\n",
      "that: 797516\n",
      "a: 773522\n",
      "is: 758050\n",
      "for: 534242\n",
      "we: 522849\n",
      "\n",
      "Top 10 words in Swedish:\n",
      "att: 1706293\n",
      "och: 1344830\n",
      "i: 1050774\n",
      "det: 924866\n",
      "som: 913276\n",
      "för: 908680\n",
      "av: 738068\n",
      "är: 694381\n",
      "en: 620310\n",
      "vi: 539797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 16:42:22 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 19) in 294 ms on 192.168.2.250 (executor 1) (1/3)\n",
      "24/03/06 16:42:22 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 18) in 312 ms on 192.168.2.250 (executor 0) (2/3)\n",
      "24/03/06 16:42:22 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 17) in 334 ms on 192.168.2.250 (executor 1) (3/3)\n",
      "24/03/06 16:42:22 INFO DAGScheduler: ResultStage 7 (takeOrdered at /tmp/ipykernel_14848/2132416953.py:10) finished in 0.350 s\n",
      "24/03/06 16:42:22 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "24/03/06 16:42:22 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/06 16:42:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
      "24/03/06 16:42:22 INFO DAGScheduler: Job 5 finished: takeOrdered at /tmp/ipykernel_14848/2132416953.py:10, took 15.219807 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check 10 most frequent words in English and Swedish\n",
    "\n",
    "words_en = processed_lines_en.flatMap(lambda line: line)\n",
    "words_sv = processed_lines_sv.flatMap(lambda line: line)\n",
    "\n",
    "word_counts_en = words_en.map(lambda word: (word, 1)).reduceByKey(add)\n",
    "word_counts_sv = words_sv.map(lambda word: (word, 1)).reduceByKey(add)\n",
    "\n",
    "top_10_words_en = word_counts_en.takeOrdered(10, key=lambda x: -x[1])\n",
    "top_10_words_sv = word_counts_sv.takeOrdered(10, key=lambda x: -x[1])\n",
    "\n",
    "print(\"\\nTop 10 words in English:\")\n",
    "for word, count in top_10_words_en:\n",
    "    print(f'{word}: {count}')\n",
    "    \n",
    "print(\"\\nTop 10 words in Swedish:\")\n",
    "for word, count in top_10_words_sv:\n",
    "    print(f'{word}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9b84eae1-31fb-4c21-9258-1c814012defb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBS!!! KOLLA DETTA SÅ DET STÄMMER, DETTA ÄR OM DET ÄR SPLIT(' ') MEN SAGAS RESULTAT ÄR OM DET ÄR SPLIT() i preprocessing delen\n",
    "# Top 10 words in English:\n",
    "# the: 3498375\n",
    "# of: 1659758\n",
    "# to: 1539760\n",
    "# and: 1288401\n",
    "# in: 1085993\n",
    "# that: 797516\n",
    "# a: 773522\n",
    "# is: 758050\n",
    "# for: 534242\n",
    "# we: 522849\n",
    "\n",
    "# Top 10 words in Swedish:\n",
    "# att: 1706293\n",
    "# och: 1344830\n",
    "# i: 1050774\n",
    "# det: 924866\n",
    "# som: 913276\n",
    "# för: 908680\n",
    "# av: 738068\n",
    "# är: 694381\n",
    "# en: 620310\n",
    "# vi: 539797\n",
    "\n",
    "# The results are reasonable, as the most frequent words are common words used to build up sentences (konjunctions, prepositions etc)\n",
    "# In the Swedish translation for example, the words also match most of the top 10 most frequent words in the Swedish language like i, och, att, det, som, en, är, av, för "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "435ab700-0ceb-4bc7-accf-e58f56667379",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 16:43:59 INFO SparkContext: Starting job: zipWithIndex at /tmp/ipykernel_14848/765333879.py:4\n",
      "24/03/06 16:43:59 INFO DAGScheduler: Got job 11 (zipWithIndex at /tmp/ipykernel_14848/765333879.py:4) with 3 output partitions\n",
      "24/03/06 16:43:59 INFO DAGScheduler: Final stage: ResultStage 13 (zipWithIndex at /tmp/ipykernel_14848/765333879.py:4)\n",
      "24/03/06 16:43:59 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/06 16:43:59 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/06 16:43:59 INFO DAGScheduler: Submitting ResultStage 13 (PythonRDD[23] at zipWithIndex at /tmp/ipykernel_14848/765333879.py:4), which has no missing parents\n",
      "24/03/06 16:43:59 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 8.3 KiB, free 433.9 MiB)\n",
      "24/03/06 16:43:59 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 433.9 MiB)\n",
      "24/03/06 16:43:59 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on host-192-168-2-147-de1:10005 (size: 5.1 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:43:59 INFO BlockManagerInfo: Removed broadcast_14_piece0 on host-192-168-2-147-de1:10005 in memory (size: 5.6 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:43:59 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 16:43:59 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 13 (PythonRDD[23] at zipWithIndex at /tmp/ipykernel_14848/765333879.py:4) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/03/06 16:43:59 INFO TaskSchedulerImpl: Adding task set 13.0 with 3 tasks resource profile 0\n",
      "24/03/06 16:43:59 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 28) (192.168.2.250, executor 1, partition 0, ANY, 7690 bytes) \n",
      "24/03/06 16:43:59 INFO TaskSetManager: Starting task 1.0 in stage 13.0 (TID 29) (192.168.2.250, executor 0, partition 1, ANY, 7690 bytes) \n",
      "24/03/06 16:43:59 INFO TaskSetManager: Starting task 2.0 in stage 13.0 (TID 30) (192.168.2.250, executor 1, partition 2, ANY, 7690 bytes) \n",
      "24/03/06 16:43:59 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 192.168.2.250:10005 in memory (size: 5.6 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:43:59 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 192.168.2.250:10005 (size: 5.1 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:43:59 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 192.168.2.250:10006 (size: 5.1 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:44:00 INFO TaskSetManager: Finished task 2.0 in stage 13.0 (TID 30) in 446 ms on 192.168.2.250 (executor 1) (1/3)\n",
      "24/03/06 16:44:02 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 28) in 2943 ms on 192.168.2.250 (executor 1) (2/3)\n",
      "24/03/06 16:44:02 INFO TaskSetManager: Finished task 1.0 in stage 13.0 (TID 29) in 2955 ms on 192.168.2.250 (executor 0) (3/3)\n",
      "24/03/06 16:44:02 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
      "24/03/06 16:44:02 INFO DAGScheduler: ResultStage 13 (zipWithIndex at /tmp/ipykernel_14848/765333879.py:4) finished in 2.973 s\n",
      "24/03/06 16:44:02 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/06 16:44:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished\n",
      "24/03/06 16:44:02 INFO DAGScheduler: Job 11 finished: zipWithIndex at /tmp/ipykernel_14848/765333879.py:4, took 2.980923 s\n",
      "24/03/06 16:44:02 INFO SparkContext: Starting job: zipWithIndex at /tmp/ipykernel_14848/765333879.py:5\n",
      "24/03/06 16:44:02 INFO DAGScheduler: Got job 12 (zipWithIndex at /tmp/ipykernel_14848/765333879.py:5) with 2 output partitions\n",
      "24/03/06 16:44:02 INFO DAGScheduler: Final stage: ResultStage 14 (zipWithIndex at /tmp/ipykernel_14848/765333879.py:5)\n",
      "24/03/06 16:44:02 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/06 16:44:02 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/06 16:44:02 INFO DAGScheduler: Submitting ResultStage 14 (PythonRDD[24] at zipWithIndex at /tmp/ipykernel_14848/765333879.py:5), which has no missing parents\n",
      "24/03/06 16:44:02 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 8.3 KiB, free 433.9 MiB)\n",
      "24/03/06 16:44:02 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 433.9 MiB)\n",
      "24/03/06 16:44:02 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on host-192-168-2-147-de1:10005 (size: 5.1 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:44:02 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 16:44:02 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 14 (PythonRDD[24] at zipWithIndex at /tmp/ipykernel_14848/765333879.py:5) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/06 16:44:02 INFO TaskSchedulerImpl: Adding task set 14.0 with 2 tasks resource profile 0\n",
      "24/03/06 16:44:02 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 192.168.2.250:10005 in memory (size: 5.1 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:44:02 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 192.168.2.250:10006 in memory (size: 5.1 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:44:02 INFO TaskSetManager: Starting task 1.0 in stage 14.0 (TID 31) (192.168.2.250, executor 0, partition 1, NODE_LOCAL, 7690 bytes) \n",
      "24/03/06 16:44:02 INFO BlockManagerInfo: Removed broadcast_15_piece0 on host-192-168-2-147-de1:10005 in memory (size: 5.1 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:44:02 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 192.168.2.250:10005 (size: 5.1 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:44:04 INFO TaskSetManager: Finished task 1.0 in stage 14.0 (TID 31) in 2081 ms on 192.168.2.250 (executor 0) (1/2)\n",
      "24/03/06 16:44:06 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 32) (192.168.2.250, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/03/06 16:44:08 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 32) in 2047 ms on 192.168.2.250 (executor 0) (2/2)\n",
      "24/03/06 16:44:08 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
      "24/03/06 16:44:08 INFO DAGScheduler: ResultStage 14 (zipWithIndex at /tmp/ipykernel_14848/765333879.py:5) finished in 5.225 s\n",
      "24/03/06 16:44:08 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/06 16:44:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished\n",
      "24/03/06 16:44:08 INFO DAGScheduler: Job 12 finished: zipWithIndex at /tmp/ipykernel_14848/765333879.py:5, took 5.229326 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Question A4\n",
    "\n",
    "# Assign a unique index to each line in both RDDs and swap so that the line number is the key\n",
    "sv_swapped = processed_lines_sv.zipWithIndex().map(lambda x: (x[1], x[0]))\n",
    "en_swapped = processed_lines_en.zipWithIndex().map(lambda x: (x[1], x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4a3fa84-d2e9-4e52-8013-08ddf55670ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 16:44:45 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/06 16:44:45 INFO DAGScheduler: Got job 13 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/06 16:44:45 INFO DAGScheduler: Final stage: ResultStage 15 (runJob at PythonRDD.scala:181)\n",
      "24/03/06 16:44:45 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/06 16:44:45 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/06 16:44:45 INFO DAGScheduler: Submitting ResultStage 15 (PythonRDD[25] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/06 16:44:45 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 9.2 KiB, free 433.9 MiB)\n",
      "24/03/06 16:44:45 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 5.6 KiB, free 433.9 MiB)\n",
      "24/03/06 16:44:45 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on host-192-168-2-147-de1:10005 (size: 5.6 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:44:45 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 16:44:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (PythonRDD[25] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/06 16:44:45 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0\n",
      "24/03/06 16:44:45 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 192.168.2.250:10005 in memory (size: 5.1 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:44:45 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 33) (192.168.2.250, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/03/06 16:44:45 INFO BlockManagerInfo: Removed broadcast_16_piece0 on host-192-168-2-147-de1:10005 in memory (size: 5.1 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:44:45 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 192.168.2.250:10005 (size: 5.6 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:44:45 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 33) in 48 ms on 192.168.2.250 (executor 0) (1/1)\n",
      "24/03/06 16:44:45 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool \n",
      "24/03/06 16:44:45 INFO DAGScheduler: ResultStage 15 (runJob at PythonRDD.scala:181) finished in 0.066 s\n",
      "24/03/06 16:44:45 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/06 16:44:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished\n",
      "24/03/06 16:44:45 INFO DAGScheduler: Job 13 finished: runJob at PythonRDD.scala:181, took 0.070337 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, ['återupptagande', 'av', 'sessionen']),\n",
       " (1,\n",
       "  ['jag',\n",
       "   'förklarar',\n",
       "   'europaparlamentets',\n",
       "   'session',\n",
       "   'återupptagen',\n",
       "   'efter',\n",
       "   'avbrottet',\n",
       "   'den',\n",
       "   '17',\n",
       "   'december.',\n",
       "   'jag',\n",
       "   'vill',\n",
       "   'på',\n",
       "   'nytt',\n",
       "   'önska',\n",
       "   'er',\n",
       "   'ett',\n",
       "   'gott',\n",
       "   'nytt',\n",
       "   'år',\n",
       "   'och',\n",
       "   'jag',\n",
       "   'hoppas',\n",
       "   'att',\n",
       "   'ni',\n",
       "   'haft',\n",
       "   'en',\n",
       "   'trevlig',\n",
       "   'semester.']),\n",
       " (2,\n",
       "  ['som',\n",
       "   'ni',\n",
       "   'kunnat',\n",
       "   'konstatera',\n",
       "   'ägde',\n",
       "   '\"den',\n",
       "   'stora',\n",
       "   'år',\n",
       "   '2000-buggen\"',\n",
       "   'aldrig',\n",
       "   'rum.',\n",
       "   'däremot',\n",
       "   'har',\n",
       "   'invånarna',\n",
       "   'i',\n",
       "   'ett',\n",
       "   'antal',\n",
       "   'av',\n",
       "   'våra',\n",
       "   'medlemsländer',\n",
       "   'drabbats',\n",
       "   'av',\n",
       "   'naturkatastrofer',\n",
       "   'som',\n",
       "   'verkligen',\n",
       "   'varit',\n",
       "   'förskräckliga.']),\n",
       " (3,\n",
       "  ['ni',\n",
       "   'har',\n",
       "   'begärt',\n",
       "   'en',\n",
       "   'debatt',\n",
       "   'i',\n",
       "   'ämnet',\n",
       "   'under',\n",
       "   'sammanträdesperiodens',\n",
       "   'kommande',\n",
       "   'dagar.']),\n",
       " (4,\n",
       "  ['till',\n",
       "   'dess',\n",
       "   'vill',\n",
       "   'jag',\n",
       "   'att',\n",
       "   'vi,',\n",
       "   'som',\n",
       "   'ett',\n",
       "   'antal',\n",
       "   'kolleger',\n",
       "   'begärt,',\n",
       "   'håller',\n",
       "   'en',\n",
       "   'tyst',\n",
       "   'minut',\n",
       "   'för',\n",
       "   'offren',\n",
       "   'för',\n",
       "   'bl.a.',\n",
       "   'stormarna',\n",
       "   'i',\n",
       "   'de',\n",
       "   'länder',\n",
       "   'i',\n",
       "   'europeiska',\n",
       "   'unionen',\n",
       "   'som',\n",
       "   'drabbats.']),\n",
       " (5, ['jag', 'ber', 'er', 'resa', 'er', 'för', 'en', 'tyst', 'minut.']),\n",
       " (6, ['(parlamentet', 'höll', 'en', 'tyst', 'minut.)']),\n",
       " (7, ['fru', 'talman!', 'det', 'gäller', 'en', 'ordningsfråga.']),\n",
       " (8,\n",
       "  ['ni',\n",
       "   'känner',\n",
       "   'till',\n",
       "   'från',\n",
       "   'media',\n",
       "   'att',\n",
       "   'det',\n",
       "   'skett',\n",
       "   'en',\n",
       "   'rad',\n",
       "   'bombexplosioner',\n",
       "   'och',\n",
       "   'mord',\n",
       "   'i',\n",
       "   'sri',\n",
       "   'lanka.']),\n",
       " (9,\n",
       "  ['en',\n",
       "   'av',\n",
       "   'de',\n",
       "   'personer',\n",
       "   'som',\n",
       "   'mycket',\n",
       "   'nyligen',\n",
       "   'mördades',\n",
       "   'i',\n",
       "   'sri',\n",
       "   'lanka',\n",
       "   'var',\n",
       "   'kumar',\n",
       "   'ponnambalam,',\n",
       "   'som',\n",
       "   'besökte',\n",
       "   'europaparlamentet',\n",
       "   'för',\n",
       "   'bara',\n",
       "   'några',\n",
       "   'månader',\n",
       "   'sedan.'])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See how it looks, so everything was done correctly\n",
    "sv_swapped.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2df64d79-ba28-46ec-9d10-8234fb07b93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 16:44:56 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/06 16:44:56 INFO DAGScheduler: Got job 14 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/06 16:44:56 INFO DAGScheduler: Final stage: ResultStage 16 (runJob at PythonRDD.scala:181)\n",
      "24/03/06 16:44:56 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/06 16:44:56 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/06 16:44:56 INFO DAGScheduler: Submitting ResultStage 16 (PythonRDD[26] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/06 16:44:56 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 9.2 KiB, free 433.9 MiB)\n",
      "24/03/06 16:44:56 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 5.6 KiB, free 433.9 MiB)\n",
      "24/03/06 16:44:56 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on host-192-168-2-147-de1:10005 (size: 5.6 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:44:56 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 16:44:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (PythonRDD[26] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/06 16:44:56 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0\n",
      "24/03/06 16:44:56 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 34) (192.168.2.250, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/03/06 16:44:56 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 192.168.2.250:10005 in memory (size: 5.6 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:44:56 INFO BlockManagerInfo: Removed broadcast_17_piece0 on host-192-168-2-147-de1:10005 in memory (size: 5.6 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:44:56 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 192.168.2.250:10005 (size: 5.6 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:44:56 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 34) in 59 ms on 192.168.2.250 (executor 0) (1/1)\n",
      "24/03/06 16:44:56 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool \n",
      "24/03/06 16:44:56 INFO DAGScheduler: ResultStage 16 (runJob at PythonRDD.scala:181) finished in 0.076 s\n",
      "24/03/06 16:44:56 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/06 16:44:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished\n",
      "24/03/06 16:44:56 INFO DAGScheduler: Job 14 finished: runJob at PythonRDD.scala:181, took 0.079826 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, ['resumption', 'of', 'the', 'session']),\n",
       " (1,\n",
       "  ['i',\n",
       "   'declare',\n",
       "   'resumed',\n",
       "   'the',\n",
       "   'session',\n",
       "   'of',\n",
       "   'the',\n",
       "   'european',\n",
       "   'parliament',\n",
       "   'adjourned',\n",
       "   'on',\n",
       "   'friday',\n",
       "   '17',\n",
       "   'december',\n",
       "   '1999,',\n",
       "   'and',\n",
       "   'i',\n",
       "   'would',\n",
       "   'like',\n",
       "   'once',\n",
       "   'again',\n",
       "   'to',\n",
       "   'wish',\n",
       "   'you',\n",
       "   'a',\n",
       "   'happy',\n",
       "   'new',\n",
       "   'year',\n",
       "   'in',\n",
       "   'the',\n",
       "   'hope',\n",
       "   'that',\n",
       "   'you',\n",
       "   'enjoyed',\n",
       "   'a',\n",
       "   'pleasant',\n",
       "   'festive',\n",
       "   'period.']),\n",
       " (2,\n",
       "  ['although,',\n",
       "   'as',\n",
       "   'you',\n",
       "   'will',\n",
       "   'have',\n",
       "   'seen,',\n",
       "   'the',\n",
       "   'dreaded',\n",
       "   \"'millennium\",\n",
       "   \"bug'\",\n",
       "   'failed',\n",
       "   'to',\n",
       "   'materialise,',\n",
       "   'still',\n",
       "   'the',\n",
       "   'people',\n",
       "   'in',\n",
       "   'a',\n",
       "   'number',\n",
       "   'of',\n",
       "   'countries',\n",
       "   'suffered',\n",
       "   'a',\n",
       "   'series',\n",
       "   'of',\n",
       "   'natural',\n",
       "   'disasters',\n",
       "   'that',\n",
       "   'truly',\n",
       "   'were',\n",
       "   'dreadful.']),\n",
       " (3,\n",
       "  ['you',\n",
       "   'have',\n",
       "   'requested',\n",
       "   'a',\n",
       "   'debate',\n",
       "   'on',\n",
       "   'this',\n",
       "   'subject',\n",
       "   'in',\n",
       "   'the',\n",
       "   'course',\n",
       "   'of',\n",
       "   'the',\n",
       "   'next',\n",
       "   'few',\n",
       "   'days,',\n",
       "   'during',\n",
       "   'this',\n",
       "   'part-session.']),\n",
       " (4,\n",
       "  ['in',\n",
       "   'the',\n",
       "   'meantime,',\n",
       "   'i',\n",
       "   'should',\n",
       "   'like',\n",
       "   'to',\n",
       "   'observe',\n",
       "   'a',\n",
       "   \"minute'\",\n",
       "   's',\n",
       "   'silence,',\n",
       "   'as',\n",
       "   'a',\n",
       "   'number',\n",
       "   'of',\n",
       "   'members',\n",
       "   'have',\n",
       "   'requested,',\n",
       "   'on',\n",
       "   'behalf',\n",
       "   'of',\n",
       "   'all',\n",
       "   'the',\n",
       "   'victims',\n",
       "   'concerned,',\n",
       "   'particularly',\n",
       "   'those',\n",
       "   'of',\n",
       "   'the',\n",
       "   'terrible',\n",
       "   'storms,',\n",
       "   'in',\n",
       "   'the',\n",
       "   'various',\n",
       "   'countries',\n",
       "   'of',\n",
       "   'the',\n",
       "   'european',\n",
       "   'union.']),\n",
       " (5, ['please', 'rise,', 'then,', 'for', 'this', \"minute'\", 's', 'silence.']),\n",
       " (6,\n",
       "  ['(the',\n",
       "   'house',\n",
       "   'rose',\n",
       "   'and',\n",
       "   'observed',\n",
       "   'a',\n",
       "   \"minute'\",\n",
       "   's',\n",
       "   'silence)']),\n",
       " (7, ['madam', 'president,', 'on', 'a', 'point', 'of', 'order.']),\n",
       " (8,\n",
       "  ['you',\n",
       "   'will',\n",
       "   'be',\n",
       "   'aware',\n",
       "   'from',\n",
       "   'the',\n",
       "   'press',\n",
       "   'and',\n",
       "   'television',\n",
       "   'that',\n",
       "   'there',\n",
       "   'have',\n",
       "   'been',\n",
       "   'a',\n",
       "   'number',\n",
       "   'of',\n",
       "   'bomb',\n",
       "   'explosions',\n",
       "   'and',\n",
       "   'killings',\n",
       "   'in',\n",
       "   'sri',\n",
       "   'lanka.']),\n",
       " (9,\n",
       "  ['one',\n",
       "   'of',\n",
       "   'the',\n",
       "   'people',\n",
       "   'assassinated',\n",
       "   'very',\n",
       "   'recently',\n",
       "   'in',\n",
       "   'sri',\n",
       "   'lanka',\n",
       "   'was',\n",
       "   'mr',\n",
       "   'kumar',\n",
       "   'ponnambalam,',\n",
       "   'who',\n",
       "   'had',\n",
       "   'visited',\n",
       "   'the',\n",
       "   'european',\n",
       "   'parliament',\n",
       "   'just',\n",
       "   'a',\n",
       "   'few',\n",
       "   'months',\n",
       "   'ago.'])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See how it looks, so everything was done correctly\n",
    "en_swapped.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eba067bb-52db-4d8a-995f-008db52af32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the two RDDs together on the line number key\n",
    "sv_en_joined_rdd = sv_swapped.join(en_swapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15c67527-577e-436b-9c6d-c1d83cd0469f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 16:45:01 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/06 16:45:01 INFO DAGScheduler: Registering RDD 31 (join at /tmp/ipykernel_14848/3095717598.py:2) as input to shuffle 2\n",
      "24/03/06 16:45:01 INFO DAGScheduler: Got job 15 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/06 16:45:01 INFO DAGScheduler: Final stage: ResultStage 18 (runJob at PythonRDD.scala:181)\n",
      "24/03/06 16:45:01 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 17)\n",
      "24/03/06 16:45:01 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 17)\n",
      "24/03/06 16:45:01 INFO DAGScheduler: Submitting ShuffleMapStage 17 (PairwiseRDD[31] at join at /tmp/ipykernel_14848/3095717598.py:2), which has no missing parents\n",
      "24/03/06 16:45:01 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 18.5 KiB, free 433.9 MiB)\n",
      "24/03/06 16:45:01 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 8.7 KiB, free 433.9 MiB)\n",
      "24/03/06 16:45:01 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on host-192-168-2-147-de1:10005 (size: 8.7 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:45:01 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 16:45:01 INFO DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 17 (PairwiseRDD[31] at join at /tmp/ipykernel_14848/3095717598.py:2) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\n",
      "24/03/06 16:45:01 INFO TaskSchedulerImpl: Adding task set 17.0 with 5 tasks resource profile 0\n",
      "24/03/06 16:45:01 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 192.168.2.250:10005 in memory (size: 5.6 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:45:01 INFO BlockManagerInfo: Removed broadcast_18_piece0 on host-192-168-2-147-de1:10005 in memory (size: 5.6 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:45:01 INFO TaskSetManager: Starting task 4.0 in stage 17.0 (TID 35) (192.168.2.250, executor 1, partition 4, NODE_LOCAL, 7788 bytes) \n",
      "24/03/06 16:45:01 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 192.168.2.250:10006 (size: 8.7 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:45:05 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 36) (192.168.2.250, executor 0, partition 0, ANY, 7788 bytes) \n",
      "24/03/06 16:45:05 INFO TaskSetManager: Starting task 1.0 in stage 17.0 (TID 37) (192.168.2.250, executor 1, partition 1, ANY, 7788 bytes) \n",
      "24/03/06 16:45:05 INFO TaskSetManager: Starting task 2.0 in stage 17.0 (TID 38) (192.168.2.250, executor 0, partition 2, ANY, 7788 bytes) \n",
      "24/03/06 16:45:05 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 192.168.2.250:10005 (size: 8.7 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:45:07 INFO TaskSetManager: Finished task 2.0 in stage 17.0 (TID 38) in 1983 ms on 192.168.2.250 (executor 0) (1/5)\n",
      "24/03/06 16:45:07 INFO TaskSetManager: Starting task 3.0 in stage 17.0 (TID 39) (192.168.2.250, executor 0, partition 3, ANY, 7788 bytes) \n",
      "24/03/06 16:45:40 INFO TaskSetManager: Finished task 4.0 in stage 17.0 (TID 35) in 39632 ms on 192.168.2.250 (executor 1) (2/5)\n",
      "24/03/06 16:45:42 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 36) in 37179 ms on 192.168.2.250 (executor 0) (3/5)\n",
      "24/03/06 16:45:42 INFO TaskSetManager: Finished task 1.0 in stage 17.0 (TID 37) in 37715 ms on 192.168.2.250 (executor 1) (4/5)\n",
      "24/03/06 16:45:43 INFO TaskSetManager: Finished task 3.0 in stage 17.0 (TID 39) in 36667 ms on 192.168.2.250 (executor 0) (5/5)\n",
      "24/03/06 16:45:43 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool \n",
      "24/03/06 16:45:43 INFO DAGScheduler: ShuffleMapStage 17 (join at /tmp/ipykernel_14848/3095717598.py:2) finished in 42.475 s\n",
      "24/03/06 16:45:43 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/03/06 16:45:43 INFO DAGScheduler: running: Set()\n",
      "24/03/06 16:45:43 INFO DAGScheduler: waiting: Set(ResultStage 18)\n",
      "24/03/06 16:45:43 INFO DAGScheduler: failed: Set()\n",
      "24/03/06 16:45:43 INFO DAGScheduler: Submitting ResultStage 18 (PythonRDD[34] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/06 16:45:43 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 12.6 KiB, free 433.9 MiB)\n",
      "24/03/06 16:45:43 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 433.9 MiB)\n",
      "24/03/06 16:45:43 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on host-192-168-2-147-de1:10005 (size: 7.2 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:45:43 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 16:45:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (PythonRDD[34] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/06 16:45:43 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0\n",
      "24/03/06 16:45:43 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 40) (192.168.2.250, executor 0, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/03/06 16:45:43 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 192.168.2.250:10005 (size: 7.2 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:45:43 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 192.168.2.250:36950\n",
      "24/03/06 16:45:43 INFO BlockManagerInfo: Removed broadcast_19_piece0 on host-192-168-2-147-de1:10005 in memory (size: 8.7 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:45:43 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 192.168.2.250:10005 in memory (size: 8.7 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:45:43 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 192.168.2.250:10006 in memory (size: 8.7 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:46:02 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 40) in 18489 ms on 192.168.2.250 (executor 0) (1/1)\n",
      "24/03/06 16:46:02 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool \n",
      "24/03/06 16:46:02 INFO DAGScheduler: ResultStage 18 (runJob at PythonRDD.scala:181) finished in 18.511 s\n",
      "24/03/06 16:46:02 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/06 16:46:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished\n",
      "24/03/06 16:46:02 INFO DAGScheduler: Job 15 finished: runJob at PythonRDD.scala:181, took 61.001374 s\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(6305,\n",
       "  (['den',\n",
       "    'största',\n",
       "    'svagheten',\n",
       "    'är',\n",
       "    'som',\n",
       "    'jag',\n",
       "    'ser',\n",
       "    'det',\n",
       "    'att',\n",
       "    'man',\n",
       "    'inte',\n",
       "    'har',\n",
       "    'tagit',\n",
       "    'tillräckligt',\n",
       "    'stor',\n",
       "    'hänsyn',\n",
       "    'till',\n",
       "    'de',\n",
       "    'medelstora',\n",
       "    'företagens',\n",
       "    'situation,',\n",
       "    'men',\n",
       "    'bilindustrin',\n",
       "    'består',\n",
       "    'inte',\n",
       "    'endast',\n",
       "    'av',\n",
       "    'storföretag.'],\n",
       "   ['there',\n",
       "    'are',\n",
       "    'certain',\n",
       "    'weaknesses,',\n",
       "    'the',\n",
       "    'greatest',\n",
       "    'of',\n",
       "    'which,',\n",
       "    'in',\n",
       "    'my',\n",
       "    'opinion,',\n",
       "    'is',\n",
       "    'that',\n",
       "    'not',\n",
       "    'enough',\n",
       "    'consideration',\n",
       "    'is',\n",
       "    'being',\n",
       "    'given',\n",
       "    'to',\n",
       "    'the',\n",
       "    'situation',\n",
       "    'of',\n",
       "    'small',\n",
       "    'businesses;',\n",
       "    'after',\n",
       "    'all,',\n",
       "    'the',\n",
       "    'car',\n",
       "    'industry',\n",
       "    'is',\n",
       "    'more',\n",
       "    'than',\n",
       "    'just',\n",
       "    'large',\n",
       "    'companies.'])),\n",
       " (6845,\n",
       "  (['europaparlamentet',\n",
       "    'har',\n",
       "    'aldrig',\n",
       "    'tvekat',\n",
       "    'att',\n",
       "    'kommentera',\n",
       "    'en',\n",
       "    'utveckling',\n",
       "    'man',\n",
       "    'inte',\n",
       "    'samtycker',\n",
       "    'till',\n",
       "    'i',\n",
       "    'medlemsstaterna.'],\n",
       "   ['the',\n",
       "    'european',\n",
       "    'parliament',\n",
       "    'has',\n",
       "    'never',\n",
       "    'been',\n",
       "    'slow',\n",
       "    'to',\n",
       "    'comment',\n",
       "    'on',\n",
       "    'developments',\n",
       "    'in',\n",
       "    'member',\n",
       "    'states',\n",
       "    'with',\n",
       "    'which',\n",
       "    'they',\n",
       "    'disagree.'])),\n",
       " (13400,\n",
       "  (['herr',\n",
       "    'talman,',\n",
       "    'herr',\n",
       "    'kommissionär,',\n",
       "    'mina',\n",
       "    'damer',\n",
       "    'och',\n",
       "    'herrar!',\n",
       "    'låt',\n",
       "    'mig',\n",
       "    'säga',\n",
       "    'direkt',\n",
       "    'att',\n",
       "    'min',\n",
       "    'grupp',\n",
       "    'välkomnar',\n",
       "    'detta',\n",
       "    'förslag',\n",
       "    'eftersom',\n",
       "    'den',\n",
       "    'statistiska',\n",
       "    'grunden',\n",
       "    'för',\n",
       "    'en',\n",
       "    'ekonomisk',\n",
       "    'och',\n",
       "    'monetär',\n",
       "    'union',\n",
       "    'som',\n",
       "    'koncentrera',\n",
       "    'till',\n",
       "    'stabilitet',\n",
       "    'och',\n",
       "    'reell',\n",
       "    'tillväxt',\n",
       "    'härigenom',\n",
       "    'utformas',\n",
       "    'mer',\n",
       "    'harmoniserat,',\n",
       "    'direkt',\n",
       "    'jämförligt',\n",
       "    'och',\n",
       "    'mer',\n",
       "    'exakt.'],\n",
       "   ['mr',\n",
       "    'president,',\n",
       "    'commissioner,',\n",
       "    'let',\n",
       "    'me',\n",
       "    'say',\n",
       "    'at',\n",
       "    'the',\n",
       "    'outset',\n",
       "    'that',\n",
       "    'my',\n",
       "    'group',\n",
       "    'welcomes',\n",
       "    'this',\n",
       "    'proposal',\n",
       "    'because',\n",
       "    'it',\n",
       "    'makes',\n",
       "    'the',\n",
       "    'basic',\n",
       "    'statistical',\n",
       "    'data',\n",
       "    'for',\n",
       "    'an',\n",
       "    'economic',\n",
       "    'and',\n",
       "    'monetary',\n",
       "    'union',\n",
       "    'directed',\n",
       "    'at',\n",
       "    'stability',\n",
       "    'and',\n",
       "    'real',\n",
       "    'growth',\n",
       "    'more',\n",
       "    'harmonised,',\n",
       "    'directly',\n",
       "    'comparable',\n",
       "    'and',\n",
       "    'more',\n",
       "    'accurate.'])),\n",
       " (15170,\n",
       "  (['herr',\n",
       "    'talman!',\n",
       "    'när',\n",
       "    'doñana-katastrofen',\n",
       "    'inträffade',\n",
       "    'lade',\n",
       "    'world',\n",
       "    'wide',\n",
       "    'fund',\n",
       "    '-',\n",
       "    'världsnaturfonden',\n",
       "    '-',\n",
       "    'fram',\n",
       "    'en',\n",
       "    'rapport',\n",
       "    'för',\n",
       "    'oss',\n",
       "    'om',\n",
       "    'den',\n",
       "    'mängd',\n",
       "    'dammar',\n",
       "    'av',\n",
       "    'det',\n",
       "    'här',\n",
       "    'slaget',\n",
       "    'som',\n",
       "    'finns',\n",
       "    'i',\n",
       "    'europa.'],\n",
       "   ['mr',\n",
       "    'president,',\n",
       "    'when',\n",
       "    'the',\n",
       "    'doñana',\n",
       "    'disaster',\n",
       "    'occurred,',\n",
       "    'the',\n",
       "    'non-governmental',\n",
       "    'organisation,',\n",
       "    'the',\n",
       "    'world',\n",
       "    'wide',\n",
       "    'fund',\n",
       "    'for',\n",
       "    'nature,',\n",
       "    'gave',\n",
       "    'us',\n",
       "    'a',\n",
       "    'report',\n",
       "    'on',\n",
       "    'the',\n",
       "    'number',\n",
       "    'of',\n",
       "    'reservoirs',\n",
       "    'of',\n",
       "    'this',\n",
       "    'type',\n",
       "    'in',\n",
       "    'europe.'])),\n",
       " (17070,\n",
       "  (['jag',\n",
       "    'har',\n",
       "    'fått',\n",
       "    'ett',\n",
       "    'förslag',\n",
       "    'till',\n",
       "    'resolution,',\n",
       "    'i',\n",
       "    'enlighet',\n",
       "    'med',\n",
       "    'artikel',\n",
       "    '37.2',\n",
       "    'i',\n",
       "    'arbetsordningen.'],\n",
       "   ['i',\n",
       "    'have',\n",
       "    'received',\n",
       "    'a',\n",
       "    'motion',\n",
       "    'for',\n",
       "    'a',\n",
       "    'resolution',\n",
       "    'pursuant',\n",
       "    'to',\n",
       "    'rule',\n",
       "    '37(2).'])),\n",
       " (31115,\n",
       "  (['det',\n",
       "    'är',\n",
       "    'riktigt',\n",
       "    'att',\n",
       "    'vi',\n",
       "    'måste',\n",
       "    'uppmuntra',\n",
       "    'administratörerna',\n",
       "    'av',\n",
       "    'sajter',\n",
       "    'och',\n",
       "    'internetleverantörerna',\n",
       "    'att',\n",
       "    'fastställa',\n",
       "    'uppförandekoder',\n",
       "    'och',\n",
       "    'att',\n",
       "    'få',\n",
       "    'fram',\n",
       "    'en',\n",
       "    'självreglering,',\n",
       "    'liksom',\n",
       "    'att',\n",
       "    'få',\n",
       "    'internetanvändarna',\n",
       "    'att',\n",
       "    'informera',\n",
       "    'myndigheterna',\n",
       "    'när',\n",
       "    'de',\n",
       "    'stöter',\n",
       "    'på',\n",
       "    'barnpornografiskt',\n",
       "    'material,',\n",
       "    'men',\n",
       "    'det',\n",
       "    'finns',\n",
       "    'också',\n",
       "    'ett',\n",
       "    'ansvar',\n",
       "    'i',\n",
       "    'ländernas',\n",
       "    'och',\n",
       "    'i',\n",
       "    'unionens',\n",
       "    'lagar.'],\n",
       "   ['and',\n",
       "    'although',\n",
       "    'it',\n",
       "    'may',\n",
       "    'be',\n",
       "    'true',\n",
       "    'that',\n",
       "    'we',\n",
       "    'have',\n",
       "    'to',\n",
       "    'encourage',\n",
       "    'web-site',\n",
       "    'administrators',\n",
       "    'and',\n",
       "    'service',\n",
       "    'providers',\n",
       "    'to',\n",
       "    'define',\n",
       "    'their',\n",
       "    'standards',\n",
       "    'of',\n",
       "    'conduct',\n",
       "    'and',\n",
       "    'to',\n",
       "    'regulate',\n",
       "    'themselves,',\n",
       "    'and',\n",
       "    'encourage',\n",
       "    'internet',\n",
       "    'users',\n",
       "    'to',\n",
       "    'inform',\n",
       "    'the',\n",
       "    'authorities',\n",
       "    'whenever',\n",
       "    'they',\n",
       "    'discover',\n",
       "    'child',\n",
       "    'pornography,',\n",
       "    'it',\n",
       "    'is',\n",
       "    'nevertheless',\n",
       "    'true',\n",
       "    'that',\n",
       "    'union',\n",
       "    'and',\n",
       "    'national',\n",
       "    'law',\n",
       "    'bear',\n",
       "    'some',\n",
       "    'responsibility.'])),\n",
       " (31655,\n",
       "  (['dessutom',\n",
       "    'kommer',\n",
       "    'de',\n",
       "    'undantagna',\n",
       "    'strukturerna',\n",
       "    'inte',\n",
       "    'att',\n",
       "    'kunna',\n",
       "    'dra',\n",
       "    'nytta',\n",
       "    'av',\n",
       "    'bestämmelserna',\n",
       "    'i',\n",
       "    'den',\n",
       "    'inre',\n",
       "    'marknaden-passet.'],\n",
       "   ['in',\n",
       "    'addition,',\n",
       "    'the',\n",
       "    'waived',\n",
       "    'schemes',\n",
       "    'will',\n",
       "    'not',\n",
       "    'benefit',\n",
       "    'from',\n",
       "    'the',\n",
       "    'single',\n",
       "    'market',\n",
       "    'passport',\n",
       "    'provisions.'])),\n",
       " (38210,\n",
       "  (['det',\n",
       "    'första',\n",
       "    'området',\n",
       "    'är',\n",
       "    'försiktighetsbestämmelser',\n",
       "    'för',\n",
       "    'pensionsfonder',\n",
       "    'och',\n",
       "    'det',\n",
       "    'sammanhanget',\n",
       "    'måste',\n",
       "    'skyddet',\n",
       "    'av',\n",
       "    'de',\n",
       "    'pensionsberättigade',\n",
       "    'vara',\n",
       "    'prioritet.'],\n",
       "   ['the',\n",
       "    'first',\n",
       "    'area',\n",
       "    'is',\n",
       "    'prudential',\n",
       "    'rules',\n",
       "    'for',\n",
       "    'pension',\n",
       "    'funds,',\n",
       "    'and',\n",
       "    'protecting',\n",
       "    'pension',\n",
       "    'scheme',\n",
       "    'members',\n",
       "    'should',\n",
       "    'take',\n",
       "    'centre',\n",
       "    'stage.'])),\n",
       " (39980,\n",
       "  (['redan',\n",
       "    'åren',\n",
       "    '1979,',\n",
       "    '1993,',\n",
       "    '1998',\n",
       "    'och',\n",
       "    '1999',\n",
       "    'överlämnades',\n",
       "    'resolutioner',\n",
       "    'från',\n",
       "    'europaparlamentet',\n",
       "    'till',\n",
       "    'detta',\n",
       "    'land,',\n",
       "    'men',\n",
       "    'vad',\n",
       "    'har',\n",
       "    'hänt',\n",
       "    'med',\n",
       "    'dessa?'],\n",
       "   ['in',\n",
       "    '1979,',\n",
       "    '1993,',\n",
       "    '1998',\n",
       "    'and',\n",
       "    '1999,',\n",
       "    'resolutions',\n",
       "    'were',\n",
       "    'sent',\n",
       "    'from',\n",
       "    'the',\n",
       "    'european',\n",
       "    'parliament',\n",
       "    'to',\n",
       "    'iran,',\n",
       "    'but',\n",
       "    'what',\n",
       "    'happened',\n",
       "    'to',\n",
       "    'them?'])),\n",
       " (41880,\n",
       "  (['ni',\n",
       "    'har',\n",
       "    'drabbats',\n",
       "    'av',\n",
       "    'stormen',\n",
       "    'och',\n",
       "    'ni',\n",
       "    'har',\n",
       "    'klarat',\n",
       "    'er',\n",
       "    'utan',\n",
       "    'kommissionens',\n",
       "    'hjälp.'],\n",
       "   ['you',\n",
       "    'encountered',\n",
       "    'a',\n",
       "    'storm',\n",
       "    'and',\n",
       "    'weathered',\n",
       "    'it',\n",
       "    'without',\n",
       "    'the',\n",
       "    'help',\n",
       "    'of',\n",
       "    'the',\n",
       "    'commission.']))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See how it looks\n",
    "sv_en_joined_rdd.take(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eca87084-218f-4a1c-b235-6b06ab2f8b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter function for the joined RDD according to lab requirements\n",
    "def filter(lines):\n",
    "    #Filter to exclude empty/missing lines\n",
    "    filtered_rdd = lines.filter(lambda x: len(x[1][0]) > 1 and len(x[1][1]) > 1)\n",
    "\n",
    "    #Filter sentences with a small number of words (optional)\n",
    "    threshold = 5\n",
    "    filtered_rdd = filtered_rdd.filter(lambda x: len(x[1][0]) < threshold and len(x[1][1]) < threshold)\n",
    "\n",
    "    # Filter pairs with the same number of words\n",
    "    filtered_rdd = filtered_rdd.filter(lambda x: len(x[1][0]) == len(x[1][1]))\n",
    "    return filtered_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f17b8cd1-ea42-43bd-bb8e-29cd22d5e661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call on the filter function to filter the joined RDD\n",
    "filtered_rdd = filter(sv_en_joined_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "953d4d5a-0b86-4534-bf0c-8d272fe09315",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 16:15:39 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/06 16:15:39 INFO DAGScheduler: Got job 11 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/06 16:15:39 INFO DAGScheduler: Final stage: ResultStage 15 (runJob at PythonRDD.scala:181)\n",
      "24/03/06 16:15:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)\n",
      "24/03/06 16:15:39 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/06 16:15:39 INFO DAGScheduler: Submitting ResultStage 15 (PythonRDD[30] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/06 16:15:39 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 14.2 KiB, free 433.8 MiB)\n",
      "24/03/06 16:15:39 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)\n",
      "24/03/06 16:15:39 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on host-192-168-2-147-de1:10005 (size: 7.7 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:15:39 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 16:15:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (PythonRDD[30] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/06 16:15:39 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0\n",
      "24/03/06 16:15:39 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 192.168.2.250:10005 in memory (size: 8.7 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:15:39 INFO BlockManagerInfo: Removed broadcast_14_piece0 on host-192-168-2-147-de1:10005 in memory (size: 8.7 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:15:39 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 192.168.2.250:10006 in memory (size: 8.7 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:15:39 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 33) (192.168.2.250, executor 0, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/03/06 16:15:39 INFO BlockManagerInfo: Removed broadcast_15_piece0 on host-192-168-2-147-de1:10005 in memory (size: 7.2 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:15:39 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 192.168.2.250:10006 in memory (size: 7.2 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:15:39 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 192.168.2.250:10006 (size: 7.7 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:15:58 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 33) in 18910 ms on 192.168.2.250 (executor 0) (1/1)\n",
      "24/03/06 16:15:58 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool \n",
      "24/03/06 16:15:58 INFO DAGScheduler: ResultStage 15 (runJob at PythonRDD.scala:181) finished in 18.933 s\n",
      "24/03/06 16:15:58 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/06 16:15:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished\n",
      "24/03/06 16:15:58 INFO DAGScheduler: Job 11 finished: runJob at PythonRDD.scala:181, took 18.941523 s\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(916685, (['det', 'är', 'inte', 'rätt.'], ['that', 'is', 'not', 'right.'])),\n",
       " (950055, (['(tumult', 'och', 'applåder)'], ['(uproar', 'and', 'applause)'])),\n",
       " (1030115, (['inga', 'ändringar.'], ['no', 'amendments'])),\n",
       " (1093240,\n",
       "  (['skriftliga', 'förklaringar', '(artikel', '142)'],\n",
       "   ['written', 'statements', '(rule', '142)'])),\n",
       " (1114365, (['i', 'vilket', 'forum?'], ['in', 'which', 'forum?'])),\n",
       " (1118050, (['detta', 'måste', 'ändras.'], ['this', 'must', 'change.'])),\n",
       " (1491205,\n",
       "  (['debatten', 'är', 'härmed', 'avslutad.'],\n",
       "   ['the', 'debate', 'is', 'closed.'])),\n",
       " (1426720,\n",
       "  (['debatten', 'är', 'härmed', 'avslutad.'],\n",
       "   ['the', 'debate', 'is', 'closed.'])),\n",
       " (1581900,\n",
       "  (['finns', 'det', 'några', 'synpunkter?'],\n",
       "   ['are', 'there', 'any', 'comments?'])),\n",
       " (993145, (['varför', 'inte?'], ['why', 'not?']))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See how it looks, if it was filtered properly\n",
    "filtered_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e41d2309-e687-4477-8ed7-e9328711a6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair words in the order they appear in each sentence pair\n",
    "word_pairs_rdd = filtered_rdd.map(lambda x: zip(x[1][0], x[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1d1c9da-1dea-42b5-8d99-995acf61e5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 16:46:37 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/06 16:46:37 INFO DAGScheduler: Got job 16 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/06 16:46:37 INFO DAGScheduler: Final stage: ResultStage 20 (runJob at PythonRDD.scala:181)\n",
      "24/03/06 16:46:37 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 19)\n",
      "24/03/06 16:46:37 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/06 16:46:37 INFO DAGScheduler: Submitting ResultStage 20 (PythonRDD[35] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/06 16:46:37 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 14.5 KiB, free 433.9 MiB)\n",
      "24/03/06 16:46:37 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 433.9 MiB)\n",
      "24/03/06 16:46:37 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on host-192-168-2-147-de1:10005 (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:46:37 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 192.168.2.250:10005 in memory (size: 7.2 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:46:37 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 16:46:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (PythonRDD[35] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/06 16:46:37 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0\n",
      "24/03/06 16:46:37 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 41) (192.168.2.250, executor 1, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/03/06 16:46:37 INFO BlockManagerInfo: Removed broadcast_20_piece0 on host-192-168-2-147-de1:10005 in memory (size: 7.2 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:46:37 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 192.168.2.250:10006 (size: 7.8 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:46:37 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 192.168.2.250:36934\n",
      "[Stage 20:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('skriftliga', 'written'), ('förklaringar', 'statements'), ('(artikel', '(rule'), ('149)', '149)')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 16:46:56 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 41) in 18598 ms on 192.168.2.250 (executor 1) (1/1)\n",
      "24/03/06 16:46:56 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool \n",
      "24/03/06 16:46:56 INFO DAGScheduler: ResultStage 20 (runJob at PythonRDD.scala:181) finished in 18.623 s\n",
      "24/03/06 16:46:56 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/06 16:46:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished\n",
      "24/03/06 16:46:56 INFO DAGScheduler: Job 16 finished: runJob at PythonRDD.scala:181, took 18.631341 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# See how it looks\n",
    "print(list(word_pairs_rdd.take(1)[0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9334c2e7-b8e5-49dd-a7fb-bb26b06c4241",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 16:47:07 INFO SparkContext: Starting job: takeOrdered at /tmp/ipykernel_14848/2833874270.py:7\n",
      "24/03/06 16:47:07 INFO DAGScheduler: Registering RDD 37 (reduceByKey at /tmp/ipykernel_14848/2833874270.py:4) as input to shuffle 3\n",
      "24/03/06 16:47:07 INFO DAGScheduler: Got job 17 (takeOrdered at /tmp/ipykernel_14848/2833874270.py:7) with 5 output partitions\n",
      "24/03/06 16:47:07 INFO DAGScheduler: Final stage: ResultStage 23 (takeOrdered at /tmp/ipykernel_14848/2833874270.py:7)\n",
      "24/03/06 16:47:07 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 22)\n",
      "24/03/06 16:47:07 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 22)\n",
      "24/03/06 16:47:07 INFO DAGScheduler: Submitting ShuffleMapStage 22 (PairwiseRDD[37] at reduceByKey at /tmp/ipykernel_14848/2833874270.py:4), which has no missing parents\n",
      "24/03/06 16:47:08 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 17.0 KiB, free 433.9 MiB)\n",
      "24/03/06 16:47:08 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 8.8 KiB, free 433.9 MiB)\n",
      "24/03/06 16:47:08 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on host-192-168-2-147-de1:10005 (size: 8.8 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:47:08 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 16:47:08 INFO DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 22 (PairwiseRDD[37] at reduceByKey at /tmp/ipykernel_14848/2833874270.py:4) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\n",
      "24/03/06 16:47:08 INFO TaskSchedulerImpl: Adding task set 22.0 with 5 tasks resource profile 0\n",
      "24/03/06 16:47:08 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 192.168.2.250:10006 in memory (size: 7.8 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:47:08 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 42) (192.168.2.250, executor 1, partition 0, NODE_LOCAL, 7426 bytes) \n",
      "24/03/06 16:47:08 INFO TaskSetManager: Starting task 1.0 in stage 22.0 (TID 43) (192.168.2.250, executor 0, partition 1, NODE_LOCAL, 7426 bytes) \n",
      "24/03/06 16:47:08 INFO TaskSetManager: Starting task 2.0 in stage 22.0 (TID 44) (192.168.2.250, executor 1, partition 2, NODE_LOCAL, 7426 bytes) \n",
      "24/03/06 16:47:08 INFO TaskSetManager: Starting task 3.0 in stage 22.0 (TID 45) (192.168.2.250, executor 0, partition 3, NODE_LOCAL, 7426 bytes) \n",
      "24/03/06 16:47:08 INFO BlockManagerInfo: Removed broadcast_21_piece0 on host-192-168-2-147-de1:10005 in memory (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:47:08 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 192.168.2.250:10006 (size: 8.8 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:47:08 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 192.168.2.250:10005 (size: 8.8 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:47:34 INFO TaskSetManager: Starting task 4.0 in stage 22.0 (TID 46) (192.168.2.250, executor 0, partition 4, NODE_LOCAL, 7426 bytes) \n",
      "24/03/06 16:47:34 INFO TaskSetManager: Finished task 1.0 in stage 22.0 (TID 43) in 26217 ms on 192.168.2.250 (executor 0) (1/5)\n",
      "24/03/06 16:47:34 INFO TaskSetManager: Finished task 3.0 in stage 22.0 (TID 45) in 26660 ms on 192.168.2.250 (executor 0) (2/5)\n",
      "24/03/06 16:47:35 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 42) in 27231 ms on 192.168.2.250 (executor 1) (3/5)\n",
      "24/03/06 16:47:35 INFO TaskSetManager: Finished task 2.0 in stage 22.0 (TID 44) in 27514 ms on 192.168.2.250 (executor 1) (4/5)\n",
      "[Stage 22:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avslutad. - closed.: 2534\n",
      "är - is: 2380\n",
      "jag - the: 1324\n",
      "debatten - is: 1324\n",
      "förklarar - debate: 1317\n",
      "debatten - the: 1225\n",
      "härmed - is: 1215\n",
      "är - debate: 1187\n",
      "(artikel - (rule: 893\n",
      "det - that: 852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 16:47:57 INFO TaskSetManager: Finished task 4.0 in stage 22.0 (TID 46) in 23582 ms on 192.168.2.250 (executor 0) (5/5)\n",
      "24/03/06 16:47:57 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool \n",
      "24/03/06 16:47:57 INFO DAGScheduler: ShuffleMapStage 22 (reduceByKey at /tmp/ipykernel_14848/2833874270.py:4) finished in 49.813 s\n",
      "24/03/06 16:47:57 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/03/06 16:47:57 INFO DAGScheduler: running: Set()\n",
      "24/03/06 16:47:57 INFO DAGScheduler: waiting: Set(ResultStage 23)\n",
      "24/03/06 16:47:57 INFO DAGScheduler: failed: Set()\n",
      "24/03/06 16:47:57 INFO DAGScheduler: Submitting ResultStage 23 (PythonRDD[40] at takeOrdered at /tmp/ipykernel_14848/2833874270.py:7), which has no missing parents\n",
      "24/03/06 16:47:57 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 11.2 KiB, free 433.9 MiB)\n",
      "24/03/06 16:47:57 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 433.9 MiB)\n",
      "24/03/06 16:47:57 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on host-192-168-2-147-de1:10005 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:47:57 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 16:47:57 INFO DAGScheduler: Submitting 5 missing tasks from ResultStage 23 (PythonRDD[40] at takeOrdered at /tmp/ipykernel_14848/2833874270.py:7) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\n",
      "24/03/06 16:47:57 INFO TaskSchedulerImpl: Adding task set 23.0 with 5 tasks resource profile 0\n",
      "24/03/06 16:47:57 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 47) (192.168.2.250, executor 1, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/03/06 16:47:57 INFO TaskSetManager: Starting task 1.0 in stage 23.0 (TID 48) (192.168.2.250, executor 0, partition 1, NODE_LOCAL, 7437 bytes) \n",
      "24/03/06 16:47:57 INFO TaskSetManager: Starting task 2.0 in stage 23.0 (TID 49) (192.168.2.250, executor 1, partition 2, NODE_LOCAL, 7437 bytes) \n",
      "24/03/06 16:47:57 INFO TaskSetManager: Starting task 3.0 in stage 23.0 (TID 50) (192.168.2.250, executor 0, partition 3, NODE_LOCAL, 7437 bytes) \n",
      "24/03/06 16:47:57 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 192.168.2.250:10005 (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:47:57 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 192.168.2.250:10006 (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:47:57 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 192.168.2.250:36934\n",
      "24/03/06 16:47:57 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 192.168.2.250:36950\n",
      "24/03/06 16:47:57 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 192.168.2.250:10006 in memory (size: 8.8 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:47:57 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 192.168.2.250:10005 in memory (size: 8.8 KiB, free: 366.2 MiB)\n",
      "24/03/06 16:47:57 INFO BlockManagerInfo: Removed broadcast_22_piece0 on host-192-168-2-147-de1:10005 in memory (size: 8.8 KiB, free: 434.3 MiB)\n",
      "24/03/06 16:47:57 INFO TaskSetManager: Starting task 4.0 in stage 23.0 (TID 51) (192.168.2.250, executor 1, partition 4, NODE_LOCAL, 7437 bytes) \n",
      "24/03/06 16:47:57 INFO TaskSetManager: Finished task 2.0 in stage 23.0 (TID 49) in 51 ms on 192.168.2.250 (executor 1) (1/5)\n",
      "24/03/06 16:47:57 INFO TaskSetManager: Finished task 3.0 in stage 23.0 (TID 50) in 54 ms on 192.168.2.250 (executor 0) (2/5)\n",
      "24/03/06 16:47:57 INFO TaskSetManager: Finished task 1.0 in stage 23.0 (TID 48) in 60 ms on 192.168.2.250 (executor 0) (3/5)\n",
      "24/03/06 16:47:57 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 47) in 60 ms on 192.168.2.250 (executor 1) (4/5)\n",
      "24/03/06 16:47:57 INFO TaskSetManager: Finished task 4.0 in stage 23.0 (TID 51) in 29 ms on 192.168.2.250 (executor 1) (5/5)\n",
      "24/03/06 16:47:57 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool \n",
      "24/03/06 16:47:57 INFO DAGScheduler: ResultStage 23 (takeOrdered at /tmp/ipykernel_14848/2833874270.py:7) finished in 0.092 s\n",
      "24/03/06 16:47:57 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/06 16:47:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished\n",
      "24/03/06 16:47:57 INFO DAGScheduler: Job 17 finished: takeOrdered at /tmp/ipykernel_14848/2833874270.py:7, took 49.919040 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count the frequency of each word-translation pair\n",
    "words_rdd = word_pairs_rdd.flatMap(lambda line: line)\n",
    "word_counts_rdd = words_rdd.map(lambda word: (word, 1))\n",
    "word_freq_rdd = word_counts_rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Print 10 of the most frequently occurring pairs of words\n",
    "top_pairs = word_freq_rdd.takeOrdered(10, key=lambda x: -x[1])\n",
    "for (word, translation), count in top_pairs:\n",
    "    print(f'{word} - {translation}: {count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e3d9b4-3e94-4b41-ab61-cc9830251fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 most frequent word pairs: \n",
    "# avslutad. - closed.: 2534\n",
    "# är - is: 2380\n",
    "# jag - the: 1324\n",
    "# debatten - is: 1324\n",
    "# förklarar - debate: 1317\n",
    "# debatten - the: 1225\n",
    "# härmed - is: 1215\n",
    "# är - debate: 1187\n",
    "# (artikel - (rule: 893\n",
    "# det - that: 852\n",
    "\n",
    "# The translations are not parfect, and the word pairs doesnt match in translation. However, the first two makes sens, and the last two. \n",
    "# The other ones are not translations of each other, but it looks like one of the most frequent short sentences perhaps could be \n",
    "# \"härmed förklarar jag debatten avslutad\", which is a completely different word structure on english - why the word pairs are a bit \"scrambled\"\n",
    "# That is my conclusion at least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cf355f7-62f9-4e52-a53e-b8d1caea1bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 17:24:38 INFO InMemoryFileIndex: It took 19 ms to list leaf files for 1 paths.\n",
      "24/03/06 17:24:38 INFO InMemoryFileIndex: It took 13 ms to list leaf files for 1 paths.\n",
      "24/03/06 17:24:38 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/03/06 17:24:38 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#267, None)) > 0)\n",
      "24/03/06 17:24:38 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 200.8 KiB, free 433.2 MiB)\n",
      "24/03/06 17:24:38 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 433.2 MiB)\n",
      "24/03/06 17:24:38 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on host-192-168-2-147-de1:10005 (size: 34.6 KiB, free: 434.2 MiB)\n",
      "24/03/06 17:24:38 INFO SparkContext: Created broadcast 10 from csv at NativeMethodAccessorImpl.java:0\n",
      "24/03/06 17:24:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/03/06 17:24:38 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "24/03/06 17:24:38 INFO DAGScheduler: Got job 5 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/03/06 17:24:38 INFO DAGScheduler: Final stage: ResultStage 6 (csv at NativeMethodAccessorImpl.java:0)\n",
      "24/03/06 17:24:38 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/06 17:24:38 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/06 17:24:38 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[29] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/06 17:24:38 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 13.5 KiB, free 433.2 MiB)\n",
      "24/03/06 17:24:38 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 433.2 MiB)\n",
      "24/03/06 17:24:38 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on host-192-168-2-147-de1:10005 (size: 6.4 KiB, free: 434.2 MiB)\n",
      "24/03/06 17:24:38 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 17:24:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[29] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/06 17:24:38 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
      "24/03/06 17:24:38 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 35) (192.168.2.250, executor 0, partition 0, ANY, 8229 bytes) \n",
      "24/03/06 17:24:38 WARN TaskSetManager: Lost task 0.0 in stage 6.0 (TID 35) (192.168.2.250 executor 0): java.nio.file.FileSystemException: /tmp/spark-5cd138a3-4681-40c6-9165-6406769c5be2/executor-da31d952-31b6-4c36-a366-3c2ac260cbd9/blockmgr-7a9bd7b7-94db-4074-ac50-c3ba1df8931b/3d: No space left on device\n",
      "\tat sun.nio.fs.UnixException.translateToIOException(UnixException.java:91)\n",
      "\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n",
      "\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n",
      "\tat sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)\n",
      "\tat java.nio.file.Files.createDirectory(Files.java:674)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:108)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.containsBlock(DiskBlockManager.scala:157)\n",
      "\tat org.apache.spark.storage.DiskStore.contains(DiskStore.scala:154)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$getCurrentBlockStatus(BlockManager.scala:885)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:2075)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1555)\n",
      "\tat org.apache.spark.storage.BlockManager$BlockStoreUpdater.save(BlockManager.scala:380)\n",
      "\tat org.apache.spark.storage.BlockManager.putBytes(BlockManager.scala:1468)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBlocks$1(TorrentBroadcast.scala:215)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.readBlocks(TorrentBroadcast.scala:192)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$4(TorrentBroadcast.scala:281)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$2(TorrentBroadcast.scala:257)\n",
      "\tat org.apache.spark.util.KeyLock.withLock(KeyLock.scala:64)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$1(TorrentBroadcast.scala:252)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\n",
      "\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:94)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:252)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:109)\n",
      "\tat org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "24/03/06 17:24:38 INFO TaskSetManager: Starting task 0.1 in stage 6.0 (TID 36) (192.168.2.250, executor 1, partition 0, ANY, 8229 bytes) \n",
      "24/03/06 17:24:38 WARN TaskSetManager: Lost task 0.1 in stage 6.0 (TID 36) (192.168.2.250 executor 1): java.nio.file.FileSystemException: /tmp/spark-5cd138a3-4681-40c6-9165-6406769c5be2/executor-da31d952-31b6-4c36-a366-3c2ac260cbd9/blockmgr-73b0fea6-ad6e-4203-add1-8686f513f8e6/3d: No space left on device\n",
      "\tat sun.nio.fs.UnixException.translateToIOException(UnixException.java:91)\n",
      "\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n",
      "\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n",
      "\tat sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)\n",
      "\tat java.nio.file.Files.createDirectory(Files.java:674)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:108)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.containsBlock(DiskBlockManager.scala:157)\n",
      "\tat org.apache.spark.storage.DiskStore.contains(DiskStore.scala:154)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$getCurrentBlockStatus(BlockManager.scala:885)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:2075)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1555)\n",
      "\tat org.apache.spark.storage.BlockManager$BlockStoreUpdater.save(BlockManager.scala:380)\n",
      "\tat org.apache.spark.storage.BlockManager.putBytes(BlockManager.scala:1468)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBlocks$1(TorrentBroadcast.scala:215)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.readBlocks(TorrentBroadcast.scala:192)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$4(TorrentBroadcast.scala:281)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$2(TorrentBroadcast.scala:257)\n",
      "\tat org.apache.spark.util.KeyLock.withLock(KeyLock.scala:64)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$1(TorrentBroadcast.scala:252)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\n",
      "\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:94)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:252)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:109)\n",
      "\tat org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "24/03/06 17:24:38 INFO TaskSetManager: Starting task 0.2 in stage 6.0 (TID 37) (192.168.2.250, executor 1, partition 0, ANY, 8229 bytes) \n",
      "24/03/06 17:24:38 INFO TaskSetManager: Lost task 0.2 in stage 6.0 (TID 37) on 192.168.2.250, executor 1: java.nio.file.FileSystemException (/tmp/spark-5cd138a3-4681-40c6-9165-6406769c5be2/executor-da31d952-31b6-4c36-a366-3c2ac260cbd9/blockmgr-73b0fea6-ad6e-4203-add1-8686f513f8e6/3d: No space left on device) [duplicate 1]\n",
      "24/03/06 17:24:38 INFO TaskSetManager: Starting task 0.3 in stage 6.0 (TID 38) (192.168.2.250, executor 1, partition 0, ANY, 8229 bytes) \n",
      "24/03/06 17:24:38 INFO TaskSetManager: Lost task 0.3 in stage 6.0 (TID 38) on 192.168.2.250, executor 1: java.nio.file.FileSystemException (/tmp/spark-5cd138a3-4681-40c6-9165-6406769c5be2/executor-da31d952-31b6-4c36-a366-3c2ac260cbd9/blockmgr-73b0fea6-ad6e-4203-add1-8686f513f8e6/3d: No space left on device) [duplicate 2]\n",
      "24/03/06 17:24:38 ERROR TaskSetManager: Task 0 in stage 6.0 failed 4 times; aborting job\n",
      "24/03/06 17:24:38 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "24/03/06 17:24:38 INFO TaskSchedulerImpl: Cancelling stage 6\n",
      "24/03/06 17:24:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 6.0 failed 4 times, most recent failure: Lost task 0.3 in stage 6.0 (TID 38) (192.168.2.250 executor 1): java.nio.file.FileSystemException: /tmp/spark-5cd138a3-4681-40c6-9165-6406769c5be2/executor-da31d952-31b6-4c36-a366-3c2ac260cbd9/blockmgr-73b0fea6-ad6e-4203-add1-8686f513f8e6/3d: No space left on device\n",
      "\tat sun.nio.fs.UnixException.translateToIOException(UnixException.java:91)\n",
      "\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n",
      "\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n",
      "\tat sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)\n",
      "\tat java.nio.file.Files.createDirectory(Files.java:674)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:108)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.containsBlock(DiskBlockManager.scala:157)\n",
      "\tat org.apache.spark.storage.DiskStore.contains(DiskStore.scala:154)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$getCurrentBlockStatus(BlockManager.scala:885)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:2075)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1555)\n",
      "\tat org.apache.spark.storage.BlockManager$BlockStoreUpdater.save(BlockManager.scala:380)\n",
      "\tat org.apache.spark.storage.BlockManager.putBytes(BlockManager.scala:1468)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBlocks$1(TorrentBroadcast.scala:215)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.readBlocks(TorrentBroadcast.scala:192)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$4(TorrentBroadcast.scala:281)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$2(TorrentBroadcast.scala:257)\n",
      "\tat org.apache.spark.util.KeyLock.withLock(KeyLock.scala:64)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$1(TorrentBroadcast.scala:252)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\n",
      "\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:94)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:252)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:109)\n",
      "\tat org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "24/03/06 17:24:38 INFO DAGScheduler: ResultStage 6 (csv at NativeMethodAccessorImpl.java:0) failed in 0.194 s due to Job aborted due to stage failure: Task 0 in stage 6.0 failed 4 times, most recent failure: Lost task 0.3 in stage 6.0 (TID 38) (192.168.2.250 executor 1): java.nio.file.FileSystemException: /tmp/spark-5cd138a3-4681-40c6-9165-6406769c5be2/executor-da31d952-31b6-4c36-a366-3c2ac260cbd9/blockmgr-73b0fea6-ad6e-4203-add1-8686f513f8e6/3d: No space left on device\n",
      "\tat sun.nio.fs.UnixException.translateToIOException(UnixException.java:91)\n",
      "\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n",
      "\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n",
      "\tat sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)\n",
      "\tat java.nio.file.Files.createDirectory(Files.java:674)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:108)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.containsBlock(DiskBlockManager.scala:157)\n",
      "\tat org.apache.spark.storage.DiskStore.contains(DiskStore.scala:154)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$getCurrentBlockStatus(BlockManager.scala:885)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:2075)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1555)\n",
      "\tat org.apache.spark.storage.BlockManager$BlockStoreUpdater.save(BlockManager.scala:380)\n",
      "\tat org.apache.spark.storage.BlockManager.putBytes(BlockManager.scala:1468)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBlocks$1(TorrentBroadcast.scala:215)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.readBlocks(TorrentBroadcast.scala:192)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$4(TorrentBroadcast.scala:281)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$2(TorrentBroadcast.scala:257)\n",
      "\tat org.apache.spark.util.KeyLock.withLock(KeyLock.scala:64)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$1(TorrentBroadcast.scala:252)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\n",
      "\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:94)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:252)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:109)\n",
      "\tat org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "24/03/06 17:24:38 INFO DAGScheduler: Job 5 failed: csv at NativeMethodAccessorImpl.java:0, took 0.205952 s\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o98.csv.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 4 times, most recent failure: Lost task 0.3 in stage 6.0 (TID 38) (192.168.2.250 executor 1): java.nio.file.FileSystemException: /tmp/spark-5cd138a3-4681-40c6-9165-6406769c5be2/executor-da31d952-31b6-4c36-a366-3c2ac260cbd9/blockmgr-73b0fea6-ad6e-4203-add1-8686f513f8e6/3d: No space left on device\n\tat sun.nio.fs.UnixException.translateToIOException(UnixException.java:91)\n\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n\tat sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)\n\tat java.nio.file.Files.createDirectory(Files.java:674)\n\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:108)\n\tat org.apache.spark.storage.DiskBlockManager.containsBlock(DiskBlockManager.scala:157)\n\tat org.apache.spark.storage.DiskStore.contains(DiskStore.scala:154)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$getCurrentBlockStatus(BlockManager.scala:885)\n\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:2075)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1555)\n\tat org.apache.spark.storage.BlockManager$BlockStoreUpdater.save(BlockManager.scala:380)\n\tat org.apache.spark.storage.BlockManager.putBytes(BlockManager.scala:1468)\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBlocks$1(TorrentBroadcast.scala:215)\n\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.broadcast.TorrentBroadcast.readBlocks(TorrentBroadcast.scala:192)\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$4(TorrentBroadcast.scala:281)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$2(TorrentBroadcast.scala:257)\n\tat org.apache.spark.util.KeyLock.withLock(KeyLock.scala:64)\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$1(TorrentBroadcast.scala:252)\n\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\n\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:94)\n\tat org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:252)\n\tat org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:109)\n\tat org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:111)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:64)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.nio.file.FileSystemException: /tmp/spark-5cd138a3-4681-40c6-9165-6406769c5be2/executor-da31d952-31b6-4c36-a366-3c2ac260cbd9/blockmgr-73b0fea6-ad6e-4203-add1-8686f513f8e6/3d: No space left on device\n\tat sun.nio.fs.UnixException.translateToIOException(UnixException.java:91)\n\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n\tat sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)\n\tat java.nio.file.Files.createDirectory(Files.java:674)\n\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:108)\n\tat org.apache.spark.storage.DiskBlockManager.containsBlock(DiskBlockManager.scala:157)\n\tat org.apache.spark.storage.DiskStore.contains(DiskStore.scala:154)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$getCurrentBlockStatus(BlockManager.scala:885)\n\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:2075)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1555)\n\tat org.apache.spark.storage.BlockManager$BlockStoreUpdater.save(BlockManager.scala:380)\n\tat org.apache.spark.storage.BlockManager.putBytes(BlockManager.scala:1468)\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBlocks$1(TorrentBroadcast.scala:215)\n\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.broadcast.TorrentBroadcast.readBlocks(TorrentBroadcast.scala:192)\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$4(TorrentBroadcast.scala:281)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$2(TorrentBroadcast.scala:257)\n\tat org.apache.spark.util.KeyLock.withLock(KeyLock.scala:64)\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$1(TorrentBroadcast.scala:252)\n\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\n\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:94)\n\tat org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:252)\n\tat org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:109)\n\tat org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Section B - Working with DataFrames and SQL\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m dataframe \u001b[38;5;241m=\u001b[39m \u001b[43mspark_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhdfs://192.168.2.250:9000/parking-citations.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minferSchema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m dataframe\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o98.csv.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 4 times, most recent failure: Lost task 0.3 in stage 6.0 (TID 38) (192.168.2.250 executor 1): java.nio.file.FileSystemException: /tmp/spark-5cd138a3-4681-40c6-9165-6406769c5be2/executor-da31d952-31b6-4c36-a366-3c2ac260cbd9/blockmgr-73b0fea6-ad6e-4203-add1-8686f513f8e6/3d: No space left on device\n\tat sun.nio.fs.UnixException.translateToIOException(UnixException.java:91)\n\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n\tat sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)\n\tat java.nio.file.Files.createDirectory(Files.java:674)\n\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:108)\n\tat org.apache.spark.storage.DiskBlockManager.containsBlock(DiskBlockManager.scala:157)\n\tat org.apache.spark.storage.DiskStore.contains(DiskStore.scala:154)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$getCurrentBlockStatus(BlockManager.scala:885)\n\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:2075)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1555)\n\tat org.apache.spark.storage.BlockManager$BlockStoreUpdater.save(BlockManager.scala:380)\n\tat org.apache.spark.storage.BlockManager.putBytes(BlockManager.scala:1468)\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBlocks$1(TorrentBroadcast.scala:215)\n\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.broadcast.TorrentBroadcast.readBlocks(TorrentBroadcast.scala:192)\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$4(TorrentBroadcast.scala:281)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$2(TorrentBroadcast.scala:257)\n\tat org.apache.spark.util.KeyLock.withLock(KeyLock.scala:64)\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$1(TorrentBroadcast.scala:252)\n\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\n\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:94)\n\tat org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:252)\n\tat org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:109)\n\tat org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:111)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:64)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.nio.file.FileSystemException: /tmp/spark-5cd138a3-4681-40c6-9165-6406769c5be2/executor-da31d952-31b6-4c36-a366-3c2ac260cbd9/blockmgr-73b0fea6-ad6e-4203-add1-8686f513f8e6/3d: No space left on device\n\tat sun.nio.fs.UnixException.translateToIOException(UnixException.java:91)\n\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n\tat sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)\n\tat java.nio.file.Files.createDirectory(Files.java:674)\n\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:108)\n\tat org.apache.spark.storage.DiskBlockManager.containsBlock(DiskBlockManager.scala:157)\n\tat org.apache.spark.storage.DiskStore.contains(DiskStore.scala:154)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$getCurrentBlockStatus(BlockManager.scala:885)\n\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:2075)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1555)\n\tat org.apache.spark.storage.BlockManager$BlockStoreUpdater.save(BlockManager.scala:380)\n\tat org.apache.spark.storage.BlockManager.putBytes(BlockManager.scala:1468)\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBlocks$1(TorrentBroadcast.scala:215)\n\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.broadcast.TorrentBroadcast.readBlocks(TorrentBroadcast.scala:192)\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$4(TorrentBroadcast.scala:281)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$2(TorrentBroadcast.scala:257)\n\tat org.apache.spark.util.KeyLock.withLock(KeyLock.scala:64)\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$1(TorrentBroadcast.scala:252)\n\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\n\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:94)\n\tat org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:252)\n\tat org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:109)\n\tat org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "# Section B - Working with DataFrames and SQL\n",
    "\n",
    "dataframe = spark_session.read.csv(\"hdfs://192.168.2.250:9000/parking-citations.csv\", header = True, inferSchema=True)\n",
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a035ddb5-2b80-40a6-b576-6aec5cf5f0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ticket number: string (nullable = true)\n",
      " |-- Issue Date: timestamp (nullable = true)\n",
      " |-- Issue time: double (nullable = true)\n",
      " |-- Meter Id: string (nullable = true)\n",
      " |-- Marked Time: double (nullable = true)\n",
      " |-- RP State Plate: string (nullable = true)\n",
      " |-- Plate Expiry Date: double (nullable = true)\n",
      " |-- VIN: string (nullable = true)\n",
      " |-- Make: string (nullable = true)\n",
      " |-- Body Style: string (nullable = true)\n",
      " |-- Color: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Route: string (nullable = true)\n",
      " |-- Agency: double (nullable = true)\n",
      " |-- Violation code: string (nullable = true)\n",
      " |-- Violation Description: string (nullable = true)\n",
      " |-- Fine amount: double (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- Agency Description: string (nullable = true)\n",
      " |-- Color Description: string (nullable = true)\n",
      " |-- Body Style Description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a18d5d3e-384e-451d-b119-ddf75e5fc344",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 17:14:38 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/03/06 17:14:38 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/03/06 17:14:38 INFO CodeGenerator: Code generated in 45.135273 ms\n",
      "24/03/06 17:14:38 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 200.7 KiB, free 433.5 MiB)\n",
      "24/03/06 17:14:38 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.2.250:10005 in memory (size: 10.0 KiB, free: 366.2 MiB)\n",
      "24/03/06 17:14:38 INFO BlockManagerInfo: Removed broadcast_5_piece0 on host-192-168-2-147-de1:10005 in memory (size: 10.0 KiB, free: 434.3 MiB)\n",
      "24/03/06 17:14:38 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 433.5 MiB)\n",
      "24/03/06 17:14:38 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on host-192-168-2-147-de1:10005 (size: 34.8 KiB, free: 434.3 MiB)\n",
      "24/03/06 17:14:38 INFO SparkContext: Created broadcast 6 from count at NativeMethodAccessorImpl.java:0\n",
      "24/03/06 17:14:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/03/06 17:14:38 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.2.250:10005 in memory (size: 34.8 KiB, free: 366.2 MiB)\n",
      "24/03/06 17:14:38 INFO BlockManagerInfo: Removed broadcast_4_piece0 on host-192-168-2-147-de1:10005 in memory (size: 34.8 KiB, free: 434.3 MiB)\n",
      "24/03/06 17:14:38 INFO DAGScheduler: Registering RDD 17 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "24/03/06 17:14:38 INFO DAGScheduler: Got map stage job 3 (count at NativeMethodAccessorImpl.java:0) with 16 output partitions\n",
      "24/03/06 17:14:38 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/03/06 17:14:38 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/06 17:14:38 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/06 17:14:38 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[17] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/06 17:14:38 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 18.6 KiB, free 433.7 MiB)\n",
      "24/03/06 17:14:38 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 433.7 MiB)\n",
      "24/03/06 17:14:38 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on host-192-168-2-147-de1:10005 (size: 9.2 KiB, free: 434.3 MiB)\n",
      "24/03/06 17:14:38 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 17:14:38 INFO DAGScheduler: Submitting 16 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[17] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
      "24/03/06 17:14:38 INFO TaskSchedulerImpl: Adding task set 3.0 with 16 tasks resource profile 0\n",
      "24/03/06 17:14:38 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 18) (192.168.2.250, executor 1, partition 0, ANY, 8218 bytes) \n",
      "24/03/06 17:14:38 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 19) (192.168.2.250, executor 0, partition 1, ANY, 8218 bytes) \n",
      "24/03/06 17:14:38 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 20) (192.168.2.250, executor 1, partition 2, ANY, 8218 bytes) \n",
      "24/03/06 17:14:38 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 21) (192.168.2.250, executor 0, partition 3, ANY, 8218 bytes) \n",
      "24/03/06 17:14:38 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.2.250:10005 (size: 9.2 KiB, free: 366.2 MiB)\n",
      "24/03/06 17:14:38 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.2.250:10006 (size: 9.2 KiB, free: 366.3 MiB)\n",
      "24/03/06 17:14:38 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.2.250:10005 (size: 34.8 KiB, free: 366.2 MiB)\n",
      "24/03/06 17:14:38 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.2.250:10006 (size: 34.8 KiB, free: 366.2 MiB)\n",
      "24/03/06 17:14:39 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 22) (192.168.2.250, executor 1, partition 4, ANY, 8218 bytes) \n",
      "24/03/06 17:14:39 INFO TaskSetManager: Starting task 5.0 in stage 3.0 (TID 23) (192.168.2.250, executor 1, partition 5, ANY, 8218 bytes) \n",
      "24/03/06 17:14:39 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 20) in 1366 ms on 192.168.2.250 (executor 1) (1/16)\n",
      "24/03/06 17:14:39 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 18) in 1369 ms on 192.168.2.250 (executor 1) (2/16)\n",
      "24/03/06 17:14:39 INFO TaskSetManager: Starting task 6.0 in stage 3.0 (TID 24) (192.168.2.250, executor 0, partition 6, ANY, 8218 bytes) \n",
      "24/03/06 17:14:39 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 19) in 1371 ms on 192.168.2.250 (executor 0) (3/16)\n",
      "24/03/06 17:14:39 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 21) in 1373 ms on 192.168.2.250 (executor 0) (4/16)\n",
      "24/03/06 17:14:39 INFO TaskSetManager: Starting task 7.0 in stage 3.0 (TID 25) (192.168.2.250, executor 0, partition 7, ANY, 8218 bytes) \n",
      "24/03/06 17:14:40 INFO TaskSetManager: Starting task 8.0 in stage 3.0 (TID 26) (192.168.2.250, executor 0, partition 8, ANY, 8218 bytes) \n",
      "24/03/06 17:14:40 INFO TaskSetManager: Finished task 6.0 in stage 3.0 (TID 24) in 623 ms on 192.168.2.250 (executor 0) (5/16)\n",
      "24/03/06 17:14:40 INFO TaskSetManager: Starting task 9.0 in stage 3.0 (TID 27) (192.168.2.250, executor 1, partition 9, ANY, 8218 bytes) \n",
      "24/03/06 17:14:40 INFO TaskSetManager: Finished task 5.0 in stage 3.0 (TID 23) in 635 ms on 192.168.2.250 (executor 1) (6/16)\n",
      "24/03/06 17:14:40 INFO TaskSetManager: Starting task 10.0 in stage 3.0 (TID 28) (192.168.2.250, executor 1, partition 10, ANY, 8218 bytes) \n",
      "24/03/06 17:14:40 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 22) in 643 ms on 192.168.2.250 (executor 1) (7/16)\n",
      "24/03/06 17:14:40 INFO TaskSetManager: Finished task 7.0 in stage 3.0 (TID 25) in 632 ms on 192.168.2.250 (executor 0) (8/16)\n",
      "24/03/06 17:14:40 INFO TaskSetManager: Starting task 11.0 in stage 3.0 (TID 29) (192.168.2.250, executor 0, partition 11, ANY, 8218 bytes) \n",
      "24/03/06 17:14:41 INFO TaskSetManager: Starting task 12.0 in stage 3.0 (TID 30) (192.168.2.250, executor 0, partition 12, ANY, 8218 bytes) \n",
      "24/03/06 17:14:41 INFO TaskSetManager: Starting task 13.0 in stage 3.0 (TID 31) (192.168.2.250, executor 1, partition 13, ANY, 8218 bytes) \n",
      "24/03/06 17:14:41 INFO TaskSetManager: Finished task 9.0 in stage 3.0 (TID 27) in 573 ms on 192.168.2.250 (executor 1) (9/16)\n",
      "24/03/06 17:14:41 INFO TaskSetManager: Finished task 8.0 in stage 3.0 (TID 26) in 584 ms on 192.168.2.250 (executor 0) (10/16)\n",
      "24/03/06 17:14:41 INFO TaskSetManager: Starting task 14.0 in stage 3.0 (TID 32) (192.168.2.250, executor 0, partition 14, ANY, 8218 bytes) \n",
      "24/03/06 17:14:41 INFO TaskSetManager: Finished task 11.0 in stage 3.0 (TID 29) in 570 ms on 192.168.2.250 (executor 0) (11/16)\n",
      "24/03/06 17:14:41 INFO TaskSetManager: Starting task 15.0 in stage 3.0 (TID 33) (192.168.2.250, executor 1, partition 15, ANY, 8218 bytes) \n",
      "24/03/06 17:14:41 INFO TaskSetManager: Finished task 10.0 in stage 3.0 (TID 28) in 608 ms on 192.168.2.250 (executor 1) (12/16)\n",
      "24/03/06 17:14:41 INFO TaskSetManager: Finished task 15.0 in stage 3.0 (TID 33) in 154 ms on 192.168.2.250 (executor 1) (13/16)\n",
      "24/03/06 17:14:41 INFO TaskSetManager: Finished task 12.0 in stage 3.0 (TID 30) in 573 ms on 192.168.2.250 (executor 0) (14/16)\n",
      "24/03/06 17:14:41 INFO TaskSetManager: Finished task 13.0 in stage 3.0 (TID 31) in 575 ms on 192.168.2.250 (executor 1) (15/16)\n",
      "24/03/06 17:14:41 INFO TaskSetManager: Finished task 14.0 in stage 3.0 (TID 32) in 615 ms on 192.168.2.250 (executor 0) (16/16)\n",
      "24/03/06 17:14:41 INFO DAGScheduler: ShuffleMapStage 3 (count at NativeMethodAccessorImpl.java:0) finished in 3.233 s\n",
      "24/03/06 17:14:41 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/03/06 17:14:41 INFO DAGScheduler: running: Set()\n",
      "24/03/06 17:14:41 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "24/03/06 17:14:41 INFO DAGScheduler: waiting: Set()\n",
      "24/03/06 17:14:41 INFO DAGScheduler: failed: Set()\n",
      "24/03/06 17:14:41 INFO CodeGenerator: Code generated in 20.772961 ms\n",
      "24/03/06 17:14:41 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "24/03/06 17:14:41 INFO DAGScheduler: Got job 4 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/03/06 17:14:41 INFO DAGScheduler: Final stage: ResultStage 5 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/03/06 17:14:41 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\n",
      "24/03/06 17:14:41 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/06 17:14:41 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[20] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/06 17:14:41 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 12.5 KiB, free 433.7 MiB)\n",
      "24/03/06 17:14:41 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 433.7 MiB)\n",
      "24/03/06 17:14:41 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on host-192-168-2-147-de1:10005 (size: 5.9 KiB, free: 434.3 MiB)\n",
      "24/03/06 17:14:41 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/06 17:14:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[20] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/06 17:14:41 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
      "24/03/06 17:14:41 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 34) (192.168.2.250, executor 0, partition 0, NODE_LOCAL, 7619 bytes) \n",
      "24/03/06 17:14:41 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.2.250:10006 (size: 5.9 KiB, free: 366.2 MiB)\n",
      "24/03/06 17:14:41 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.2.250:51008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 13077724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 17:14:42 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 34) in 298 ms on 192.168.2.250 (executor 0) (1/1)\n",
      "24/03/06 17:14:42 INFO DAGScheduler: ResultStage 5 (count at NativeMethodAccessorImpl.java:0) finished in 0.325 s\n",
      "24/03/06 17:14:42 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/06 17:14:42 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "24/03/06 17:14:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
      "24/03/06 17:14:42 INFO DAGScheduler: Job 4 finished: count at NativeMethodAccessorImpl.java:0, took 0.347751 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Row count of CSV file\n",
    "print(f\"Number of rows: {dataframe.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b971020-55db-4f3d-857e-0c0c9586ddfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of rows: 13077724"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a09c11f6-040c-49e0-b315-5f98928e9650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 17:15:27 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/03/06 17:15:27 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/03/06 17:15:27 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 200.7 KiB, free 433.5 MiB)\n",
      "24/03/06 17:15:27 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 433.4 MiB)\n",
      "24/03/06 17:15:27 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on host-192-168-2-147-de1:10005 (size: 34.8 KiB, free: 434.2 MiB)\n",
      "24/03/06 17:15:27 INFO SparkContext: Created broadcast 9 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "24/03/06 17:15:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.\n"
     ]
    }
   ],
   "source": [
    "# Number of partitions in the underlying RDD\n",
    "print(f\"Number of partitions: {dataframe.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70882529-d9f4-4543-ada3-1da718fa6602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns VIN, Latitude, and Longitude\n",
    "dataframe = dataframe.drop(\"VIN\", \"Latitude\", \"Longitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb3056a3-d145-447b-b391-9cb370d9a570",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Fine_amount` cannot be resolved. Did you mean one of the following? [`Fine amount`, `Location`, `Agency`, `Color`, `Make`].;\n'Project [Ticket number#17, Issue Date#18, Issue time#19, Meter Id#20, Marked Time#21, RP State Plate#22, Plate Expiry Date#23, Make#25, Body Style#26, Color#27, Location#28, Route#29, Agency#30, Violation code#31, Violation Description#32, Fine amount#33, Agency Description#36, Color Description#37, Body Style Description#38, cast('Fine_amount as float) AS FineAmountFloat#266]\n+- Project [Ticket number#17, Issue Date#18, Issue time#19, Meter Id#20, Marked Time#21, RP State Plate#22, Plate Expiry Date#23, Make#25, Body Style#26, Color#27, Location#28, Route#29, Agency#30, Violation code#31, Violation Description#32, Fine amount#33, Agency Description#36, Color Description#37, Body Style Description#38]\n   +- Relation [Ticket number#17,Issue Date#18,Issue time#19,Meter Id#20,Marked Time#21,RP State Plate#22,Plate Expiry Date#23,VIN#24,Make#25,Body Style#26,Color#27,Location#28,Route#29,Agency#30,Violation code#31,Violation Description#32,Fine amount#33,Latitude#34,Longitude#35,Agency Description#36,Color Description#37,Body Style Description#38] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Maximum fine amount and how many fines this amount has\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m col\n\u001b[0;32m----> 5\u001b[0m dataframe \u001b[38;5;241m=\u001b[39m \u001b[43mdataframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFineAmountFloat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFine_amount\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfloat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Show the DataFrame to verify the new column\u001b[39;00m\n\u001b[1;32m      9\u001b[0m parking_citations_df\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/dataframe.py:5170\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   5165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[1;32m   5166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   5167\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5168\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   5169\u001b[0m     )\n\u001b[0;32m-> 5170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Fine_amount` cannot be resolved. Did you mean one of the following? [`Fine amount`, `Location`, `Agency`, `Color`, `Make`].;\n'Project [Ticket number#17, Issue Date#18, Issue time#19, Meter Id#20, Marked Time#21, RP State Plate#22, Plate Expiry Date#23, Make#25, Body Style#26, Color#27, Location#28, Route#29, Agency#30, Violation code#31, Violation Description#32, Fine amount#33, Agency Description#36, Color Description#37, Body Style Description#38, cast('Fine_amount as float) AS FineAmountFloat#266]\n+- Project [Ticket number#17, Issue Date#18, Issue time#19, Meter Id#20, Marked Time#21, RP State Plate#22, Plate Expiry Date#23, Make#25, Body Style#26, Color#27, Location#28, Route#29, Agency#30, Violation code#31, Violation Description#32, Fine amount#33, Agency Description#36, Color Description#37, Body Style Description#38]\n   +- Relation [Ticket number#17,Issue Date#18,Issue time#19,Meter Id#20,Marked Time#21,RP State Plate#22,Plate Expiry Date#23,VIN#24,Make#25,Body Style#26,Color#27,Location#28,Route#29,Agency#30,Violation code#31,Violation Description#32,Fine amount#33,Latitude#34,Longitude#35,Agency Description#36,Color Description#37,Body Style Description#38] csv\n"
     ]
    }
   ],
   "source": [
    "# Maximum fine amount and how many fines this amount has\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "dataframe = dataframe.withColumn(\"FineAmountFloat\", col(\"Fine_amount\").cast(\"float\"))\n",
    "\n",
    "\n",
    "# Show the DataFrame to verify the new column\n",
    "parking_citations_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eda738-e27a-469f-a4fd-900ec739f5d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# release the cores for another application!\n",
    "# spark_context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6167cac4-7534-4a8b-a217-f06a4a2b5df5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
